<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Notes of Andrew Ngâ€™s Machine Learning â€”â€” (17) Large Scale Machine LearningGradient Descent with Large DatasetsLearning With Large DatasetsWhen learning with large datasets, it can be high cost to do a">
<meta property="og:type" content="article">
<meta property="og:title" content="å¤§æ•°æ®æœºå™¨å­¦ä¹ ">
<meta property="og:url" content="https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/index.html">
<meta property="og:site_name" content="clownote">
<meta property="og:description" content="Notes of Andrew Ngâ€™s Machine Learning â€”â€” (17) Large Scale Machine LearningGradient Descent with Large DatasetsLearning With Large DatasetsWhen learning with large datasets, it can be high cost to do a">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwgy1g9rn4jr6zxj30om07r0vl.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwly1g9ywywm9hgj30yg0ih79d.jpg">
<meta property="article:published_time" content="2019-12-17T16:17:03.000Z">
<meta property="article:modified_time" content="2021-02-21T06:58:37.445Z">
<meta property="article:author" content="CDFMLR">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/006tNbRwgy1g9rn4jr6zxj30om07r0vl.jpg">
    
    
        
          
              <link rel="shortcut icon" href="/images/rabbit.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/rabbit_192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/rabbit_180.png">
          
        
    
    <!-- title -->
    <title>å¤§æ•°æ®æœºå™¨å­¦ä¹ </title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
    <!--Google search varification (PRIVATE)-->
    <meta name="google-site-verification" content="MrqlpFAD8nDanw3Ypv7ZsIWHLnTdhRuLa4QhSVwxIvc" />
    <!--Google AdSense å…³è” (PRIVATE)-->
    <script data-ad-client="ca-pub-1510963483941114" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<meta name="generator" content="Hexo 5.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">é¦–é¡µ</a></li>
         
          <li><a href="/about/">å…³äº</a></li>
         
          <li><a href="/archives/">å½’æ¡£</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">é¡¹ç›®</a></li>
         
          <li><a href="/search/">æœç´¢</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2020/01/16/AndrewNgML/AndrewNgMLNotes/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/12/10/blog/CSharpAlarm/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">ä¸Šä¸€ç¯‡</span>
      <span id="i-next" class="info" style="display:none;">ä¸‹ä¸€ç¯‡</span>
      <span id="i-top" class="info" style="display:none;">è¿”å›é¡¶éƒ¨</span>
      <span id="i-share" class="info" style="display:none;">åˆ†äº«æ–‡ç« </span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&text=å¤§æ•°æ®æœºå™¨å­¦ä¹ "><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=å¤§æ•°æ®æœºå™¨å­¦ä¹ "><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&is_video=false&description=å¤§æ•°æ®æœºå™¨å­¦ä¹ "><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=å¤§æ•°æ®æœºå™¨å­¦ä¹ &body=Check out this article: https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=å¤§æ•°æ®æœºå™¨å­¦ä¹ "><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=å¤§æ•°æ®æœºå™¨å­¦ä¹ "><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=å¤§æ•°æ®æœºå™¨å­¦ä¹ "><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=å¤§æ•°æ®æœºå™¨å­¦ä¹ "><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&name=å¤§æ•°æ®æœºå™¨å­¦ä¹ &description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Notes-of-Andrew-Ng%E2%80%99s-Machine-Learning-%E2%80%94%E2%80%94-17-Large-Scale-Machine-Learning"><span class="toc-number">1.</span> <span class="toc-text">Notes of Andrew Ngâ€™s Machine Learning â€”â€” (17) Large Scale Machine Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent-with-Large-Datasets"><span class="toc-number">1.1.</span> <span class="toc-text">Gradient Descent with Large Datasets</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Learning-With-Large-Datasets"><span class="toc-number">1.1.1.</span> <span class="toc-text">Learning With Large Datasets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-Gradient-Descent"><span class="toc-number">1.1.2.</span> <span class="toc-text">Stochastic Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-Batch-Gradient-Descent"><span class="toc-number">1.1.3.</span> <span class="toc-text">Mini-Batch Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-Gradient-Descent-Convergence"><span class="toc-number">1.1.4.</span> <span class="toc-text">Stochastic Gradient Descent Convergence</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Checking-for-convergence"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">Checking for convergence</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Advanced-Topics"><span class="toc-number">1.2.</span> <span class="toc-text">Advanced Topics</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Online-Learning"><span class="toc-number">1.2.1.</span> <span class="toc-text">Online Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Map-Reduce-and-Data-Parallelism"><span class="toc-number">1.2.2.</span> <span class="toc-text">Map Reduce and Data Parallelism</span></a></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        å¤§æ•°æ®æœºå™¨å­¦ä¹ 
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">clownote</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-12-17T16:17:03.000Z" itemprop="datePublished">2019-12-17</time>
        
        (Updated: <time datetime="2021-02-21T06:58:37.445Z" itemprop="dateModified">2021-02-21</time>)
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a> â€º <a class="category-link" href="/categories/Machine-Learning/AndrewNg/">AndrewNg</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="Notes-of-Andrew-Ngâ€™s-Machine-Learning-â€”â€”-17-Large-Scale-Machine-Learning"><a href="#Notes-of-Andrew-Ngâ€™s-Machine-Learning-â€”â€”-17-Large-Scale-Machine-Learning" class="headerlink" title="Notes of Andrew Ngâ€™s Machine Learning â€”â€” (17) Large Scale Machine Learning"></a>Notes of Andrew Ngâ€™s Machine Learning â€”â€” (17) Large Scale Machine Learning</h1><h2 id="Gradient-Descent-with-Large-Datasets"><a href="#Gradient-Descent-with-Large-Datasets" class="headerlink" title="Gradient Descent with Large Datasets"></a>Gradient Descent with Large Datasets</h2><h3 id="Learning-With-Large-Datasets"><a href="#Learning-With-Large-Datasets" class="headerlink" title="Learning With Large Datasets"></a>Learning With Large Datasets</h3><p>When learning with large datasets, it can be high cost to do a gradient descent. So before we actually use this large datasets (say, $m = 100,000,000$) for model training, weâ€™d better try to randomly pick a small part of data ($m=1,000$) from it and learning from it. If this subset works already satisfying, weâ€™ll no longer need to do a learning with large dataset.</p>
<p>Suppose you are facing a supervised learning problem and have a very large dataset (m = 100,000,000). How can you tell if using all of the data is likely to perform much better than using a small subset of the data (say m = 1,000)?</p>
<p>What er can do is plotting  a learning curve for a range of values of m and verify that the algorithm has high variance when m is small.</p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1g9rn4jr6zxj30om07r0vl.jpg" alt="image-20191210150555045"></p>
<p>Facing a left-liked curve, maybe we use a large dataset can offer a better model. But if we get the curve like the right one, use a $m&gt;1000$ can not actually improve our model effectively.</p>
<h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h3><p>Before talking about the Stochastic Gradient Descent, letâ€™s take a look at the gradient descent we used (Actually, it is called <em>Batch Gradient Descent</em>), A linear regression for example:</p>
<blockquote>
<p>$J_{\textrm{train}}(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$ </p>
<p>Repeat {</p>
<p>â€‹        $\quad \theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$    (for every $j=0,\cdots,n$)</p>
<p>}</p>
</blockquote>
<p>With this Batch Gradient Descent, every step we take will need to calculate the sum of derivatives of all data. If our dataset is large, the time cost will be very expensive.</p>
<p>To deal with a large dataset, here is the <em>Stochastic Gradient Descent</em>:</p>
<p>$$<br>\begin{array}{l}<br>cost\left(\theta,(x^{(i)},y^{(i)})\right)=\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2\\<br>J_{train}(\theta)=\frac{1}{m}\sum_{i=1}^{m}cost\left(\theta,(x^{(i)},y^{(i)})\right)\\</p>
<p>1.\quad \textrm{Randomly shuffle(reorder) training examples}\<br>2.\quad \textrm{Repeat } { \<br>\qquad\qquad \textrm{for $i:= 1, \cdots,m$ } { \<br>\qquad\qquad\qquad\quad \theta_j:=\theta_j-\alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \quad \textrm{(for every $j=0,\cdots,n$)} \<br>\qquad\qquad\qquad\quad } \<br>\qquad\qquad } \<br>\end{array}<br>$$<br>Notice that, instead of calculate the sum of all derivatives and change the whole theta, in the inner for loop, each step we only care about the cost of one theta. When the training set size m is very large, stochastic gradient descent can be much faster than gradient descent.</p>
<p>Learning rate $\alpha$ is typically held constant. Can slowly decrease $\alpha$ over time if we want $\theta$ to converge better. (E.g. $\alpha=\frac{CONST_1}{iterationNumber+CONST_2}$)</p>
<h3 id="Mini-Batch-Gradient-Descent"><a href="#Mini-Batch-Gradient-Descent" class="headerlink" title="Mini-Batch Gradient Descent"></a>Mini-Batch Gradient Descent</h3><ul>
<li>Batch gradient descent: Use all $m$ examples in each iteration</li>
<li>Stochastic gradient descent: Use $1$ example in each iteration</li>
<li>Mini-batch gradient descent: Use $b$ examples in each iteration ($b\in[2, 100]$, Usual $10$)</li>
</ul>
<p>Mini-batch gradient descent sometimes faster than stochastic gradient descent:<br>$$<br>\begin{array}{l}<br>\textrm{Say $b=10,m=1000$.}\</p>
<p>\textrm{Repeat } { \<br>\qquad \textrm{for $i:= 1, 11, 21, 31, \cdots,991$ } { \<br>\qquad\qquad\quad \theta_j:=\theta_j-\alpha\frac{1}{10}\sum_{k=i}^{i+9}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \quad \textrm{(for every $j=0,\cdots,n$)} \<br>\qquad\qquad\quad } \<br>} \<br>\end{array}<br>$$</p>
<h3 id="Stochastic-Gradient-Descent-Convergence"><a href="#Stochastic-Gradient-Descent-Convergence" class="headerlink" title="Stochastic Gradient Descent Convergence"></a>Stochastic Gradient Descent Convergence</h3><h4 id="Checking-for-convergence"><a href="#Checking-for-convergence" class="headerlink" title="Checking for convergence"></a>Checking for convergence</h4><ul>
<li><p><strong>Batch gradient descent</strong>:</p>
<p>Plot $J_{train}(\theta)$ as a function of the number of iterations of gradient descent.</p>
<p>$J_{train}(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$</p>
</li>
<li><p><strong>Stochastic gradient descent</strong>:</p>
<p>$cost(\theta,(x^{(i)},y^{(i)}))=\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$</p>
<p>During learning, compute $cost(\theta,(x^{(i)},y^{(i)}))$ before updating $\theta$ using $(x^{(i)},y^{(i)})$.</p>
<p>Every 1000 iterations, plot $cost(\theta,(x^{(i)},y^{(i)}))$ averaged over the last 1000 examples processed by algorithm. (If we get a plot that is too noisy to see itâ€™s converging or not, try ploting for every 5000 (or summat bigger than 1000) iterations rather than 1000)</p>
</li>
</ul>
<h2 id="Advanced-Topics"><a href="#Advanced-Topics" class="headerlink" title="Advanced Topics"></a>Advanced Topics</h2><h3 id="Online-Learning"><a href="#Online-Learning" class="headerlink" title="Online Learning"></a>Online Learning</h3><p>An online learning algorithm allows us to learn from a continuous stream of data (have no fixed dataset),  since we use each example once then no longer need to process it again. Another advantage of online learning is also that if we have a changing pool of users, or if the things weâ€™re trying to predict are slowly changing, the online learning algorithm can slowly adapt our learned hypothesis to whatever the latest sets of user behaviors are like as well.</p>
<p>Here is an example:</p>
<blockquote>
<p>Shipping service website where user comes, specifies origin and destination, you offer to ship their package for some asking price, and users sometimes choose to use your shipping service ($y=1$), sometimes not ($y=0$).</p>
<p>Features $x$ capture properties of user, of origin/destination and asking price. We want to learn $p(y=1|x;\theta)$ to optimize price.</p>
</blockquote>
<p>Here is how an online learning version of logistic regression work for this problem:<br>$$<br>\begin{array}{l}<br>\textrm{Repeat forever } {\<br>\qquad \textrm{Get $(x,y)$ corresponding to user.}\<br>\qquad \textrm{Update $\theta$ using $(x,y)$:}\<br>\qquad \qquad \theta_j:=\theta_j-\alpha(h_\theta(x)-y)\cdot x_j (j=0,\cdots,n) \<br>}<br>\end{array}<br>$$</p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwly1g9ywywm9hgj30yg0ih79d.jpg" alt="image-20191216220518514"></p>
<h3 id="Map-Reduce-and-Data-Parallelism"><a href="#Map-Reduce-and-Data-Parallelism" class="headerlink" title="Map Reduce and Data Parallelism"></a>Map Reduce and Data Parallelism</h3><p>We can divide up batch gradient descent and dispatch the cost function for a subset of the data to many different machines so that we can train our algorithm in parallel.</p>
<p>You can split your training set into z subsets corresponding to the number of machines you have. On each of those machines calculate $\displaystyle \sum_{i=p}^{q}(h_{\theta}(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}$, where weâ€™ve split the data starting at p and ending at q.</p>
<p>MapReduce will take all these dispatched (or â€˜mappedâ€™) jobs and â€˜reduceâ€™ them by calculating:</p>
<p>$\Theta_j := \Theta_j - \alpha \dfrac{1}{z}(temp_j^{(1)} + temp_j^{(2)} + \cdots + temp_j^{(z)})$</p>
<p>For all $j=0,â€¦,n$.</p>
<p>This is simply taking the computed cost from all the machines, calculating their average, multiplying by the learning rate, and updating theta.</p>
<p>Your learning algorithm is MapReduceable if it can be <em>expressed as computing sums of functions over the training set</em>. Linear regression and logistic regression are easily parallelizable.</p>
<p>For neural networks, you can compute forward propagation and back propagation on subsets of your data on many machines. Those machines can report their derivatives back to a â€˜masterâ€™ server that will combine them.</p>

  </div>
</article>
<!--Disqus-->


<!--Livere-->

    <div class="blog-post-comments">
        <div id="lv-container" data-id="city" data-uid="MTAyMC80NjEzMi8yMjY0Mw==">
            <noscript>ä¸å¯ç”¨ JavaScript æ”¯æŒçš„äººæ˜¯çœ‹ä¸åˆ°å¯çˆ±çš„è¯„è®ºåŒºçš„ã€‚ğŸ˜¥</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">é¦–é¡µ</a></li>
         
          <li><a href="/about/">å…³äº</a></li>
         
          <li><a href="/archives/">å½’æ¡£</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">é¡¹ç›®</a></li>
         
          <li><a href="/search/">æœç´¢</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Notes-of-Andrew-Ng%E2%80%99s-Machine-Learning-%E2%80%94%E2%80%94-17-Large-Scale-Machine-Learning"><span class="toc-number">1.</span> <span class="toc-text">Notes of Andrew Ngâ€™s Machine Learning â€”â€” (17) Large Scale Machine Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent-with-Large-Datasets"><span class="toc-number">1.1.</span> <span class="toc-text">Gradient Descent with Large Datasets</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Learning-With-Large-Datasets"><span class="toc-number">1.1.1.</span> <span class="toc-text">Learning With Large Datasets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-Gradient-Descent"><span class="toc-number">1.1.2.</span> <span class="toc-text">Stochastic Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-Batch-Gradient-Descent"><span class="toc-number">1.1.3.</span> <span class="toc-text">Mini-Batch Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-Gradient-Descent-Convergence"><span class="toc-number">1.1.4.</span> <span class="toc-text">Stochastic Gradient Descent Convergence</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Checking-for-convergence"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">Checking for convergence</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Advanced-Topics"><span class="toc-number">1.2.</span> <span class="toc-text">Advanced Topics</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Online-Learning"><span class="toc-number">1.2.1.</span> <span class="toc-text">Online Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Map-Reduce-and-Data-Parallelism"><span class="toc-number">1.2.2.</span> <span class="toc-text">Map Reduce and Data Parallelism</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&text=å¤§æ•°æ®æœºå™¨å­¦ä¹ "><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=å¤§æ•°æ®æœºå™¨å­¦ä¹ "><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&is_video=false&description=å¤§æ•°æ®æœºå™¨å­¦ä¹ "><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=å¤§æ•°æ®æœºå™¨å­¦ä¹ &body=Check out this article: https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=å¤§æ•°æ®æœºå™¨å­¦ä¹ "><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=å¤§æ•°æ®æœºå™¨å­¦ä¹ "><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=å¤§æ•°æ®æœºå™¨å­¦ä¹ "><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=å¤§æ•°æ®æœºå™¨å­¦ä¹ "><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&name=å¤§æ•°æ®æœºå™¨å­¦ä¹ &description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> èœå•</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> ç›®å½•</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> åˆ†äº«</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> è¿”å›é¡¶éƒ¨</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2021 CDFMLR
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">é¦–é¡µ</a></li>
         
          <li><a href="/about/">å…³äº</a></li>
         
          <li><a href="/archives/">å½’æ¡£</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">é¡¹ç›®</a></li>
         
          <li><a href="/search/">æœç´¢</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"å¤åˆ¶åˆ°ç²˜è´´æ¿!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "å¤åˆ¶æˆåŠŸ!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-146911386-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?9a0d2e6fde93dad496ac79f04f3aba97";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->


<!--Livere Comments-->

    <script type="text/javascript">
      (function (d, s) {
        var j, e = d.getElementsByTagName(s)[0];

        if (typeof LivereTower === 'function') { return; }

        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;

        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>

</body>
</html>
