<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Deep Learning with Python这篇文章是我学习《Deep Learning with Python》(第二版，François Chollet 著) 时写的系列笔记之一。文章的内容是从  Jupyter notebooks 转成 Markdown 的，你可以去 GitHub 或 Gitee 找到原始的 .ipynb 笔记本。 你可以去这个网站在线阅读这本书的正版原文(英文)。这">
<meta property="og:type" content="article">
<meta property="og:title" content="Python深度学习之用卷积神经网络处理序列">
<meta property="og:url" content="https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/index.html">
<meta property="og:site_name" content="clownote">
<meta property="og:description" content="Deep Learning with Python这篇文章是我学习《Deep Learning with Python》(第二版，François Chollet 著) 时写的系列笔记之一。文章的内容是从  Jupyter notebooks 转成 Markdown 的，你可以去 GitHub 或 Gitee 找到原始的 .ipynb 笔记本。 你可以去这个网站在线阅读这本书的正版原文(英文)。这">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghmwvss1ibj317a0msn02.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghq3bm3j93j30af07c3yq.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghq3bkuud0j30af07c0sy.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghq3bl7gimj30al07caac.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghmy56fkp4j30ji0ncgni.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghq3blng5dj30al07c3yt.jpg">
<meta property="article:published_time" content="2020-08-14T09:41:21.000Z">
<meta property="article:modified_time" content="2021-04-03T05:20:57.744Z">
<meta property="article:author" content="CDFMLR">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghmwvss1ibj317a0msn02.jpg">
    
    
        
          
              <link rel="shortcut icon" href="/images/rabbit.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/rabbit_192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/rabbit_180.png">
          
        
    
    <!-- title -->
    <title>Python深度学习之用卷积神经网络处理序列</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
    <!--Google search varification (PRIVATE)-->
    <meta name="google-site-verification" content="MrqlpFAD8nDanw3Ypv7ZsIWHLnTdhRuLa4QhSVwxIvc" />
    <!--Google AdSense 关联 (PRIVATE)-->
    <script data-ad-client="ca-pub-1510963483941114" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<meta name="generator" content="Hexo 5.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2020/08/17/DeepLearningWithPython/Deep-Learning%20with-Python-ch7_1/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2020/08/13/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_3/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&text=Python深度学习之用卷积神经网络处理序列"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&title=Python深度学习之用卷积神经网络处理序列"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&is_video=false&description=Python深度学习之用卷积神经网络处理序列"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Python深度学习之用卷积神经网络处理序列&body=Check out this article: https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&title=Python深度学习之用卷积神经网络处理序列"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&title=Python深度学习之用卷积神经网络处理序列"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&title=Python深度学习之用卷积神经网络处理序列"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&title=Python深度学习之用卷积神经网络处理序列"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&name=Python深度学习之用卷积神经网络处理序列&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Deep-Learning-with-Python"><span class="toc-number">1.</span> <span class="toc-text">Deep Learning with Python</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-Sequence-processing-with-convnets"><span class="toc-number">1.1.</span> <span class="toc-text">6.4 Sequence processing with convnets</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E7%BB%B4%E5%8D%B7%E7%A7%AF%E3%80%81%E6%B1%A0%E5%8C%96"><span class="toc-number">1.1.1.</span> <span class="toc-text">序列数据的一维卷积、池化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%B8%80%E7%BB%B4%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.2.</span> <span class="toc-text">实现一维卷积神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E5%90%88-CNN-%E5%92%8C-RNN-%E5%A4%84%E7%90%86%E9%95%BF%E5%BA%8F%E5%88%97"><span class="toc-number">1.1.3.</span> <span class="toc-text">结合 CNN 和 RNN 处理长序列</span></a></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Python深度学习之用卷积神经网络处理序列
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">clownote</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2020-08-14T09:41:21.000Z" itemprop="datePublished">2020-08-14</time>
        
        (Updated: <time datetime="2021-04-03T05:20:57.744Z" itemprop="dateModified">2021-04-03</time>)
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a> › <a class="category-link" href="/categories/Machine-Learning/Deep-Learning-with-Python/">Deep Learning with Python</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a>, <a class="tag-link-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="Deep-Learning-with-Python"><a href="#Deep-Learning-with-Python" class="headerlink" title="Deep Learning with Python"></a>Deep Learning with Python</h1><p>这篇文章是我学习《Deep Learning with Python》(第二版，François Chollet 著) 时写的系列笔记之一。文章的内容是从  Jupyter notebooks 转成 Markdown 的，你可以去 <a target="_blank" rel="noopener" href="https://github.com/cdfmlr/Deep-Learning-with-Python-Notebooks">GitHub</a> 或 <a target="_blank" rel="noopener" href="https://gitee.com/cdfmlr/Deep-Learning-with-Python-Notebooks">Gitee</a> 找到原始的 <code>.ipynb</code> 笔记本。</p>
<p>你可以去<a target="_blank" rel="noopener" href="https://livebook.manning.com/book/deep-learning-with-python">这个网站在线阅读这本书的正版原文</a>(英文)。这本书的作者也给出了配套的 <a target="_blank" rel="noopener" href="https://github.com/fchollet/deep-learning-with-python-notebooks">Jupyter notebooks</a>。</p>
<p>本文为 <strong>第6章  深度学习用于文本和序列</strong> (Chapter 6. <em>Deep learning for text and sequences</em>) 的笔记。</p>
<p>[TOC]</p>
<h2 id="6-4-Sequence-processing-with-convnets"><a href="#6-4-Sequence-processing-with-convnets" class="headerlink" title="6.4 Sequence processing with convnets"></a>6.4 Sequence processing with convnets</h2><blockquote>
<p>用卷积神经网络处理序列</p>
</blockquote>
<p>卷积神经网络可以有效利用数据，提取局部特征，将表示模块化。由于这种特效，CNN 不但善于处理计算机时间问题，也可以高效处理序列问题，在有些序列问题上，CNN 的效果、效率甚至可以超过 RNN。</p>
<p>不同于处理图像用的二维 Conv2D，时间序列是一维的，所以要用一维卷积神经网络来处理。</p>
<h3 id="序列数据的一维卷积、池化"><a href="#序列数据的一维卷积、池化" class="headerlink" title="序列数据的一维卷积、池化"></a>序列数据的一维卷积、池化</h3><p>和二维的卷积类似，一维卷积从序列中提取局部的片段（子序列），然后对每个片段做相同的变换。一维的卷积窗口是时间轴上的一维窗口。该运算的性质可以保证，在前面某个位置学到的模式稍后可以在其他位置被识出来（具有时间平移不变性）。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghmwvss1ibj317a0msn02.jpg" alt="一维卷积神经网络的工作原理:每个输出时间步都是利用输入序列在时间维度上的一小段得到的"></p>
<p>一维的池化运算，也和二维池化运算类似：从输入中提取一维片段，输出其中的最大值(最大池化)或平均值(平均池化)。该运算也是用来降低数据的长度的(做子采样)。</p>
<h3 id="实现一维卷积神经网络"><a href="#实现一维卷积神经网络" class="headerlink" title="实现一维卷积神经网络"></a>实现一维卷积神经网络</h3><p>在 Keras 中，一维卷积神经网络用 Conv1D 层来表示。用法和 Conv2D 很类似，它接收形状为 <code>(samples, time, features)</code> 的输入，返回还是这个形状的。注意，它的窗口是在 time 上的，即输入的第二个轴。Conv2D 里我们的窗口一般用 3x3、5x5 这样的，对应的 Conv1D 里，我们一般取 7 或 9 的窗口大小。</p>
<p>常情况下，我们都将 Conv1D 层和 MaxPooling1D 层堆叠在一起，在所有卷积池化堆叠的最后，再用一个全局池化运算或展平的操作。</p>
<p>还是以 IMDB 为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"></span><br><span class="line">max_features = <span class="number">10000</span></span><br><span class="line">max_len = <span class="number">500</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Loading data...&#x27;</span>)</span><br><span class="line">(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)</span><br><span class="line">print(<span class="built_in">len</span>(x_train), <span class="string">&#x27;train sequences&#x27;</span>)</span><br><span class="line">print(<span class="built_in">len</span>(x_test), <span class="string">&#x27;test sequences&#x27;</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Pad sequences (samples x time)&#x27;</span>)</span><br><span class="line">x_train = sequence.pad_sequences(x_train, maxlen=max_len)</span><br><span class="line">x_test = sequence.pad_sequences(x_test, maxlen=max_len)</span><br><span class="line">print(<span class="string">&#x27;x_train shape:&#x27;</span>, x_train.shape)</span><br><span class="line">print(<span class="string">&#x27;x_test shape:&#x27;</span>, x_test.shape)</span><br></pre></td></tr></table></figure>
<pre><code>Loading data...
25000 train sequences
25000 test sequences
Pad sequences (samples x time)
x_train shape: (25000, 500)
x_test shape: (25000, 500)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制历史</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_acc_and_loss</span>(<span class="params">history</span>):</span></span><br><span class="line"></span><br><span class="line">    epochs = <span class="built_in">range</span>(<span class="built_in">len</span>(history.history[<span class="string">&#x27;loss&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        acc = history.history[<span class="string">&#x27;acc&#x27;</span>]</span><br><span class="line">        val_acc = history.history[<span class="string">&#x27;val_acc&#x27;</span>]</span><br><span class="line">        </span><br><span class="line">        plt.plot(epochs, acc, <span class="string">&#x27;bo-&#x27;</span>, label=<span class="string">&#x27;Training acc&#x27;</span>)</span><br><span class="line">        plt.plot(epochs, val_acc, <span class="string">&#x27;rs-&#x27;</span>, label=<span class="string">&#x27;Validation acc&#x27;</span>)</span><br><span class="line">        plt.title(<span class="string">&#x27;Training and validation accuracy&#x27;</span>)</span><br><span class="line">        plt.legend()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">&#x27;No acc. Skip&#x27;</span>)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        plt.figure()</span><br><span class="line"></span><br><span class="line">    loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">    val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    plt.plot(epochs, loss, <span class="string">&#x27;bo-&#x27;</span>, label=<span class="string">&#x27;Training loss&#x27;</span>)</span><br><span class="line">    plt.plot(epochs, val_loss, <span class="string">&#x27;rs-&#x27;</span>, label=<span class="string">&#x27;Validation loss&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Training and validation loss&#x27;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 IMDB 上训练并评估一个简单的一维卷积神经网络</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.Embedding(max_features, <span class="number">128</span>, input_length=max_len))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv1D(<span class="number">32</span>, <span class="number">7</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.MaxPooling1D(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv1D(<span class="number">32</span>, <span class="number">7</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.GlobalMaxPooling1D())</span><br><span class="line"></span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=RMSprop(lr=<span class="number">1e-4</span>),</span><br><span class="line">              loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">              metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line">history = model.fit(x_train, y_train,</span><br><span class="line">                    epochs=<span class="number">10</span>,</span><br><span class="line">                    batch_size=<span class="number">128</span>,</span><br><span class="line">                    validation_split=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">plot_acc_and_loss(history)</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 500, 128)          1280000   
_________________________________________________________________
conv1d (Conv1D)              (None, 494, 32)           28704     
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 98, 32)            0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 92, 32)            7200      
_________________________________________________________________
global_max_pooling1d (Global (None, 32)                0         
_________________________________________________________________
dense (Dense)                (None, 1)                 33        
=================================================================
Total params: 1,315,937
Trainable params: 1,315,937
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
157/157 [==============================] - 30s 188ms/step - loss: 0.9049 - acc: 0.5124 - val_loss: 0.6875 - val_acc: 0.5566
Epoch 2/10
157/157 [==============================] - 28s 178ms/step - loss: 0.6724 - acc: 0.6433 - val_loss: 0.6699 - val_acc: 0.6394
...
Epoch 9/10
157/157 [==============================] - 27s 174ms/step - loss: 0.2445 - acc: 0.9171 - val_loss: 0.4238 - val_acc: 0.8666
Epoch 10/10
157/157 [==============================] - 27s 170ms/step - loss: 0.2211 - acc: 0.9277 - val_loss: 0.4305 - val_acc: 0.8746</code></pre>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghq3bm3j93j30af07c3yq.jpg" alt="png"></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghq3bkuud0j30af07c0sy.jpg" alt="png"></p>
<p>虽然结果略比 RNN 差，但还是挺不错的，而且训练起来比 LSTM 快。</p>
<h3 id="结合-CNN-和-RNN-处理长序列"><a href="#结合-CNN-和-RNN-处理长序列" class="headerlink" title="结合 CNN 和 RNN 处理长序列"></a>结合 CNN 和 RNN 处理长序列</h3><p>一维卷积神经网络是把序列分成片段去学习的，对时间顺序不敏感。所以对于那些序列的顺序影响重大的问题来说，CNN 表现的远不如 RNN。例如，耶拿数据集(气温预测)问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">data_dir = <span class="string">&quot;/Volumes/WD/Files/dataset/jena_climate&quot;</span></span><br><span class="line">fname = os.path.join(data_dir, <span class="string">&#x27;jena_climate_2009_2016.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">f = <span class="built_in">open</span>(fname)</span><br><span class="line">data = f.read()</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line">lines = data.split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">header = lines[<span class="number">0</span>].split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">lines = lines[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">float_data = np.zeros((<span class="built_in">len</span>(lines), <span class="built_in">len</span>(header) - <span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(lines):</span><br><span class="line">    values = [<span class="built_in">float</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> line.split(<span class="string">&#x27;,&#x27;</span>)[<span class="number">1</span>:]]</span><br><span class="line">    float_data[i, :] = values</span><br><span class="line">    </span><br><span class="line">mean = float_data[:<span class="number">200000</span>].mean(axis=<span class="number">0</span>)</span><br><span class="line">float_data -= mean</span><br><span class="line">std = float_data[:<span class="number">200000</span>].std(axis=<span class="number">0</span>)</span><br><span class="line">float_data /= std</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span>(<span class="params">data, lookback, delay, min_index, max_index,</span></span></span><br><span class="line"><span class="function"><span class="params">              shuffle=<span class="literal">False</span>, batch_size=<span class="number">128</span>, step=<span class="number">6</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> max_index <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        max_index = <span class="built_in">len</span>(data) - delay - <span class="number">1</span></span><br><span class="line">    i = min_index + lookback</span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> shuffle:</span><br><span class="line">            rows = np.random.randint(</span><br><span class="line">                min_index + lookback, max_index, size=batch_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> i + batch_size &gt;= max_index:</span><br><span class="line">                i = min_index + lookback</span><br><span class="line">            rows = np.arange(i, <span class="built_in">min</span>(i + batch_size, max_index))</span><br><span class="line">            i += <span class="built_in">len</span>(rows)</span><br><span class="line"></span><br><span class="line">        samples = np.zeros((<span class="built_in">len</span>(rows),</span><br><span class="line">                           lookback // step,</span><br><span class="line">                           data.shape[-<span class="number">1</span>]))</span><br><span class="line">        targets = np.zeros((<span class="built_in">len</span>(rows),))</span><br><span class="line">        <span class="keyword">for</span> j, row <span class="keyword">in</span> <span class="built_in">enumerate</span>(rows):</span><br><span class="line">            indices = <span class="built_in">range</span>(rows[j] - lookback, rows[j], step)</span><br><span class="line">            samples[j] = data[indices]</span><br><span class="line">            targets[j] = data[rows[j] + delay][<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> samples, targets</span><br><span class="line">        </span><br><span class="line">lookback = <span class="number">1440</span></span><br><span class="line">step = <span class="number">6</span></span><br><span class="line">delay = <span class="number">144</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">train_gen = generator(float_data,</span><br><span class="line">                      lookback=lookback,</span><br><span class="line">                      delay=delay,</span><br><span class="line">                      min_index=<span class="number">0</span>,</span><br><span class="line">                      max_index=<span class="number">200000</span>,</span><br><span class="line">                      shuffle=<span class="literal">True</span>,</span><br><span class="line">                      step=step, </span><br><span class="line">                      batch_size=batch_size)</span><br><span class="line">val_gen = generator(float_data,</span><br><span class="line">                    lookback=lookback,</span><br><span class="line">                    delay=delay,</span><br><span class="line">                    min_index=<span class="number">200001</span>,</span><br><span class="line">                    max_index=<span class="number">300000</span>,</span><br><span class="line">                    step=step,</span><br><span class="line">                    batch_size=batch_size)</span><br><span class="line">test_gen = generator(float_data,</span><br><span class="line">                     lookback=lookback,</span><br><span class="line">                     delay=delay,</span><br><span class="line">                     min_index=<span class="number">300001</span>,</span><br><span class="line">                     max_index=<span class="literal">None</span>,</span><br><span class="line">                     step=step,</span><br><span class="line">                     batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">val_steps = (<span class="number">300000</span> - <span class="number">200001</span> - lookback) // batch_size</span><br><span class="line"></span><br><span class="line">test_steps = (<span class="built_in">len</span>(float_data) - <span class="number">300001</span> - lookback) // batch_size</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在耶拿数据集上训练并评估一个简单的一维卷积神经网络</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.Conv1D(<span class="number">32</span>, <span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">                        input_shape=(<span class="literal">None</span>, float_data.shape[-<span class="number">1</span>])))</span><br><span class="line">model.add(layers.MaxPooling1D(<span class="number">3</span>))</span><br><span class="line">model.add(layers.Conv1D(<span class="number">32</span>, <span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.MaxPooling1D(<span class="number">3</span>))</span><br><span class="line">model.add(layers.Conv1D(<span class="number">32</span>, <span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.GlobalMaxPooling1D())</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=RMSprop(), loss=<span class="string">&#x27;mae&#x27;</span>)</span><br><span class="line">history = model.fit_generator(train_gen,</span><br><span class="line">                              steps_per_epoch=<span class="number">500</span>,</span><br><span class="line">                              epochs=<span class="number">20</span>,</span><br><span class="line">                              validation_data=val_gen,</span><br><span class="line">                              validation_steps=val_steps)</span><br><span class="line"></span><br><span class="line">plot_acc_and_loss(history)</span><br></pre></td></tr></table></figure>
<pre><code>WARNING:tensorflow:From &lt;ipython-input-6-02e34f317812&gt;:22: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
Please use Model.fit, which supports generators.
Epoch 1/20
500/500 [==============================] - 27s 54ms/step - loss: 0.4144 - val_loss: 0.4308
Epoch 2/20
500/500 [==============================] - 26s 52ms/step - loss: 0.3620 - val_loss: 0.4306
...
Epoch 19/20
500/500 [==============================] - 22s 43ms/step - loss: 0.2450 - val_loss: 0.4603
Epoch 20/20
500/500 [==============================] - 21s 41ms/step - loss: 0.2453 - val_loss: 0.4721</code></pre>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghq3bl7gimj30al07caac.jpg" alt="png"></p>
<p>这还不如咱用的那个常识法呢，可见顺序信息对这个问题还是非常关键的。为了学到顺序上的信息，同时保持卷积神经网络的速度和轻量，我们可以结合使用 CNN 和 RNN。</p>
<p>我们可以在 RNN 前面使用 Conv1D。对于非常长的序列 (比如包含上千个时间步)，直接用 RNN 处理起来太慢、甚至无法处理。在 RNN 前面加上一些 Conv1D 就可以把过长的输入序列转换(下采样)为由高级特征组成的更短序列，然后再用 RNN 去处理有可以学到顺序敏感的信息。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghmy56fkp4j30ji0ncgni.jpg" alt="结合一维 CNN 和 RNN 来处理长序列"></p>
<p>我们用这种方法再做一次气温预测问题，由于这种方法可以学习更长的序列，我们可以让网络查看更早的数据(增大数据生成器的 lookback 参数)，也可以让网络查看分辨率更高的时间序列（减小生成器的 step 参数）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">step = <span class="number">3</span>  <span class="comment"># 30 分钟一步，比以前缩短了一半</span></span><br><span class="line">lookback = <span class="number">720</span></span><br><span class="line">delay = <span class="number">144</span></span><br><span class="line"></span><br><span class="line">train_gen = generator(float_data,</span><br><span class="line">                      lookback=lookback,</span><br><span class="line">                      delay=delay,</span><br><span class="line">                      min_index=<span class="number">0</span>,</span><br><span class="line">                      max_index=<span class="number">200000</span>,</span><br><span class="line">                      shuffle=<span class="literal">True</span>,</span><br><span class="line">                      step=step)</span><br><span class="line"></span><br><span class="line">val_gen = generator(float_data,</span><br><span class="line">                    lookback=lookback,</span><br><span class="line">                    delay=delay,</span><br><span class="line">                    min_index=<span class="number">200001</span>,</span><br><span class="line">                    max_index=<span class="number">300000</span>,</span><br><span class="line">                    step=step)</span><br><span class="line"></span><br><span class="line">test_gen = generator(float_data,</span><br><span class="line">                     lookback=lookback,</span><br><span class="line">                     delay=delay,</span><br><span class="line">                     min_index=<span class="number">300001</span>,</span><br><span class="line">                     max_index=<span class="literal">None</span>,</span><br><span class="line">                     step=step)</span><br><span class="line"></span><br><span class="line">val_steps = (<span class="number">300000</span> - <span class="number">200001</span> - lookback) // <span class="number">128</span></span><br><span class="line">test_steps = (<span class="built_in">len</span>(float_data) - <span class="number">300001</span> - lookback) // <span class="number">128</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用 Conv1D + GRU</span></span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(layers.Conv1D(<span class="number">32</span>, <span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">                        input_shape=(<span class="literal">None</span>, float_data.shape[-<span class="number">1</span>])))</span><br><span class="line">model.add(layers.MaxPooling1D(<span class="number">3</span>))</span><br><span class="line">model.add(layers.Conv1D(<span class="number">32</span>, <span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.GRU(<span class="number">32</span>, dropout=<span class="number">0.1</span>, recurrent_dropout=<span class="number">0.5</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=RMSprop(), loss=<span class="string">&#x27;mae&#x27;</span>)</span><br><span class="line">history = model.fit_generator(train_gen,</span><br><span class="line">                              steps_per_epoch=<span class="number">500</span>,</span><br><span class="line">                              epochs=<span class="number">20</span>,</span><br><span class="line">                              validation_data=val_gen,</span><br><span class="line">                              validation_steps=val_steps)</span><br><span class="line"></span><br><span class="line">plot_acc_and_loss(history)</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_2&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_5 (Conv1D)            (None, None, 32)          2272      
_________________________________________________________________
max_pooling1d_3 (MaxPooling1 (None, None, 32)          0         
_________________________________________________________________
conv1d_6 (Conv1D)            (None, None, 32)          5152      
_________________________________________________________________
gru (GRU)                    (None, 32)                6336      
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 33        
=================================================================
Total params: 13,793
Trainable params: 13,793
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
500/500 [==============================] - 49s 97ms/step - loss: 0.3301 - val_loss: 0.3056
Epoch 2/20
500/500 [==============================] - 47s 95ms/step - loss: 0.2950 - val_loss: 0.2750
...
Epoch 19/20
500/500 [==============================] - 53s 106ms/step - loss: 0.2029 - val_loss: 0.3107
Epoch 20/20
500/500 [==============================] - 53s 106ms/step - loss: 0.2011 - val_loss: 0.3112</code></pre>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghq3blng5dj30al07c3yt.jpg" alt="png"></p>
<p>从验证损失来看，这种架构的效果不如只用正则化 GRU，但速度要快很多。它查看了两倍的数据量，在本例中可能不是非常有用，但对于其他数据集可能非常重要。</p>
<hr>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">By(<span class="string">&quot;CDFMLR&quot;</span>, <span class="string">&quot;2020-08-14&quot;</span>);</span><br></pre></td></tr></table></figure>

  </div>
</article>
<!--Disqus-->


<!--Livere-->

    <div class="blog-post-comments">
        <div id="lv-container" data-id="city" data-uid="MTAyMC80NjEzMi8yMjY0Mw==">
            <noscript>不启用 JavaScript 支持的人是看不到可爱的评论区的。😥</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Deep-Learning-with-Python"><span class="toc-number">1.</span> <span class="toc-text">Deep Learning with Python</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-Sequence-processing-with-convnets"><span class="toc-number">1.1.</span> <span class="toc-text">6.4 Sequence processing with convnets</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E7%BB%B4%E5%8D%B7%E7%A7%AF%E3%80%81%E6%B1%A0%E5%8C%96"><span class="toc-number">1.1.1.</span> <span class="toc-text">序列数据的一维卷积、池化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%B8%80%E7%BB%B4%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.2.</span> <span class="toc-text">实现一维卷积神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E5%90%88-CNN-%E5%92%8C-RNN-%E5%A4%84%E7%90%86%E9%95%BF%E5%BA%8F%E5%88%97"><span class="toc-number">1.1.3.</span> <span class="toc-text">结合 CNN 和 RNN 处理长序列</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&text=Python深度学习之用卷积神经网络处理序列"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&title=Python深度学习之用卷积神经网络处理序列"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&is_video=false&description=Python深度学习之用卷积神经网络处理序列"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Python深度学习之用卷积神经网络处理序列&body=Check out this article: https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&title=Python深度学习之用卷积神经网络处理序列"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&title=Python深度学习之用卷积神经网络处理序列"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&title=Python深度学习之用卷积神经网络处理序列"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&title=Python深度学习之用卷积神经网络处理序列"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2020/08/14/DeepLearningWithPython/Deep-Learning%20with-Python-ch6_4/&name=Python深度学习之用卷积神经网络处理序列&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2021 CDFMLR
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-146911386-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?9a0d2e6fde93dad496ac79f04f3aba97";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->


<!--Livere Comments-->

    <script type="text/javascript">
      (function (d, s) {
        var j, e = d.getElementsByTagName(s)[0];

        if (typeof LivereTower === 'function') { return; }

        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;

        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>

</body>
</html>
