<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Notes of Andrew Ng’s Machine Learning —— (17) Large Scale Machine LearningGradient Descent with Large DatasetsLearning With Large DatasetsWhen learning with large datasets, it can be high cost to do a">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据机器学习">
<meta property="og:url" content="https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/index.html">
<meta property="og:site_name" content="clownote">
<meta property="og:description" content="Notes of Andrew Ng’s Machine Learning —— (17) Large Scale Machine LearningGradient Descent with Large DatasetsLearning With Large DatasetsWhen learning with large datasets, it can be high cost to do a">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwgy1g9rn4jr6zxj30om07r0vl.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwly1g9ywywm9hgj30yg0ih79d.jpg">
<meta property="article:published_time" content="2019-12-17T16:17:03.000Z">
<meta property="article:modified_time" content="2021-02-21T06:58:37.445Z">
<meta property="article:author" content="CDFMLR">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/006tNbRwgy1g9rn4jr6zxj30om07r0vl.jpg">
    
    
        
          
              <link rel="shortcut icon" href="/images/rabbit.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/rabbit_192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/rabbit_180.png">
          
        
    
    <!-- title -->
    <title>大数据机器学习</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
    <!--Google search varification (PRIVATE)-->
    <meta name="google-site-verification" content="MrqlpFAD8nDanw3Ypv7ZsIWHLnTdhRuLa4QhSVwxIvc" />
    <!--Google AdSense 关联 (PRIVATE)-->
    <script data-ad-client="ca-pub-1510963483941114" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<meta name="generator" content="Hexo 5.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2020/01/16/AndrewNgML/AndrewNgMLNotes/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/12/10/blog/CSharpAlarm/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&text=大数据机器学习"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=大数据机器学习"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&is_video=false&description=大数据机器学习"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=大数据机器学习&body=Check out this article: https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=大数据机器学习"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=大数据机器学习"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=大数据机器学习"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=大数据机器学习"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&name=大数据机器学习&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Notes-of-Andrew-Ng%E2%80%99s-Machine-Learning-%E2%80%94%E2%80%94-17-Large-Scale-Machine-Learning"><span class="toc-number">1.</span> <span class="toc-text">Notes of Andrew Ng’s Machine Learning —— (17) Large Scale Machine Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent-with-Large-Datasets"><span class="toc-number">1.1.</span> <span class="toc-text">Gradient Descent with Large Datasets</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Learning-With-Large-Datasets"><span class="toc-number">1.1.1.</span> <span class="toc-text">Learning With Large Datasets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-Gradient-Descent"><span class="toc-number">1.1.2.</span> <span class="toc-text">Stochastic Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-Batch-Gradient-Descent"><span class="toc-number">1.1.3.</span> <span class="toc-text">Mini-Batch Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-Gradient-Descent-Convergence"><span class="toc-number">1.1.4.</span> <span class="toc-text">Stochastic Gradient Descent Convergence</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Checking-for-convergence"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">Checking for convergence</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Advanced-Topics"><span class="toc-number">1.2.</span> <span class="toc-text">Advanced Topics</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Online-Learning"><span class="toc-number">1.2.1.</span> <span class="toc-text">Online Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Map-Reduce-and-Data-Parallelism"><span class="toc-number">1.2.2.</span> <span class="toc-text">Map Reduce and Data Parallelism</span></a></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        大数据机器学习
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">clownote</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-12-17T16:17:03.000Z" itemprop="datePublished">2019-12-17</time>
        
        (Updated: <time datetime="2021-02-21T06:58:37.445Z" itemprop="dateModified">2021-02-21</time>)
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a> › <a class="category-link" href="/categories/Machine-Learning/AndrewNg/">AndrewNg</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="Notes-of-Andrew-Ng’s-Machine-Learning-——-17-Large-Scale-Machine-Learning"><a href="#Notes-of-Andrew-Ng’s-Machine-Learning-——-17-Large-Scale-Machine-Learning" class="headerlink" title="Notes of Andrew Ng’s Machine Learning —— (17) Large Scale Machine Learning"></a>Notes of Andrew Ng’s Machine Learning —— (17) Large Scale Machine Learning</h1><h2 id="Gradient-Descent-with-Large-Datasets"><a href="#Gradient-Descent-with-Large-Datasets" class="headerlink" title="Gradient Descent with Large Datasets"></a>Gradient Descent with Large Datasets</h2><h3 id="Learning-With-Large-Datasets"><a href="#Learning-With-Large-Datasets" class="headerlink" title="Learning With Large Datasets"></a>Learning With Large Datasets</h3><p>When learning with large datasets, it can be high cost to do a gradient descent. So before we actually use this large datasets (say, $m = 100,000,000$) for model training, we’d better try to randomly pick a small part of data ($m=1,000$) from it and learning from it. If this subset works already satisfying, we’ll no longer need to do a learning with large dataset.</p>
<p>Suppose you are facing a supervised learning problem and have a very large dataset (m = 100,000,000). How can you tell if using all of the data is likely to perform much better than using a small subset of the data (say m = 1,000)?</p>
<p>What er can do is plotting  a learning curve for a range of values of m and verify that the algorithm has high variance when m is small.</p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1g9rn4jr6zxj30om07r0vl.jpg" alt="image-20191210150555045"></p>
<p>Facing a left-liked curve, maybe we use a large dataset can offer a better model. But if we get the curve like the right one, use a $m&gt;1000$ can not actually improve our model effectively.</p>
<h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h3><p>Before talking about the Stochastic Gradient Descent, let’s take a look at the gradient descent we used (Actually, it is called <em>Batch Gradient Descent</em>), A linear regression for example:</p>
<blockquote>
<p>$J_{\textrm{train}}(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$ </p>
<p>Repeat {</p>
<p>​        $\quad \theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$    (for every $j=0,\cdots,n$)</p>
<p>}</p>
</blockquote>
<p>With this Batch Gradient Descent, every step we take will need to calculate the sum of derivatives of all data. If our dataset is large, the time cost will be very expensive.</p>
<p>To deal with a large dataset, here is the <em>Stochastic Gradient Descent</em>:</p>
<p>$$<br>\begin{array}{l}<br>cost\left(\theta,(x^{(i)},y^{(i)})\right)=\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2\\<br>J_{train}(\theta)=\frac{1}{m}\sum_{i=1}^{m}cost\left(\theta,(x^{(i)},y^{(i)})\right)\\</p>
<p>1.\quad \textrm{Randomly shuffle(reorder) training examples}\<br>2.\quad \textrm{Repeat } { \<br>\qquad\qquad \textrm{for $i:= 1, \cdots,m$ } { \<br>\qquad\qquad\qquad\quad \theta_j:=\theta_j-\alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \quad \textrm{(for every $j=0,\cdots,n$)} \<br>\qquad\qquad\qquad\quad } \<br>\qquad\qquad } \<br>\end{array}<br>$$<br>Notice that, instead of calculate the sum of all derivatives and change the whole theta, in the inner for loop, each step we only care about the cost of one theta. When the training set size m is very large, stochastic gradient descent can be much faster than gradient descent.</p>
<p>Learning rate $\alpha$ is typically held constant. Can slowly decrease $\alpha$ over time if we want $\theta$ to converge better. (E.g. $\alpha=\frac{CONST_1}{iterationNumber+CONST_2}$)</p>
<h3 id="Mini-Batch-Gradient-Descent"><a href="#Mini-Batch-Gradient-Descent" class="headerlink" title="Mini-Batch Gradient Descent"></a>Mini-Batch Gradient Descent</h3><ul>
<li>Batch gradient descent: Use all $m$ examples in each iteration</li>
<li>Stochastic gradient descent: Use $1$ example in each iteration</li>
<li>Mini-batch gradient descent: Use $b$ examples in each iteration ($b\in[2, 100]$, Usual $10$)</li>
</ul>
<p>Mini-batch gradient descent sometimes faster than stochastic gradient descent:<br>$$<br>\begin{array}{l}<br>\textrm{Say $b=10,m=1000$.}\</p>
<p>\textrm{Repeat } { \<br>\qquad \textrm{for $i:= 1, 11, 21, 31, \cdots,991$ } { \<br>\qquad\qquad\quad \theta_j:=\theta_j-\alpha\frac{1}{10}\sum_{k=i}^{i+9}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \quad \textrm{(for every $j=0,\cdots,n$)} \<br>\qquad\qquad\quad } \<br>} \<br>\end{array}<br>$$</p>
<h3 id="Stochastic-Gradient-Descent-Convergence"><a href="#Stochastic-Gradient-Descent-Convergence" class="headerlink" title="Stochastic Gradient Descent Convergence"></a>Stochastic Gradient Descent Convergence</h3><h4 id="Checking-for-convergence"><a href="#Checking-for-convergence" class="headerlink" title="Checking for convergence"></a>Checking for convergence</h4><ul>
<li><p><strong>Batch gradient descent</strong>:</p>
<p>Plot $J_{train}(\theta)$ as a function of the number of iterations of gradient descent.</p>
<p>$J_{train}(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$</p>
</li>
<li><p><strong>Stochastic gradient descent</strong>:</p>
<p>$cost(\theta,(x^{(i)},y^{(i)}))=\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$</p>
<p>During learning, compute $cost(\theta,(x^{(i)},y^{(i)}))$ before updating $\theta$ using $(x^{(i)},y^{(i)})$.</p>
<p>Every 1000 iterations, plot $cost(\theta,(x^{(i)},y^{(i)}))$ averaged over the last 1000 examples processed by algorithm. (If we get a plot that is too noisy to see it’s converging or not, try ploting for every 5000 (or summat bigger than 1000) iterations rather than 1000)</p>
</li>
</ul>
<h2 id="Advanced-Topics"><a href="#Advanced-Topics" class="headerlink" title="Advanced Topics"></a>Advanced Topics</h2><h3 id="Online-Learning"><a href="#Online-Learning" class="headerlink" title="Online Learning"></a>Online Learning</h3><p>An online learning algorithm allows us to learn from a continuous stream of data (have no fixed dataset),  since we use each example once then no longer need to process it again. Another advantage of online learning is also that if we have a changing pool of users, or if the things we’re trying to predict are slowly changing, the online learning algorithm can slowly adapt our learned hypothesis to whatever the latest sets of user behaviors are like as well.</p>
<p>Here is an example:</p>
<blockquote>
<p>Shipping service website where user comes, specifies origin and destination, you offer to ship their package for some asking price, and users sometimes choose to use your shipping service ($y=1$), sometimes not ($y=0$).</p>
<p>Features $x$ capture properties of user, of origin/destination and asking price. We want to learn $p(y=1|x;\theta)$ to optimize price.</p>
</blockquote>
<p>Here is how an online learning version of logistic regression work for this problem:<br>$$<br>\begin{array}{l}<br>\textrm{Repeat forever } {\<br>\qquad \textrm{Get $(x,y)$ corresponding to user.}\<br>\qquad \textrm{Update $\theta$ using $(x,y)$:}\<br>\qquad \qquad \theta_j:=\theta_j-\alpha(h_\theta(x)-y)\cdot x_j (j=0,\cdots,n) \<br>}<br>\end{array}<br>$$</p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwly1g9ywywm9hgj30yg0ih79d.jpg" alt="image-20191216220518514"></p>
<h3 id="Map-Reduce-and-Data-Parallelism"><a href="#Map-Reduce-and-Data-Parallelism" class="headerlink" title="Map Reduce and Data Parallelism"></a>Map Reduce and Data Parallelism</h3><p>We can divide up batch gradient descent and dispatch the cost function for a subset of the data to many different machines so that we can train our algorithm in parallel.</p>
<p>You can split your training set into z subsets corresponding to the number of machines you have. On each of those machines calculate $\displaystyle \sum_{i=p}^{q}(h_{\theta}(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}$, where we’ve split the data starting at p and ending at q.</p>
<p>MapReduce will take all these dispatched (or ‘mapped’) jobs and ‘reduce’ them by calculating:</p>
<p>$\Theta_j := \Theta_j - \alpha \dfrac{1}{z}(temp_j^{(1)} + temp_j^{(2)} + \cdots + temp_j^{(z)})$</p>
<p>For all $j=0,…,n$.</p>
<p>This is simply taking the computed cost from all the machines, calculating their average, multiplying by the learning rate, and updating theta.</p>
<p>Your learning algorithm is MapReduceable if it can be <em>expressed as computing sums of functions over the training set</em>. Linear regression and logistic regression are easily parallelizable.</p>
<p>For neural networks, you can compute forward propagation and back propagation on subsets of your data on many machines. Those machines can report their derivatives back to a ‘master’ server that will combine them.</p>

  </div>
</article>
<!--Disqus-->


<!--Livere-->

    <div class="blog-post-comments">
        <div id="lv-container" data-id="city" data-uid="MTAyMC80NjEzMi8yMjY0Mw==">
            <noscript>不启用 JavaScript 支持的人是看不到可爱的评论区的。😥</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Notes-of-Andrew-Ng%E2%80%99s-Machine-Learning-%E2%80%94%E2%80%94-17-Large-Scale-Machine-Learning"><span class="toc-number">1.</span> <span class="toc-text">Notes of Andrew Ng’s Machine Learning —— (17) Large Scale Machine Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent-with-Large-Datasets"><span class="toc-number">1.1.</span> <span class="toc-text">Gradient Descent with Large Datasets</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Learning-With-Large-Datasets"><span class="toc-number">1.1.1.</span> <span class="toc-text">Learning With Large Datasets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-Gradient-Descent"><span class="toc-number">1.1.2.</span> <span class="toc-text">Stochastic Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-Batch-Gradient-Descent"><span class="toc-number">1.1.3.</span> <span class="toc-text">Mini-Batch Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-Gradient-Descent-Convergence"><span class="toc-number">1.1.4.</span> <span class="toc-text">Stochastic Gradient Descent Convergence</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Checking-for-convergence"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">Checking for convergence</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Advanced-Topics"><span class="toc-number">1.2.</span> <span class="toc-text">Advanced Topics</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Online-Learning"><span class="toc-number">1.2.1.</span> <span class="toc-text">Online Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Map-Reduce-and-Data-Parallelism"><span class="toc-number">1.2.2.</span> <span class="toc-text">Map Reduce and Data Parallelism</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&text=大数据机器学习"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=大数据机器学习"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&is_video=false&description=大数据机器学习"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=大数据机器学习&body=Check out this article: https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=大数据机器学习"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=大数据机器学习"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=大数据机器学习"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&title=大数据机器学习"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/&name=大数据机器学习&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2021 CDFMLR
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-146911386-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?9a0d2e6fde93dad496ac79f04f3aba97";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->


<!--Livere Comments-->

    <script type="text/javascript">
      (function (d, s) {
        var j, e = d.getElementsByTagName(s)[0];

        if (typeof LivereTower === 'function') { return; }

        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;

        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>

</body>
</html>
