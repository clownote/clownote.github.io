<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="æœºå™¨å­¦ä¹ Emmmmï¼Œè¿™å­¦æœŸåœ¨ Coursera å­¦å®Œäº† Andrew Ng çš„ Machine Learning è¯¾ç¨‹ã€‚æˆ‘å¯¹è¿™ä¸ªè¯¾ç¨‹ä¸€å‘æ˜¯ä¸ä»¥ä¸ºæ„çš„ï¼Œå´ä¸å°å¿ƒæŠ¥äº†ä¸ªåï¼Œè¿˜æ‰‹è´±ç”³è¯·äº†ä¸ªç»æµæ´åŠ©ï¼Œå­¦å®Œå°±å¯ä»¥å…è´¹æ‹¿è¯ä¹¦ï¼ˆå–å‡ ç™¾å—å“’ï¼‰ï¼Œè¯¾ç¨‹æœŸé—´è¿˜é€æ­£ç‰ˆçš„ Matlab Onlineï¼Œè¿™ä¸€ç³»åˆ—çš„å¶(å )ç„¶(å°)äº‹(ä¾¿)ä»¶(å®œ)ä¿ƒä½¿æˆ‘å¼€å§‹åˆ·è¿™ä¸ªè¯¾äº†ã€‚è¶Šå­¦è¶Šè§‰å¾—ï¼Œå—¯ï¼ŒçœŸé¦™ï¼Œæ˜¯çœŸçš„å¾ˆé¦™ï¼è¿™ä¸ªè¯¾çœŸçš„æ˜¯å¾ˆå¥½çš„æœºå™¨å­¦ä¹ ">
<meta property="og:type" content="article">
<meta property="og:title" content="Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“">
<meta property="og:url" content="https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/index.html">
<meta property="og:site_name" content="clownote">
<meta property="og:description" content="æœºå™¨å­¦ä¹ Emmmmï¼Œè¿™å­¦æœŸåœ¨ Coursera å­¦å®Œäº† Andrew Ng çš„ Machine Learning è¯¾ç¨‹ã€‚æˆ‘å¯¹è¿™ä¸ªè¯¾ç¨‹ä¸€å‘æ˜¯ä¸ä»¥ä¸ºæ„çš„ï¼Œå´ä¸å°å¿ƒæŠ¥äº†ä¸ªåï¼Œè¿˜æ‰‹è´±ç”³è¯·äº†ä¸ªç»æµæ´åŠ©ï¼Œå­¦å®Œå°±å¯ä»¥å…è´¹æ‹¿è¯ä¹¦ï¼ˆå–å‡ ç™¾å—å“’ï¼‰ï¼Œè¯¾ç¨‹æœŸé—´è¿˜é€æ­£ç‰ˆçš„ Matlab Onlineï¼Œè¿™ä¸€ç³»åˆ—çš„å¶(å )ç„¶(å°)äº‹(ä¾¿)ä»¶(å®œ)ä¿ƒä½¿æˆ‘å¼€å§‹åˆ·è¿™ä¸ªè¯¾äº†ã€‚è¶Šå­¦è¶Šè§‰å¾—ï¼Œå—¯ï¼ŒçœŸé¦™ï¼Œæ˜¯çœŸçš„å¾ˆé¦™ï¼è¿™ä¸ªè¯¾çœŸçš„æ˜¯å¾ˆå¥½çš„æœºå™¨å­¦ä¹ ">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8n5s7zmffj30lv0ca40u.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwly1gauxvtzb5qj30ur0g8wgd.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwly1gauyq6oc10j306e04umx0.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwly1gauyt0wbq3j30mv08xwfh.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g78nj8fvy4j30d507agmp.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g7aopbltb1j30f0046dg2.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwgy1gax9ziqe3sj324i0nu0z9.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwgy1gaxa3eq7zuj30c5083gm3.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jljliww8j30o90dmwjt.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jmrwo86pj30ph0dwwm1.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jmzc2qq4j30ot0ditej.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jul8myufj30mv0cz0xu.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8n62mi54pj30lw0catay.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006tNbRwgy1gaxja4d68ej30cb0c3jro.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8ta88gafkj30oz0djaeo.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8bmwqbebxj30po0dgahc.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g9bi5bv4etj30nz06lac1.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g9ds8xkjatj30ob0bd0w0.jpg">
<meta property="article:published_time" content="2020-01-16T12:39:27.000Z">
<meta property="article:modified_time" content="2021-03-12T09:27:41.313Z">
<meta property="article:author" content="CDFMLR">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8n5s7zmffj30lv0ca40u.jpg">
    
    
        
          
              <link rel="shortcut icon" href="/images/rabbit.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/rabbit_192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/rabbit_180.png">
          
        
    
    <!-- title -->
    <title>Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
    <!--Google search varification (PRIVATE)-->
    <meta name="google-site-verification" content="MrqlpFAD8nDanw3Ypv7ZsIWHLnTdhRuLa4QhSVwxIvc" />
    <!--Google AdSense å…³è” (PRIVATE)-->
    <script data-ad-client="ca-pub-1510963483941114" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<meta name="generator" content="Hexo 5.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">é¦–é¡µ</a></li>
         
          <li><a href="/about/">å…³äº</a></li>
         
          <li><a href="/archives/">å½’æ¡£</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">é¡¹ç›®</a></li>
         
          <li><a href="/search/">æœç´¢</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2020/01/28/blog/a-swift-tour-cn-md/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/12/17/AndrewNgML/AndrewNg-MachineLearning-17-LargeScaleMachineLearning/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">ä¸Šä¸€ç¯‡</span>
      <span id="i-next" class="info" style="display:none;">ä¸‹ä¸€ç¯‡</span>
      <span id="i-top" class="info" style="display:none;">è¿”å›é¡¶éƒ¨</span>
      <span id="i-share" class="info" style="display:none;">åˆ†äº«æ–‡ç« </span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&text=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&title=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&is_video=false&description=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“&body=Check out this article: https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&title=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&title=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&title=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&title=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&name=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">æœºå™¨å­¦ä¹ </span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.</span> <span class="toc-text">ç›‘ç£å­¦ä¹ </span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.1.</span> <span class="toc-text">å›å½’é—®é¢˜</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#h-theta-x"><span class="toc-number">2.</span> <span class="toc-text">h_\theta(x)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sum-i-0-m-theta-ix-i"><span class="toc-number">3.</span> <span class="toc-text">\sum_{i&#x3D;0}^m \theta_ix_i</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#left-begin-array-c-x-0-x-1-vdots-x-n-end-array-right"><span class="toc-number">4.</span> <span class="toc-text">\left[\begin{array}{c}x_0 \ x_1 \ \vdots \ x_n \end{array}\right]</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">4.0.0.1.</span> <span class="toc-text">æ¢¯åº¦ä¸‹é™</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="toc-number">4.0.0.2.</span> <span class="toc-text">æ­£è§„æ–¹ç¨‹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8Dvs%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="toc-number">4.0.0.3.</span> <span class="toc-text">æ¢¯åº¦ä¸‹é™vsæ­£è§„æ–¹ç¨‹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%EF%BC%9A%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="toc-number">4.0.0.4.</span> <span class="toc-text">æ³¨ï¼šç‰¹å¾ç¼©æ”¾</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">4.0.1.</span> <span class="toc-text">åˆ†ç±»é—®é¢˜</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">4.0.1.1.</span> <span class="toc-text">é€»è¾‘å›å½’</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB"><span class="toc-number">4.0.1.1.1.</span> <span class="toc-text">å¤šå…ƒåˆ†ç±»</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%EF%BC%9A%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.0.1.2.</span> <span class="toc-text">æ³¨ï¼šè¿‡æ‹Ÿåˆ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">4.0.1.3.</span> <span class="toc-text">ç¥ç»ç½‘ç»œ</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB-1"><span class="toc-number">4.0.1.3.1.</span> <span class="toc-text">å¤šå…ƒåˆ†ç±»</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%8B%9F%E5%90%88"><span class="toc-number">4.0.1.3.2.</span> <span class="toc-text">ç¥ç»ç½‘ç»œçš„æ‹Ÿåˆ</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">4.0.1.4.</span> <span class="toc-text">æ”¯æŒå‘é‡æœº</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E6%96%B9%E6%B3%95"><span class="toc-number">4.0.1.4.1.</span> <span class="toc-text">æ ¸æ–¹æ³•</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#SVM-%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%A0%B8"><span class="toc-number">4.0.1.4.2.</span> <span class="toc-text">SVM ä¸­ä½¿ç”¨æ ¸</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB-2"><span class="toc-number">4.0.1.4.3.</span> <span class="toc-text">å¤šå…ƒåˆ†ç±»</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-vs-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-vs-SVM"><span class="toc-number">4.0.1.5.</span> <span class="toc-text">é€»è¾‘å›å½’ vs ç¥ç»ç½‘ç»œ vs SVM</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.1.</span> <span class="toc-text">æ— ç›‘ç£å­¦ä¹ </span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means-%E8%81%9A%E7%B1%BB"><span class="toc-number">4.1.1.</span> <span class="toc-text">K-Means èšç±»</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PCA-%E7%BB%B4%E6%95%B0%E7%BA%A6%E5%87%8F"><span class="toc-number">4.1.2.</span> <span class="toc-text">PCA ç»´æ•°çº¦å‡</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B"><span class="toc-number">4.1.3.</span> <span class="toc-text">å¼‚å¸¸æ£€æµ‹</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="toc-number">4.1.3.1.</span> <span class="toc-text">é«˜æ–¯åˆ†å¸ƒ</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BC%82%E5%B8%B8%E6%A3%80%E6%9F%A5%E7%AE%97%E6%B3%95"><span class="toc-number">4.1.3.1.1.</span> <span class="toc-text">å¼‚å¸¸æ£€æŸ¥ç®—æ³•</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="toc-number">4.1.3.2.</span> <span class="toc-text">å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%94%A8%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E5%BC%82%E5%B8%B8%E6%A3%80%E6%9F%A5"><span class="toc-number">4.1.3.2.1.</span> <span class="toc-text">ç”¨å¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„å¼‚å¸¸æ£€æŸ¥</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%A8%E6%A7%9B%E9%80%89%E6%8B%A9"><span class="toc-number">4.1.3.3.</span> <span class="toc-text">é—¨æ§›é€‰æ‹©</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="toc-number">4.1.4.</span> <span class="toc-text">æ¨èç³»ç»Ÿ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E6%8E%A8%E8%8D%90"><span class="toc-number">4.1.4.1.</span> <span class="toc-text">åŸºäºå†…å®¹æ¨è</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4"><span class="toc-number">4.1.4.2.</span> <span class="toc-text">ååŒè¿‡æ»¤</span></a></li></ol></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">clownote</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2020-01-16T12:39:27.000Z" itemprop="datePublished">2020-01-16</time>
        
        (Updated: <time datetime="2021-03-12T09:27:41.313Z" itemprop="dateModified">2021-03-12</time>)
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a> â€º <a class="category-link" href="/categories/Machine-Learning/AndrewNg/">AndrewNg</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="æœºå™¨å­¦ä¹ "><a href="#æœºå™¨å­¦ä¹ " class="headerlink" title="æœºå™¨å­¦ä¹ "></a>æœºå™¨å­¦ä¹ </h1><p>Emmmmï¼Œè¿™å­¦æœŸåœ¨ Coursera å­¦å®Œäº† Andrew Ng çš„ Machine Learning è¯¾ç¨‹ã€‚æˆ‘å¯¹è¿™ä¸ªè¯¾ç¨‹ä¸€å‘æ˜¯ä¸ä»¥ä¸ºæ„çš„ï¼Œå´ä¸å°å¿ƒæŠ¥äº†ä¸ªåï¼Œè¿˜æ‰‹è´±ç”³è¯·äº†ä¸ªç»æµæ´åŠ©ï¼Œå­¦å®Œå°±å¯ä»¥å…è´¹æ‹¿è¯ä¹¦ï¼ˆå–å‡ ç™¾å—å“’ï¼‰ï¼Œè¯¾ç¨‹æœŸé—´è¿˜é€æ­£ç‰ˆçš„ Matlab Onlineï¼Œè¿™ä¸€ç³»åˆ—çš„å¶(å )ç„¶(å°)äº‹(ä¾¿)ä»¶(å®œ)ä¿ƒä½¿æˆ‘å¼€å§‹åˆ·è¿™ä¸ªè¯¾äº†ã€‚è¶Šå­¦è¶Šè§‰å¾—ï¼Œå—¯ï¼ŒçœŸé¦™ï¼Œæ˜¯çœŸçš„å¾ˆé¦™ï¼è¿™ä¸ªè¯¾çœŸçš„æ˜¯å¾ˆå¥½çš„æœºå™¨å­¦ä¹ å…¥é—¨ï¼Œéš¾æ€ªé‚£ä¹ˆå¤šäººæ¨èã€‚</p>
<p>Coursera é‡Œè¯¾ç¨‹ç¬”è®°æœ‰æ¯ä¸€ç« çš„æ€»ç»“ï¼Œæ€»ç»“çš„éå¸¸å¥½ï¼Œæ¨èå­¦å®Œä¹‹åçœ‹ä¸€çœ‹ã€‚ä½†æˆ‘è¿˜æ˜¯å–œæ¬¢è‡ªå·±å†™è‡ªå·±çš„ï¼Œæ‰€ä»¥æˆ‘ä¹‹å‰è¾¹çœ‹è§†é¢‘è¾¹å†™äº†å‡ ä¹æ¶µç›–æ•´ä¸ªè¯¾ç¨‹çš„<a href="https://clownote.github.io/categories/Machine-Learning/AndrewNg/">ç¬”è®°</a>ï¼Œå…¶å®å¥½å¤šæ˜¯åœ¨æŠ„è€å¸ˆçš„åŸè¯å’ŒPPTğŸ˜‚ï¼Œå°±å½“ç»ƒä¹ æ‰“å­—ã€è‹±è¯­è¿˜æœ‰ $\LaTeX$ äº†ã€‚æ”¾å‡å›å®¶åœ¨ç«è½¦ä¸Š<del>ç™¾æ— èŠèµ–</del>å¿ƒè¡€æ¥æ½®ï¼Œæƒ³åˆ°äº†åº”è¯¥æ•´ç†ä¸€ä¸‹è¯¾ç¨‹é‡Œé¢å­¦åˆ°çš„ä¸œè¥¿ï¼Œå°±æœ‰äº†è¿™ç¯‡æ–‡ç« ã€‚</p>
<p>è¿™é‡Œæˆ‘ä¸»è¦æ˜¯å†™äº†å„ç§ç®—æ³•çš„æè¿°ï¼Œè¿˜ä»ç¼–ç¨‹ä½œä¸šé‡Œæå–äº†ç®—æ³•å¤§æ¦‚çš„ä»£ç å®ç°ï¼Œæ–¹ä¾¿æ—¥åå¿«é€ŸæŸ¥é˜…å§ã€‚ä¸€å¼€å§‹çš„å›å½’æ¯”è¾ƒç®€å•ï¼Œæ‰€ä»¥æˆ‘å†™çš„å¾ˆå°‘ï¼Œå°±å †äº†ç‚¹å…¬å¼ï¼ˆå…¶å®æ˜¯ç¡¬å§ä¸Šé“ºç©ºè°ƒå¤ªå†·è‡´ä½¿æˆ‘ç”Ÿç—…äº†ï¼Œæ€è·¯å µå¡å†™ä¸å‡ºä¸œè¥¿æ¥ğŸ˜·ï¼‰ï¼›åé¢SVMã€æ¨èç³»ç»Ÿä»€ä¹ˆçš„æ¯”è¾ƒå¤æ‚å°±å¤šå†™äº†ä¸€äº›ï¼ˆå…¶å®æ˜¯æˆ‘æŒæ¡çš„ä¸å¥½ï¼Œå½’çº³ä¸å‡ºé‡ç‚¹ğŸ¤¯ï¼‰ã€‚è‡³äºè¯¾ç¨‹é‡Œè€å¸ˆèŠ±å¤§åŠ›æ°”è®²çš„å…³äºæœºå™¨å­¦ä¹ ç³»ç»Ÿçš„è®¾è®¡ã€ä¼˜åŒ–ã€debug è¿˜æœ‰å„ç§<del>å¥‡æŠ€æ·«å·§</del> <del>éªšæ“ä½œ</del> å®ç”¨æŠ€å·§ ä»¥åŠ Octave å…¥é—¨å“ªä¸€å—æˆ‘å°±ä¸€æ¦‚ä¸æäº†ï¼Œè¿™äº›ä¸œè¥¿è¿˜æ˜¯è¦çœ‹è€å¸ˆçš„è§†é¢‘æ‰èƒ½ä½“ä¼šåˆ°ç²¾é«“ï¼ˆâ€¦ä¼˜(æŒ‚)ç§€(ç§‘)çš„å¤§å­¦ç”Ÿè‡ªç„¶æ˜¯ä¸èƒ½æ‰¿è®¤åŸå› æ˜¯è‡ªå·±æ‡’æƒ°çš„ğŸ˜ï¼‰ã€‚</p>
<p>ç”±äºæˆ‘çœ‹è¯¾ç¨‹çš„æ—¶å€™å…¨ç¨‹æ²¡æœ‰å¼€ä¸­æ–‡å­—å¹•ï¼Œå¹³æ—¶æŸ¥é˜…çš„ä¸­æ–‡èµ„æ–™ä¹Ÿæ¯”è¾ƒå°‘ï¼Œæ‰€ä»¥å¥½å¤šæœ¯è¯­æˆ‘éƒ½ä¸çŸ¥é“è¦æ€ä¹ˆç¿»è¯‘ï¼Œå†™è¿™ç¯‡æ–‡ç« çš„æ—¶å€™æˆ‘å¤§æ¦‚æŸ¥äº†ä¸€äº›è‡ªå·±éš¾ä»¥è¡¨è¾¾çš„ï¼Œå…¶ä»–çš„å…¨é è‡†æµ‹ï¼Œæˆ‘ä¸ä¿è¯æ­£ç¡®ã€‚</p>
<p>Emmmï¼Œä¸å°å¿ƒå°±å†™äº†å‡ ç™¾å­—çš„åºŸè¯ğŸ˜‚ä¸‹é¢å°±å¼€å§‹å§ã€‚</p>
<h2 id="ç›‘ç£å­¦ä¹ "><a href="#ç›‘ç£å­¦ä¹ " class="headerlink" title="ç›‘ç£å­¦ä¹ "></a>ç›‘ç£å­¦ä¹ </h2><p>ç›‘ç£å­¦ä¹ æ˜¯ç»™xã€yæ•°æ®å»è®­ç»ƒçš„ã€‚</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8n5s7zmffj30lv0ca40u.jpg" alt="image-20191105144318970"></p>
<h3 id="å›å½’é—®é¢˜"><a href="#å›å½’é—®é¢˜" class="headerlink" title="å›å½’é—®é¢˜"></a>å›å½’é—®é¢˜</h3><blockquote>
<p>åšé¢„æµ‹ï¼Œå€¼åŸŸä¸ºè¿ç»­çš„æ•°ï¼ˆä¾‹å¦‚åŒºé—´$[0,100]$ï¼‰</p>
</blockquote>
<p>æ•°å­¦æ¨¡å‹ï¼š</p>
<ul>
<li><p><strong>é¢„æµ‹å‡½æ•°</strong>ï¼š<br>$$</p>
<h1 id="h-theta-x"><a href="#h-theta-x" class="headerlink" title="h_\theta(x)"></a>h_\theta(x)</h1><h1 id="sum-i-0-m-theta-ix-i"><a href="#sum-i-0-m-theta-ix-i" class="headerlink" title="\sum_{i=0}^m \theta_ix_i"></a>\sum_{i=0}^m \theta_ix_i</h1><p>\left[\begin{array}{c}\theta_0 &amp; \theta_1 &amp; \ldots &amp; \theta_n\end{array}\right]</p>
<h1 id="left-begin-array-c-x-0-x-1-vdots-x-n-end-array-right"><a href="#left-begin-array-c-x-0-x-1-vdots-x-n-end-array-right" class="headerlink" title="\left[\begin{array}{c}x_0 \ x_1 \ \vdots \ x_n \end{array}\right]"></a>\left[\begin{array}{c}x_0 \ x_1 \ \vdots \ x_n \end{array}\right]</h1><p>\theta^TX<br>$$</p>
</li>
<li><p><strong>å¾…æ±‚å‚æ•°</strong>ï¼š$\theta=\left[\begin{array}{c}\theta_0 &amp; \theta_1 &amp; \ldots &amp; \theta_n\end{array}\right]$</p>
</li>
<li><p><strong>ä»£ä»·å‡½æ•°</strong>ï¼š<br>$$<br>J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h(x^{(i)})-y^{(i)})^2<br>=\frac{(\sum_{i=1}^mh_\theta(X)-Y)^2}{2m}<br>$$</p>
</li>
</ul>
<p>ğŸ‘‰ä»£ç å®ç°ï¼š</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">J</span> = <span class="title">computeCostMulti</span><span class="params">(X, y, theta)</span></span></span><br><span class="line"><span class="comment">%COMPUTECOSTMULTI Compute cost for linear regression with multiple variables</span></span><br><span class="line"><span class="comment">%   J = COMPUTECOSTMULTI(X, y, theta) computes the cost of using theta as the</span></span><br><span class="line"><span class="comment">%   parameter for linear regression to fit the data points in X and y</span></span><br><span class="line"></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line"></span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">J = <span class="number">1</span> / (<span class="number">2</span>*m) * (X*theta - y)&#x27; * (X*theta - y);</span><br><span class="line"></span><br><span class="line"><span class="comment">% or less vectorizedly: </span></span><br><span class="line"><span class="comment">% predictions = X * theta;</span></span><br><span class="line"><span class="comment">% sqrErrors = (predictions - y) .^ 2;</span></span><br><span class="line"><span class="comment">% J = 1 / (2*m) * sum(sqrErrors);</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>ä¼˜åŒ–ç›®æ ‡</strong>ï¼šæ‰¾åˆ°ä¸€ç»„$\theta$ä½¿$J$æœ€å°ã€‚</li>
</ul>
<p>æ±‚è§£æ–¹æ³•ï¼š</p>
<h4 id="æ¢¯åº¦ä¸‹é™"><a href="#æ¢¯åº¦ä¸‹é™" class="headerlink" title="æ¢¯åº¦ä¸‹é™"></a>æ¢¯åº¦ä¸‹é™</h4><p>ï¼ˆè¿™é‡Œæš‚ä¸”åªè®¨è®º batch gradient descentï¼‰</p>
<p>$$<br>\begin{array}{ll}<br>\textrm{repeat until convergence } { \<br>\qquad \theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)\<br>\qquad\quad:= \theta_j - \alpha \frac{1}{m} \sum^m_{i=1}[h_\theta(x^{(i)})-y^{(i)}] \cdot x_j^{(i)}\qquad \textrm{for }j:=0, â€¦, n \<br>}<br>\end{array}<br>$$</p>
<p>å‘é‡åŒ–è¡¨ç¤ºï¼š$\theta=\theta-\frac{\alpha}{m}X^T(h_\theta(X)-Y)$</p>
<p>ğŸ‘‰ä»£ç å®ç°ï¼š</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta, J_history]</span> = <span class="title">gradientDescentMulti</span><span class="params">(X, y, theta, alpha, num_iters)</span></span></span><br><span class="line"><span class="comment">%GRADIENTDESCENTMULTI Performs gradient descent to learn theta</span></span><br><span class="line"><span class="comment">%   theta = GRADIENTDESCENTMULTI(x, y, theta, alpha, num_iters) updates theta by</span></span><br><span class="line"><span class="comment">%   taking num_iters gradient steps with learning rate alpha</span></span><br><span class="line"></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line">J_history = <span class="built_in">zeros</span>(num_iters, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</span><br><span class="line"></span><br><span class="line">    predictions = X * theta;</span><br><span class="line">    errors = (predictions - y);</span><br><span class="line">    theta = theta - alpha / m * (X&#x27; * errors);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">% Save the cost J in every iteration    </span></span><br><span class="line">    J_history(iter) = computeCostMulti(X, y, theta);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<h4 id="æ­£è§„æ–¹ç¨‹"><a href="#æ­£è§„æ–¹ç¨‹" class="headerlink" title="æ­£è§„æ–¹ç¨‹"></a>æ­£è§„æ–¹ç¨‹</h4><p>$$<br>\theta = (X^TX)^{-1}X^Ty<br>$$<br>ğŸ‘‰ä»£ç å®ç°ï¼š</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta]</span> = <span class="title">normalEqn</span><span class="params">(X, y)</span></span></span><br><span class="line"><span class="comment">%NORMALEQN Computes the closed-form solution to linear regression </span></span><br><span class="line"><span class="comment">%   NORMALEQN(X,y) computes the closed-form solution to linear </span></span><br><span class="line"><span class="comment">%   regression using the normal equations.</span></span><br><span class="line"></span><br><span class="line">theta = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X, <span class="number">2</span>), <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">theta = pinv(X&#x27; * X) * X&#x27; * y;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>è¿™é‡Œæˆ‘ä»¬æ±‚ä¼ªé€†ä»¥ç¡®ä¿æ­£å¸¸è¿è¡Œã€‚é€šå¸¸é€ æˆ$X^TX$ä¸å¯é€†çš„åŸå› æ˜¯ï¼š</p>
<ol>
<li>å­˜åœ¨å¯çº¦ç‰¹å¾ï¼Œå³ç»™å®šçš„æŸä¸¤/å¤šä¸ªç‰¹å¾çº¿æ€§ç›¸å…³ï¼Œåªä¿ç•™ä¸€ä¸ªåˆ é™¤å…¶ä»–å³å¯è§£å†³ã€‚ï¼ˆe.g. there are the size of house in feet^2 and the size of house in meter^2, where we know that 1 meter = 3.28 feetï¼‰</li>
<li>ç»™å®šç‰¹å¾è¿‡å¤šï¼Œ($m \le n$). å¯ä»¥åˆ é™¤ä¸€äº›ä¸é‡è¦çš„ç‰¹å¾ï¼ˆè€ƒè™‘PCAç®—æ³•ï¼‰</li>
</ol>
<h4 id="æ¢¯åº¦ä¸‹é™vsæ­£è§„æ–¹ç¨‹"><a href="#æ¢¯åº¦ä¸‹é™vsæ­£è§„æ–¹ç¨‹" class="headerlink" title="æ¢¯åº¦ä¸‹é™vsæ­£è§„æ–¹ç¨‹"></a>æ¢¯åº¦ä¸‹é™vsæ­£è§„æ–¹ç¨‹</h4><table>
<thead>
<tr>
<th></th>
<th>Gradient Descent</th>
<th>Normal Equation</th>
</tr>
</thead>
<tbody><tr>
<td>éœ€è¦é€‰æ‹© alpha</td>
<td>âœ…</td>
<td>-</td>
</tr>
<tr>
<td>ç¬¬ä¸‰æ–¹</td>
<td>âœ…</td>
<td>-</td>
</tr>
<tr>
<td>æ—¶é—´å¤æ‚åº¦</td>
<td>$O(kn^2)$</td>
<td>æ±‚$X^TX$çš„ä¼ªé€†éœ€è¦$O(n^3)$</td>
</tr>
<tr>
<td>n ç›¸å½“å¤§æ—¶</td>
<td>å¯ä»¥å·¥ä½œ</td>
<td>ååˆ†ç¼“æ…¢ç”šè‡³ä¸å¯è®¡ç®—</td>
</tr>
</tbody></table>
<p>å®é™…ä¸Šï¼Œå½“ $n&gt;10,000$ æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸æ›´å€¾å‘äºä½¿ç”¨æ¢¯åº¦ä¸‹é™ï¼Œå¦åˆ™æ­£è§„æ–¹ç¨‹ä¸€èˆ¬éƒ½è¡¨ç°å¾—æ›´å¥½ã€‚</p>
<h4 id="æ³¨ï¼šç‰¹å¾ç¼©æ”¾"><a href="#æ³¨ï¼šç‰¹å¾ç¼©æ”¾" class="headerlink" title="æ³¨ï¼šç‰¹å¾ç¼©æ”¾"></a>æ³¨ï¼šç‰¹å¾ç¼©æ”¾</h4><p>æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿è¾“å…¥å€¼å¤§æ¦‚åœ¨ä¸€å®šçš„èŒƒå›´å†…æ¥ä½¿æ¢¯åº¦ä¸‹é™è¿è¡Œæ›´å¿«ï¼Œæ¯”å¦‚è¯´ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ‰€æœ‰å€¼å˜åˆ° $[-1,1]$ çš„èŒƒå›´å†…ï¼ŒåŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡å¤„ç†è®©è¾“å…¥å€¼ä¹‹é—´çš„å·®è·ä¸è¦å¤ªå¤§ï¼ˆä¾‹å¦‚ï¼Œè¾“å…¥å€¼ä¸­åŒæ—¶æœ‰ 0.000001 å’Œ 1 è¿™æ ·å·®è·å¤§çš„å€¼ä¼šå½±å“æ¢¯åº¦ä¸‹é™çš„æ•ˆç‡ï¼‰ã€‚</p>
<p>åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æƒ³è¦ä¿è¯å˜é‡å€¼åœ¨ $[-3,-\frac{1}{3}) \cup (+\frac{1}{3}, +3]$ è¿™ä¸ªèŒƒå›´å†…å–å€¼ã€‚</p>
<p>ä¸ºè¾¾æˆè¯¥ç›®æ ‡ï¼Œæˆ‘ä»¬åšå¦‚ä¸‹æ“ä½œï¼š</p>
<ol>
<li>Feature scaling</li>
</ol>
<p>$$<br>\begin{array}{rl}<br>\textrm{Range:} &amp; s_i = max(x_i)-min(x_i)\<br>\textrm{Or Range:} &amp; s_i = \textrm{standard deviation of } x_i\<br>\textrm{Scaling:} &amp; x_i:=\frac{x_i}{s_i}<br>\end{array}<br>$$</p>
<ol start="2">
<li>Mean normalizaton</li>
</ol>
<p>$$<br>\begin{array}{rl}<br>\textrm{Mean(Average):} &amp; \mu_i = \frac{sum(x_i)}{m}\<br>\textrm{normalizing:} &amp; x_i:=x_i-\mu_i<br>\end{array}<br>$$</p>
<p>æŠŠä¸¤ä¸ªæ“ä½œå’Œåœ¨ä¸€èµ·ï¼Œå³ï¼š<br>$$<br>x_i:=\frac{x_i-\mu_i}{s_i}<br>$$<br>å…¶ä¸­ï¼Œ$\mu_i$ æ˜¯ç‰¹å¾(i)çš„å€¼çš„å¹³å‡ï¼Œ$s_i$æ˜¯å€¼çš„èŒƒå›´ã€‚</p>
<p>ğŸ‘‰ä»£ç å®ç°ï¼š</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[X_norm, mu, sigma]</span> = <span class="title">featureNormalize</span><span class="params">(X)</span></span></span><br><span class="line"><span class="comment">%FEATURENORMALIZE Normalizes the features in X </span></span><br><span class="line"><span class="comment">%   FEATURENORMALIZE(X) returns a normalized version of X where</span></span><br><span class="line"><span class="comment">%   the mean value of each feature is 0 and the standard deviation</span></span><br><span class="line"><span class="comment">%   is 1. This is often a good preprocessing step to do when</span></span><br><span class="line"><span class="comment">%   working with learning algorithms.</span></span><br><span class="line"></span><br><span class="line">X_norm = X;</span><br><span class="line">mu = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="built_in">size</span>(X, <span class="number">2</span>));</span><br><span class="line">sigma = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="built_in">size</span>(X, <span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">mu = <span class="built_in">mean</span>(X);</span><br><span class="line">sigma = std(X);</span><br><span class="line">X_norm = (X - mu) ./ sigma;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>


<h3 id="åˆ†ç±»é—®é¢˜"><a href="#åˆ†ç±»é—®é¢˜" class="headerlink" title="åˆ†ç±»é—®é¢˜"></a>åˆ†ç±»é—®é¢˜</h3><blockquote>
<p>åšé¢„æµ‹ï¼Œå€¼åŸŸä¸ºç¦»æ•£çš„å‡ ä¸ªç‰¹å®šå€¼ï¼ˆä¾‹å¦‚ 0 æˆ– 1ï¼›0/1/2/3ï¼‰</p>
</blockquote>
<h4 id="é€»è¾‘å›å½’"><a href="#é€»è¾‘å›å½’" class="headerlink" title="é€»è¾‘å›å½’"></a>é€»è¾‘å›å½’</h4><p>å‡è®¾å‡½æ•°ï¼š<br>$$<br>\left{\begin{array}{l}<br>h_\theta(x) = g(\theta^Tx)\<br>z = \theta^T x\<br>g(z) = \frac{1}{1+e^{-z}}\<br>\end{array}\right.<br>$$<br>å…¶ä¸­ï¼Œ$g(z)$ ç§°ä¸º Simoid å‡½æ•°ï¼Œæˆ–é€»è¾‘å‡½æ•°ï¼Œå…¶å›¾åƒå¦‚ä¸‹ï¼š</p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gauxvtzb5qj30ur0g8wgd.jpg" alt="image-20190917162504420"></p>
<p>ğŸ‘‰ä»£ç å®ç°ï¼š</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">g</span> = <span class="title">sigmoid</span><span class="params">(z)</span></span></span><br><span class="line"><span class="comment">%SIGMOID Compute sigmoid functoon</span></span><br><span class="line"><span class="comment">%   J = SIGMOID(z) computes the sigmoid of z.</span></span><br><span class="line"></span><br><span class="line">g = <span class="number">1.0</span> ./ (<span class="number">1.0</span> + <span class="built_in">exp</span>(-z));</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>

<p>ä¸Šå¼å¯åŒ–ç®€å¾—ï¼š<br>$$<br>h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}<br>$$</p>
<p>$h_\theta$ çš„è¾“å‡ºæ˜¯é¢„æµ‹å€¼ä¸º1çš„å¯èƒ½æ€§ï¼Œå¹¶æœ‰ä¸‹ä¸¤å¼æˆç«‹ï¼š<br>$$<br>h_\theta(x)=P(y=1 \mid x;\theta)=1-P(y=0 \mid x; \theta)<br>$$</p>
<p>$$<br>P(y=0 \mid x;\theta) + P(y=1 \mid x;\theta) = 1<br>$$</p>
<p><strong>å†³ç­–è¾¹ç•Œ</strong>ï¼šé€»è¾‘å›å½’çš„å†³ç­–è¾¹ç•Œå°±æ˜¯å°†åŒºåŸŸåˆ†æˆ$y=0$å’Œ $y=1$ ä¸¤éƒ¨åˆ†çš„ä¸€ä¸ªè¶…å¹³é¢ã€‚</p>
<p>å†³ç­–è¾¹ç•Œç”±å‡è®¾å‡½æ•°å†³å®šã€‚è¿™æ˜¯ç”±äºè¦å®Œæˆåˆ†ç±»ï¼Œéœ€ç”¨$h_\theta$çš„è¾“å‡ºæ¥å†³å®šç»“æœæ˜¯ 0 è¿˜æ˜¯ 1ã€‚å®š 0.5 ä¸ºåˆ†ç•Œï¼Œå³ï¼š<br>$$<br>\begin{array}{rcl}<br>h_\theta(x) \ge 0.5 &amp;\Rightarrow&amp; y=1\<br>h_\theta(x) &lt; 0.5 &amp;\Rightarrow&amp; y=0<br>\end{array}<br>$$<br>ç”± Simoid å‡½æ•°çš„æ€§è´¨ï¼Œä¸Šå¼ç­‰ä»·ä¸ºï¼š<br>$$<br>\begin{array}{rcl}<br>\theta^TX \ge 0 &amp;\Rightarrow&amp; y=1\<br>\theta^TX \le 0 &amp;\Rightarrow&amp; y=0\<br>\end{array}<br>$$<br>é‚£ä¹ˆå¯¹äºç»™å®šçš„ä¸€ç»„ $\theta$ï¼Œä¾‹å¦‚$\theta=\left[\begin{array}{c}5\-1\0\end{array} \right]$ï¼Œæœ‰ $y=1$ å½“ä¸”ä»…å½“ $5+(-1)x_1+0x_2 \ge 0$ï¼Œè¿™æ—¶å†³ç­–è¾¹ç•Œä¸º $x_1=5$ã€‚</p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gauyq6oc10j306e04umx0.jpg" alt="image-20190917173722734"></p>
<p>å†³ç­–è¾¹ç•Œä¹Ÿå¯ä»¥æ˜¯ä¸‹é¢è¿™ç§å¤æ‚çš„æƒ…å†µï¼š</p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwly1gauyt0wbq3j30mv08xwfh.jpg" alt="image-20190917174144868"></p>
<p><strong>é€»è¾‘å›å½’æ¨¡å‹</strong>ï¼š<br>$$<br>\begin{array}{rcl}<br>\textrm{Training set} &amp;:&amp; {(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \ldots, (x^{(m)},y^{(m)})}\<br>\<br>\textrm{m examples} &amp;:&amp;<br>x \in \left[\begin{array}{c}<br>x_0\x_1\ \vdots \ x_n<br>\end{array}\right] \textrm{where }(x_0=1)<br>,\quad y \in {0,1}\<br>\<br>\textrm{Hypothesis} &amp;:&amp; h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}\\<br>\textrm{Cost Function} &amp;:&amp;<br>J(\theta)=-\frac{1}{m}\sum_{i=1}^m\Bigg[y^{(i)}log\Big(h_\theta(x)\Big)+(1-y^{(i)})log\Big(1-h_\theta(x^{(i)})\Big)\Bigg]<br>\end{array}<br>$$<br>å‘é‡åŒ–è¡¨ç¤ºï¼š<br>$$<br>\begin{array}{l}<br>h=g(X\theta)\<br>J(\theta)=\frac{1}{m}\cdot\big(-y^T log(h) -(1-y)^T log(1-h)\big)<br>\end{array}<br>$$<br><strong>æ¢¯åº¦ä¸‹é™</strong>ï¼š<br>$$<br>\begin{array}{l}<br>Repeat \quad {\<br>\qquad \theta_j:=\theta_j-\frac{\alpha}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)}\<br>}<br>\end{array}<br>$$<br>å‘é‡åŒ–è¡¨ç¤ºï¼š<br>$$<br>\theta:=\theta-\frac{\alpha}{m}X^T(g(X\theta)-\overrightarrow{y})<br>$$<br>ğŸ‘‰ä»£ç å®ç°ï¼ˆä½¿ç”¨Advanced Optimizationï¼‰ï¼š</p>
<ol>
<li>æä¾›$J(\theta), \frac{\partial}{\partial\theta_j}J(\theta)$</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J, grad]</span> = <span class="title">costFunction</span><span class="params">(theta, X, y)</span></span></span><br><span class="line"><span class="comment">%COSTFUNCTION Compute cost and gradient for logistic regression</span></span><br><span class="line"><span class="comment">%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the</span></span><br><span class="line"><span class="comment">%   parameter for logistic regression and the gradient of the cost</span></span><br><span class="line"><span class="comment">%   w.r.t. to the parameters.</span></span><br><span class="line"></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line"></span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line">grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(theta));</span><br><span class="line"></span><br><span class="line">h = sigmoid(X*theta);</span><br><span class="line"></span><br><span class="line">J = <span class="number">1</span>/m * (-y&#x27;*<span class="built_in">log</span>(h) - (<span class="number">1</span>-y)&#x27;*<span class="built_in">log</span>(<span class="number">1</span>-h));</span><br><span class="line"></span><br><span class="line">grad = <span class="number">1</span>/m * X&#x27;*(h-y);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>è°ƒç”¨ Advanced Optimization å‡½æ•°è§£å†³ä¼˜åŒ–é—®é¢˜ï¼š</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">options = optimset(<span class="string">&#x27;GradObj&#x27;</span>, <span class="string">&#x27;on&#x27;</span>, <span class="string">&#x27;MaxIter&#x27;</span>, <span class="number">100</span>);</span><br><span class="line">initialTheta = <span class="built_in">zeros</span>(<span class="number">2</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);</span><br></pre></td></tr></table></figure>
<h5 id="å¤šå…ƒåˆ†ç±»"><a href="#å¤šå…ƒåˆ†ç±»" class="headerlink" title="å¤šå…ƒåˆ†ç±»"></a>å¤šå…ƒåˆ†ç±»</h5><p>æˆ‘ä»¬é‡‡ç”¨ä¸€ç³»åˆ—çš„å•å…ƒï¼ˆé€»è¾‘ï¼‰åˆ†ç±»æ¥å®Œæˆå¤šå…ƒåˆ†ç±»ï¼š<br>$$<br>\begin{array}{l}<br>y \in {0,1,\cdots,n}\\<br>h_\theta^{(0)}(x)=P(y=0|x;\theta)\<br>h_\theta^{(0)}(x)=P(y=0|x;\theta)\<br>\vdots\<br>h_\theta^{(0)}(x)=P(y=0|x;\theta)\\<br>prediction = \mathop{max}\limits_{\theta}\big(h_\theta^{(i)}(x)\big)<br>\end{array}<br>$$</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g78nj8fvy4j30d507agmp.jpg" alt="img"></p>
<h4 id="æ³¨ï¼šè¿‡æ‹Ÿåˆ"><a href="#æ³¨ï¼šè¿‡æ‹Ÿåˆ" class="headerlink" title="æ³¨ï¼šè¿‡æ‹Ÿåˆ"></a>æ³¨ï¼šè¿‡æ‹Ÿåˆ</h4><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g7aopbltb1j30f0046dg2.jpg" alt="img"></p>
<p>è¿‡æ‹Ÿåˆå¯¹è®­ç»ƒé›†ä¸­çš„æ•°æ®é¢„æµ‹çš„å¾ˆå¥½ï¼Œä½†å¯¹æ²¡è§è¿‡çš„æ–°æ ·æœ¬é¢„æµ‹æ•ˆæœä¸ä½³ã€‚</p>
<p>è§£å†³è¿‡æ‹Ÿåˆçš„æ–¹æ³•æœ‰ï¼š</p>
<ol>
<li><p>å‡å°‘ç‰¹å¾æ•°é‡ï¼ˆPCAï¼‰</p>
</li>
<li><p>æ­£åˆ™åŒ–ï¼šåœ¨ä»£ä»·å‡½æ•°ä¸­åŠ å…¥ $\theta$ çš„æƒé‡ï¼š</p>
<p>$\mathop{min}\limits_{\theta} \dfrac{1}{2m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2$</p>
<blockquote>
<p>æ³¨æ„ï¼Œ$\theta_0$æ˜¯æˆ‘ä»¬åŠ ä¸Šçš„å¸¸æ•°é¡¹ï¼Œä¸åº”è¯¥è¢«æ­£åˆ™åŒ–ã€‚</p>
</blockquote>
<p>ä»£ç å®ç°ï¼š</p>
</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J, grad]</span> = <span class="title">lrCostFunction</span><span class="params">(theta, X, y, lambda)</span></span></span><br><span class="line"><span class="comment">%LRCOSTFUNCTION Compute cost and gradient for logistic regression with </span></span><br><span class="line"><span class="comment">%regularization</span></span><br><span class="line"><span class="comment">%   J = LRCOSTFUNCTION(theta, X, y, lambda) computes the cost of using</span></span><br><span class="line"><span class="comment">%   theta as the parameter for regularized logistic regression and the</span></span><br><span class="line"><span class="comment">%   gradient of the cost w.r.t. to the parameters. </span></span><br><span class="line"></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line"></span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line">grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(theta));</span><br><span class="line"></span><br><span class="line"><span class="comment">% Unregularized cost function &amp; gradient for logistic regression</span></span><br><span class="line">h = sigmoid(X * theta);</span><br><span class="line">J = <span class="number">1</span>/m * (-y&#x27;*<span class="built_in">log</span>(h) - (<span class="number">1</span>-y)&#x27;*<span class="built_in">log</span>(<span class="number">1</span>-h));</span><br><span class="line">grad = <span class="number">1</span>/m * X&#x27;*(h-y);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Regularize</span></span><br><span class="line">temp = theta;</span><br><span class="line">temp(<span class="number">1</span>) = <span class="number">0</span>;</span><br><span class="line">J = J + lambda/(<span class="number">2</span>*m) * sum(temp.^<span class="number">2</span>);</span><br><span class="line">grad = grad + lambda/m * temp;</span><br><span class="line"></span><br><span class="line">grad = grad(:);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<h4 id="ç¥ç»ç½‘ç»œ"><a href="#ç¥ç»ç½‘ç»œ" class="headerlink" title="ç¥ç»ç½‘ç»œ"></a>ç¥ç»ç½‘ç»œ</h4><p>$$<br>\left[\begin{array}{c}x_0 \ x_1 \ x_2 \ x_3\end{array}\right]<br>\rightarrow<br>\left[\begin{array}{c}a_1^{(2)} \ a_2^{(2)} \ a_3^{(2)} \ \end{array}\right]<br>\rightarrow<br>\left[\begin{array}{c}a_1^{(3)} \ a_2^{(3)} \ a_3^{(3)} \ \end{array}\right]<br>\rightarrow<br>h_\theta(x)<br>$$</p>
<p>ç¬¬ä¸€å±‚æ˜¯æ•°æ®é›†ï¼Œç§°ä¸ºè¾“å…¥å±‚ï¼Œå¯ä»¥çœ‹ä½œ $a^{(0)}$ ï¼›ä¸­é—´æ˜¯æ•°ä¸ªéšè—å±‚ï¼Œæœ€ç»ˆå¾—åˆ°çš„å°±æ˜¯é¢„æµ‹å‡½æ•°ï¼Œè¿™ä¸€å±‚å«åšè¾“å‡ºå±‚ã€‚</p>
<p>$$<br>z^{(j)} = \Theta^{(j-1)}a^{(j-1)}<br>$$</p>
<p>$$<br>a^{(j)} = g(z^{(j)})<br>$$</p>
<p>å‡è®¾æœ‰ c ä¸ªå±‚ï¼Œåˆ™:</p>
<p>$$<br>h_\Theta(x)=a^{(c+1)}=g(z^{(c+1)})<br>$$</p>
<p>ä¾‹å¦‚ï¼Œç”¨ä¸€å±‚çš„ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹ä¸€äº›è¡¨è¾¾é€»è¾‘å‡½æ•°çš„ç¥ç»ç½‘ç»œï¼š<br>$$<br>\begin{array}{l}AND:\&amp;\Theta^{(1)} &amp;=\begin{bmatrix}-30 &amp; 20 &amp; 20\end{bmatrix} \ NOR:\&amp;\Theta^{(1)} &amp;= \begin{bmatrix}10 &amp; -20 &amp; -20\end{bmatrix} \ OR:\&amp;\Theta^{(1)} &amp;= \begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix} \\end{array}<br>$$</p>
<h5 id="å¤šå…ƒåˆ†ç±»-1"><a href="#å¤šå…ƒåˆ†ç±»-1" class="headerlink" title="å¤šå…ƒåˆ†ç±»"></a>å¤šå…ƒåˆ†ç±»</h5><p>$$<br>y^{(i)}=\begin{bmatrix}1\0\0\0\end{bmatrix},\begin{bmatrix}0\1\0\0\end{bmatrix},\begin{bmatrix}0\0\1\0\end{bmatrix},\begin{bmatrix}0\0\0\1\end{bmatrix}<br>$$</p>
<p>$$<br>\left[\begin{array}{c}x_0 \ x_1 \ x_2 \ x_3\end{array}\right]<br>\rightarrow<br>\left[\begin{array}{c}a_1^{(2)} \ a_2^{(2)} \ a_3^{(2)} \ â€¦ \end{array}\right]<br>\rightarrow<br>\left[\begin{array}{c}a_1^{(3)} \ a_2^{(3)} \ a_3^{(3)} \ â€¦ \end{array}\right]<br>\rightarrow<br>\cdots<br>\rightarrow<br>\left[\begin{array}{c}h_\Theta(x)_1 \ h_\Theta(x)_2 \ h_\Theta(x)_3 \ h_\Theta(x)_4 \end{array}\right]<br>$$</p>
<h5 id="ç¥ç»ç½‘ç»œçš„æ‹Ÿåˆ"><a href="#ç¥ç»ç½‘ç»œçš„æ‹Ÿåˆ" class="headerlink" title="ç¥ç»ç½‘ç»œçš„æ‹Ÿåˆ"></a>ç¥ç»ç½‘ç»œçš„æ‹Ÿåˆ</h5><table>
<thead>
<tr>
<th>Notation</th>
<th>Represent</th>
</tr>
</thead>
<tbody><tr>
<td>$L$</td>
<td>ç¥ç»ç½‘ç»œä¸­çš„æ€»å±‚æ•°</td>
</tr>
<tr>
<td>$s_l$</td>
<td>ç¬¬$l$å±‚ä¸­çš„èŠ‚ç‚¹æ•°ï¼ˆä¸ç®—åç§»å•å…ƒ$a_0$ï¼‰</td>
</tr>
<tr>
<td>$K$</td>
<td>è¾“å‡ºèŠ‚ç‚¹æ•°</td>
</tr>
</tbody></table>
<p><strong>ä»£ä»·å‡½æ•°</strong>ï¼š<br>$$<br>J(\Theta)=-\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}\Big[<br>y_k^{(i)}log\Big(\big(h_\Theta(x^{(i)})\big)<em>k\Big)+<br>(1-y_k^{(i)})log\Big(1-\big(h_\Theta(x^{(i)})\big)<em>k\Big)<br>\Big]+<br>\frac{\lambda}{2m}\sum</em>{l=1}^{L-1}\sum</em>{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}\Big(\Theta_{j,i}^{(l)}\Big)^2<br>$$<br><strong>å‘åä¼ æ’­ç®—æ³•</strong>ï¼š<br>$$<br>\begin{array}{lll}<br>\textrm{Give training set }{(x^{(1)},y^{(1)}),â€¦,(x^{(m)},y^{(m)})}\<br>\textrm{Set }\Delta_{i,j}^{(l)}:=0\textrm{ for each } l,i,j \textrm{ (get a matrix full of zeros)}\<br>\mathop{\textrm{For}} \textrm{ training example $t=1$ to $m$}:\<br>\qquad a^{(1)}:= x^{(t)}\<br>\qquad \textrm{Compute $a^{(l)}$ for $l=2,3,\cdots,L$ by forward propagation}\<br>\qquad \textrm{Using $y^{(t)}$ to compute } \delta^{(L)}=a^{(L)}-y^{(t)}\<br>\qquad \textrm{Compute } \delta^{(l)}=\big((\Theta^{(l)})^T\delta^{(l+1)}\big).<em>a^{(l)}.</em>(1-a^{(l)}) \textrm{ for } \delta^{(L-1)},\delta^{(L-2)},â€¦,\delta^{(2)}\<br>\qquad \Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T\<br>\textrm{End For}\<br>D_{i,j}^{(l)}:=\frac{1}{m}\Delta_{i,j}^{(l)}\textrm{ if } j=0\<br>D_{i,j}^{(l)}:=\frac{1}{m}\big(\Delta_{i,j}^{(l)}+\lambda\Theta_{i,j}^{(l)}\big) \textrm{ if } j\neq 0 \<br>\textrm{Get }<br>\frac{\partial}{\partial\Theta_{i,j}^{(l)}}J(\Theta)=D_{i,j}^{(l)}<br>\end{array}<br>$$<br>æ³¨ï¼šä¸Šå¼ä¸­ $.*$ ä»£è¡¨ Matlab/Octave ä¸­çš„ element-wise çš„ä¹˜æ³•ã€‚</p>
<p><strong>å‘åä¼ æ’­çš„ä½¿ç”¨</strong>ï¼š</p>
<p>å…ˆçœ‹å‡ ä¸ªæ¶‰åŠåˆ°çš„æ–¹æ³•ï¼š</p>
<ul>
<li>å‚æ•°å±•å¼€ï¼šä¸ºä½¿ç”¨ä¼˜åŒ–å‡½æ•°ï¼Œæˆ‘ä»¬éœ€è¦æŠŠæ‰€æœ‰çš„$\Theta$çŸ©é˜µå±•å¼€å¹¶æ‹¼æ¥æˆä¸€ä¸ªé•¿å‘é‡ï¼š</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">thetaVector = [ Theta1(:); Theta2(:); Theta3(:) ];</span><br><span class="line">deltaVector = [ D1(:); D2(:); D3(:) ];</span><br></pre></td></tr></table></figure>
<p>åœ¨å¾—åˆ°ä¼˜åŒ–ç»“æœåè¿”å›åŸæ¥çš„çŸ©é˜µï¼š</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Theta1 = <span class="built_in">reshape</span>(thetaVector(<span class="number">1</span>:<span class="number">110</span>),<span class="number">10</span>,<span class="number">11</span>)</span><br><span class="line">Theta2 = <span class="built_in">reshape</span>(thetaVector(<span class="number">111</span>:<span class="number">220</span>),<span class="number">10</span>,<span class="number">11</span>)</span><br><span class="line">Theta3 = <span class="built_in">reshape</span>(thetaVector(<span class="number">221</span>:<span class="number">231</span>),<span class="number">1</span>,<span class="number">11</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>æ¢¯åº¦æ£€æŸ¥ï¼šåˆ©ç”¨ $\frac{\partial}{\partial\Theta_j}J(\Theta) \approx \frac{J(\Theta_1,â€¦,\Theta_j+\epsilon,â€¦,\Theta_n)-J(\Theta_1,â€¦,\Theta_j-\epsilon,â€¦,\Theta_n)}{2\epsilon}$ å–ä¸€ä¸ªå°çš„é‚»åŸŸå¦‚ $\epsilon=10^{-4}$ï¼Œå¯ä»¥æ£€æŸ¥æˆ‘ä»¬ç”¨å‘åä¼ æ’­æ±‚å‡ºçš„æ¢¯åº¦æ˜¯å¦æ­£ç¡®ï¼ˆè‹¥æ­£ç¡®ï¼Œæœ‰ gradApprox â‰ˆ deltaVector æˆç«‹ï¼‰ã€‚ä»£ç å®ç°ï¼š</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">epsilon = <span class="number">1e-4</span>;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : n</span><br><span class="line">	thetaPlus = theta;</span><br><span class="line">	thetaPlus(<span class="built_in">i</span>) += epsilon;</span><br><span class="line">	thetaMinus = theta;</span><br><span class="line">	thetaMinus(<span class="built_in">i</span>) += epsilon;</span><br><span class="line">	gradApprox(<span class="built_in">i</span>) = (J(thetaPlus) - J(thetaMinus)) / (<span class="number">2</span>*epsilon);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ul>
<li>éšå³åˆå§‹åŒ–ï¼šåœ¨å¼€å§‹æ—¶ï¼Œå°† $\Theta_{ij}^{(l)}$ éšæœºåˆå§‹åŒ–ï¼Œåº”ä¿è¯éšæœºå€¼çš„å–å€¼åœ¨ä¸€ä¸ª $[-\epsilon,\epsilon]$ çš„èŒƒå›´å†…ï¼ˆè¿™ä¸ª $\epsilon$ ä¸æ¢¯åº¦æ£€æŸ¥ä¸­çš„æ— å…³ï¼‰ã€‚ä»£ç å®ç°ï¼š</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.</span><br><span class="line"></span><br><span class="line">Theta1 = <span class="built_in">rand</span>(<span class="number">10</span>,<span class="number">11</span>) * (<span class="number">2</span> * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta2 = <span class="built_in">rand</span>(<span class="number">10</span>,<span class="number">11</span>) * (<span class="number">2</span> * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta3 = <span class="built_in">rand</span>(<span class="number">1</span>,<span class="number">11</span>) * (<span class="number">2</span> * INIT_EPSILON) - INIT_EPSILON;</span><br></pre></td></tr></table></figure>
<p>å°†ä¸Šè¿°æŠ€å·§ä¸å‘åä¼ æ’­ç®—æ³•ç»“åˆï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†äº†è®­ç»ƒç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼š</p>
<ol>
<li>éšæœºåˆå§‹åŒ–</li>
<li>å‘å‰ä¼ æ’­å¾—åˆ° $h_\Theta(x^{(i)})$ å¯¹ä»»æ„ $x^{(i)}$</li>
<li>è®¡ç®—ä»£ä»·å‡½æ•°</li>
<li>ä½¿ç”¨å‘åä¼ æ’­è®¡ç®—åå¯¼</li>
<li>åˆ©ç”¨æ¢¯åº¦æ£€æŸ¥éªŒè¯å‘åä¼ æ’­æ˜¯å¦æ­£ç¡®ï¼Œè‹¥æ²¡é—®é¢˜åˆ™å…³é—­æ¢¯åº¦æ£€æŸ¥åŠŸèƒ½</li>
<li>ä½¿ç”¨æ¢¯åº¦ä¸‹é™æˆ–ä¼˜åŒ–å‡½æ•°å¾—åˆ°$\Theta$</li>
</ol>
<p>ğŸ‘‰ä»£ç å®ç°ï¼š</p>
<ol>
<li>éšæœºåˆå§‹åŒ–</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">W</span> = <span class="title">randInitializeWeights</span><span class="params">(L_in, L_out)</span></span></span><br><span class="line"><span class="comment">%RANDINITIALIZEWEIGHTS Randomly initialize the weights of a layer with L_in</span></span><br><span class="line"><span class="comment">%incoming connections and L_out outgoing connections</span></span><br><span class="line"><span class="comment">%   W = RANDINITIALIZEWEIGHTS(L_in, L_out) randomly initializes the weights </span></span><br><span class="line"><span class="comment">%   of a layer with L_in incoming connections and L_out outgoing </span></span><br><span class="line"><span class="comment">%   connections. </span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%   Note that W should be set to a matrix of size(L_out, 1 + L_in) as</span></span><br><span class="line"><span class="comment">%   the first column of W handles the &quot;bias&quot; terms</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line">W = <span class="built_in">zeros</span>(L_out, <span class="number">1</span> + L_in);</span><br><span class="line"></span><br><span class="line"><span class="comment">% epsilon_init = 0.12</span></span><br><span class="line">epsilon_init = <span class="built_in">sqrt</span>(<span class="number">6</span> / (L_in + L_out));</span><br><span class="line">W = <span class="built_in">rand</span>(L_out, <span class="number">1</span> + L_in) * (<span class="number">2</span> * epsilon_init) - epsilon_init;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>è®¡ç®—ä»£ä»·</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J grad]</span> = <span class="title">nnCostFunction</span><span class="params">(nn_params, ...</span></span></span><br><span class="line"><span class="function"><span class="params">                                   input_layer_size, ...</span></span></span><br><span class="line"><span class="function"><span class="params">                                   hidden_layer_size, ...</span></span></span><br><span class="line"><span class="function"><span class="params">                                   num_labels, ...</span></span></span><br><span class="line"><span class="function"><span class="params">                                   X, y, lambda)</span></span></span><br><span class="line"><span class="comment">%NNCOSTFUNCTION Implements the neural network cost function for a two layer</span></span><br><span class="line"><span class="comment">%neural network which performs classification</span></span><br><span class="line"><span class="comment">%   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...</span></span><br><span class="line"><span class="comment">%   X, y, lambda) computes the cost and gradient of the neural network. The</span></span><br><span class="line"><span class="comment">%   parameters for the neural network are &quot;unrolled&quot; into the vector</span></span><br><span class="line"><span class="comment">%   nn_params and need to be converted back into the weight matrices. </span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">%   The returned parameter grad should be a &quot;unrolled&quot; vector of the</span></span><br><span class="line"><span class="comment">%   partial derivatives of the neural network.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Reshape nn_params back into the parameters Theta_1 and Theta_2, the weight matrices</span></span><br><span class="line"><span class="comment">% for our 2 layer neural network</span></span><br><span class="line">Theta_1 = <span class="built_in">reshape</span>(nn_params(<span class="number">1</span>:hidden_layer_size * (input_layer_size + <span class="number">1</span>)), ...</span><br><span class="line">                 hidden_layer_size, (input_layer_size + <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">Theta_2 = <span class="built_in">reshape</span>(nn_params((<span class="number">1</span> + (hidden_layer_size * (input_layer_size + <span class="number">1</span>))):<span class="keyword">end</span>), ...</span><br><span class="line">                 num_labels, (hidden_layer_size + <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">% Setup some useful variables</span></span><br><span class="line">m = <span class="built_in">size</span>(X, <span class="number">1</span>);</span><br><span class="line">K = num_labels;</span><br><span class="line"></span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line">Theta_1_grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(Theta_1));</span><br><span class="line">Theta_2_grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(Theta_2));</span><br><span class="line"></span><br><span class="line"><span class="comment">% y(5000x1) -&gt; Y(5000x10)</span></span><br><span class="line">Y = <span class="built_in">zeros</span>(m, K);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</span><br><span class="line">    Y(<span class="built_in">i</span>, y(<span class="built_in">i</span>)) = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Feedforward</span></span><br><span class="line">a_1 = X;</span><br><span class="line">a_1_bias = [<span class="built_in">ones</span>(m, <span class="number">1</span>), a_1];</span><br><span class="line"></span><br><span class="line">z_2 = a_1_bias * Theta_1&#x27;;</span><br><span class="line">a_2 = sigmoid(z_2);</span><br><span class="line">a_2_bias = [<span class="built_in">ones</span>(m, <span class="number">1</span>), a_2];</span><br><span class="line"></span><br><span class="line">z_3 = a_2_bias * Theta_2&#x27;;</span><br><span class="line">a_3 = sigmoid(z_3);</span><br><span class="line">h = a_3;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Cost Function</span></span><br><span class="line"><span class="comment">% for i = 1 : K</span></span><br><span class="line"><span class="comment">%     yK = Y(:, i);</span></span><br><span class="line"><span class="comment">%     hK = h(:, i);</span></span><br><span class="line"><span class="comment">%     J += 1/m * (-yK&#x27;*log(hK) - (1-yK)&#x27;*log(1-hK));</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% J can be get by element-wise compute more elegantly.</span></span><br><span class="line">J = <span class="number">1</span>/m * sum(sum((-Y.*<span class="built_in">log</span>(h) - (<span class="number">1</span>-Y).*<span class="built_in">log</span>(<span class="number">1</span>-h))));</span><br><span class="line"></span><br><span class="line"><span class="comment">% Regularize</span></span><br><span class="line">J = J + lambda/(<span class="number">2</span>*m) * (sum(sum(Theta_1(:, <span class="number">2</span>:<span class="keyword">end</span>).^<span class="number">2</span>)) + sum(sum(Theta_2(:, <span class="number">2</span>:<span class="keyword">end</span>).^<span class="number">2</span>)));</span><br><span class="line"></span><br><span class="line"><span class="comment">% Backpropagation</span></span><br><span class="line"></span><br><span class="line">delta_3 = a_3 .- Y;</span><br><span class="line">delta_2 = (delta_3 * Theta_2) .* sigmoidGradient([<span class="built_in">ones</span>(m, <span class="number">1</span>), z_2]);</span><br><span class="line"><span class="comment">% sigmoidGradient: return g = sigmoid(z) .* (1 - sigmoid(z));</span></span><br><span class="line">delta_2 = delta_2(:, <span class="number">2</span>:<span class="keyword">end</span>);</span><br><span class="line"></span><br><span class="line">Delta_1 = delta_2&#x27; * a_1_bias;</span><br><span class="line">Delta_2 = delta_3&#x27; * a_2_bias;</span><br><span class="line"></span><br><span class="line">Theta_1_grad = Delta_1 ./ m + lambda/m * [<span class="built_in">zeros</span>(<span class="built_in">size</span>(Theta_1, <span class="number">1</span>), <span class="number">1</span>), Theta_1(:, <span class="number">2</span>:<span class="keyword">end</span>)];</span><br><span class="line">Theta_2_grad = Delta_2 ./ m + lambda/m * [<span class="built_in">zeros</span>(<span class="built_in">size</span>(Theta_2, <span class="number">1</span>), <span class="number">1</span>), Theta_2(:, <span class="number">2</span>:<span class="keyword">end</span>)];</span><br><span class="line"></span><br><span class="line"><span class="comment">% Unroll gradients</span></span><br><span class="line">grad = [Theta_1_grad(:) ; Theta_2_grad(:)];</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ol start="3">
<li>é¢„æµ‹</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">p</span> = <span class="title">predict</span><span class="params">(Theta1, Theta2, X)</span></span></span><br><span class="line"><span class="comment">%PREDICT Predict the label of an input given a trained neural network</span></span><br><span class="line"><span class="comment">%   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the</span></span><br><span class="line"><span class="comment">%   trained weights of a neural network (Theta1, Theta2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Useful values</span></span><br><span class="line">m = <span class="built_in">size</span>(X, <span class="number">1</span>);</span><br><span class="line">num_labels = <span class="built_in">size</span>(Theta2, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">p = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X, <span class="number">1</span>), <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">h1 = sigmoid([<span class="built_in">ones</span>(m, <span class="number">1</span>) X] * Theta1&#x27;);</span><br><span class="line">h2 = sigmoid([<span class="built_in">ones</span>(m, <span class="number">1</span>) h1] * Theta2&#x27;);</span><br><span class="line">[dummy, p] = <span class="built_in">max</span>(h2, [], <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ol start="4">
<li>é©±åŠ¨</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">input_layer_size  = <span class="number">400</span>;  <span class="comment">% 20x20 Input Images of Digits</span></span><br><span class="line">hidden_layer_size = <span class="number">25</span>;   <span class="comment">% 25 hidden units</span></span><br><span class="line">num_labels = <span class="number">10</span>;          <span class="comment">% 10 labels, from 1 to 10   </span></span><br><span class="line">                          <span class="comment">% (note that we have mapped &quot;0&quot; to label 10)</span></span><br><span class="line"></span><br><span class="line">fprintf(<span class="string">&#x27;\nInitializing Neural Network Parameters ...\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">load(<span class="string">&#x27;Xy.mat&#x27;</span>);</span><br><span class="line"></span><br><span class="line">fprintf(<span class="string">&#x27;\nTraining Neural Network... \n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">%  value to see how more training helps.</span></span><br><span class="line">options = optimset(<span class="string">&#x27;MaxIter&#x27;</span>, <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">lambda = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Create &quot;short hand&quot; for the cost function to be minimized</span></span><br><span class="line">costFunction = @(p) nnCostFunction(p, ...</span><br><span class="line">                                   input_layer_size, ...</span><br><span class="line">                                   hidden_layer_size, ...</span><br><span class="line">                                   num_labels, X, y, lambda);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Now, costFunction is a function that takes in only one argument (the</span></span><br><span class="line"><span class="comment">% neural network parameters)</span></span><br><span class="line">[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Obtain Theta1 and Theta2 back from nn_params</span></span><br><span class="line">Theta1 = <span class="built_in">reshape</span>(nn_params(<span class="number">1</span>:hidden_layer_size * (input_layer_size + <span class="number">1</span>)), ...</span><br><span class="line">                 hidden_layer_size, (input_layer_size + <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">Theta2 = <span class="built_in">reshape</span>(nn_params((<span class="number">1</span> + (hidden_layer_size * (input_layer_size + <span class="number">1</span>))):<span class="keyword">end</span>), ...</span><br><span class="line">                 num_labels, (hidden_layer_size + <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">pred = predict(Theta1, Theta2, X);</span><br><span class="line"></span><br><span class="line">fprintf(<span class="string">&#x27;\nTraining Set Accuracy: %f\n&#x27;</span>, <span class="built_in">mean</span>(double(pred == y)) * <span class="number">100</span>);</span><br></pre></td></tr></table></figure>
<h4 id="æ”¯æŒå‘é‡æœº"><a href="#æ”¯æŒå‘é‡æœº" class="headerlink" title="æ”¯æŒå‘é‡æœº"></a>æ”¯æŒå‘é‡æœº</h4><p>ä¼˜åŒ–ç›®æ ‡ï¼š<br>$$<br>\min_\theta<br>C\sum_{i=1}^m \large[ y^{(i)} \mathop{\textrm{cost}_1}(\theta^Tx^{(i)})</p>
<ul>
<li>(1 - y^{(i)})\ \mathop{\textrm{cost}_0}(\theta^Tx^{(i)})\large]</li>
<li>\frac{1}{2}\sum_{j=1}^n \theta_j^2<br>$$<br>å½“ $C$ å€¼æ¯”è¾ƒå¤§æ—¶ï¼Œè¿™ä¸ªä¼˜åŒ–ç›®æ ‡ä¼šé€‰æ‹©å°†ç¬¬ä¸€ä¸ªæ±‚å’Œé¡¹è¶‹äºé›¶ï¼Œè¿™æ ·ä¼˜åŒ–ç›®æ ‡å°±å˜æˆäº†ï¼š<br>$$<br>\begin{array}{l}<br>  \min_\theta \frac{1}{2}\sum_{j=1}^{n}\theta_j^2\\<br>  s.t. \quad \begin{array}{l}<pre><code>  \theta^Tx^&#123;(i)&#125; \ge 1 &amp; \textrm&#123;if &#125; y^&#123;(i)&#125;=1\\
  \theta^Tx^&#123;(i)&#125; \le -1 &amp; \textrm&#123;if &#125; y^&#123;(i)&#125;=0</code></pre>
  \end{array}<br>\end{array}<br>$$<br>ç”±æ¬§æ°ç©ºé—´çš„çŸ¥è¯†ï¼š<br>$$<br>\begin{array}{ccl}<br>||u|| &amp;=&amp; \textrm{length of vector } u= \sqrt{u_1^2+u_2^2}\<br>p &amp;=&amp; \textrm{length of projection of } v \textrm{ onto } u \textrm{ (signed)} \ \<br>u^Tv &amp;=&amp; p \cdot ||u||<br>\end{array}<br>$$<br>ä¸Šå¼å¯è¡¨ç¤ºä¸ºï¼š<br>$$<br>\begin{array}{l}<br>  \min_\theta \frac{1}{2}\sum_{j=1}^{n}\theta_j^2<br>  =\frac{1}{2}\Big(\sqrt{\sum_{j=1}^n\theta_j^2}\Big)^2<br>  =\frac{1}{2}||\theta||^2 \\<br>  s.t. \quad \begin{array}{l}<pre><code>  p^&#123;(i)&#125;\cdot ||\theta|| \ge 1 &amp; \textrm&#123;if &#125; y^&#123;(i)&#125;=1\\
  p^&#123;(i)&#125;\cdot ||\theta|| \le -1 &amp; \textrm&#123;if &#125; y^&#123;(i)&#125;=0</code></pre>
  \end{array}\\<br>  \textrm{where $p^{(i)}$ is the projection of $x^{(i)}$ onto the vector $\theta$.}<br>\end{array}<br>$$<br>SVM ä¼šé€‰æ‹©æœ€å¤§çš„é—´éš™ï¼š</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gax9ziqe3sj324i0nu0z9.jpg" alt="å±å¹•å¿«ç…§ 2019-10-31 12.58.43"></p>
<h5 id="æ ¸æ–¹æ³•"><a href="#æ ¸æ–¹æ³•" class="headerlink" title="æ ¸æ–¹æ³•"></a>æ ¸æ–¹æ³•</h5><p>é¢å¯¹å¦‚ä¸‹åˆ†ç±»é—®é¢˜ï¼š</p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaxa3eq7zuj30c5083gm3.jpg" alt="image-20191102114127170"></p>
<p>æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¤šé¡¹å¼æ¥å›å½’ï¼Œä¾‹å¦‚ï¼Œå½“ $\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1x_2+\theta_4x_1^2+\theta_5x_2^2+\cdots\ge 0$ æ—¶é¢„æµ‹ $y=1$ï¼›</p>
<p>è¿™æ ·æœ‰å¤ªå¤šå¤šé¡¹å¼æ¯”è¾ƒéº»çƒ¦ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘å¦‚ä¸‹æ–¹æ³•ï¼š<br>$$<br>\begin{array}{lcl}<br>\textrm{Predict } y=1 &amp;\textrm{if}&amp;\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3+\cdots\ge 0<br>\end{array}<br>$$<br>è¿™é‡Œçš„ $f_1=x_1,f_2=x_2,f_3=x_1x_2,f_4=x_1^2,â€¦$</p>
<p>æˆ‘ä»¬ç”¨ $f_i$ æ›¿æ¢äº†å¤šé¡¹å¼ï¼Œé¿å…äº†é«˜æ¬¡é¡¹çš„éº»çƒ¦ï¼Œé‚£ä¹ˆå¦‚ä½•ç¡®å®š $f_i$ï¼Ÿå¤§æ¦‚çš„æ€æƒ³å¦‚ä¸‹ï¼š</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jljliww8j30o90dmwjt.jpg" alt="image-20191102124615797"></p>
<p>ä¸ºæ–¹ä¾¿æè¿°ï¼Œå‡è®¾æˆ‘ä»¬åªæœ‰ $x_0,x_1,x_2$ï¼Œå¹¶ä¸”åªæ‰“ç®—æ„é€  $f_1,f_2,f_3$ã€‚é‚£ä¹ˆï¼Œä¸ç®¡ $x_0$ï¼ˆåç§»é¡¹ï¼‰ï¼Œæˆ‘ä»¬ä» $x_1$-$x_2$ çš„å›¾åƒä¸­é€‰æ‹© 3 ä¸ªç‚¹ï¼Œè®°ä¸º $l^{(1)},l^{(2)},l^{(3)}$ï¼Œç§°ä¹‹ä¸º<em>æ ‡è®°ç‚¹</em>ã€‚ä»»ç»™ $x$ï¼Œæˆ‘ä»¬é€šè¿‡è®¡ç®—å…¶ä¸å„æ ‡è®°ç‚¹çš„ä¸´è¿‘ç¨‹åº¦å¾—åˆ°ä¸€ç»„ $f_i$ï¼š<br>$$<br>f_i = \mathop{\textrm{similarity}}(x,l^{(i)}) = \exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2}) = \exp(-\frac{\sum_{j=1}^n(x_j-l_j^{(i)})^2}{2\sigma^2})<br>$$<br>è¿™é‡Œå…·ä½“çš„ similarity å‡½æ•°ç§°ä¸º<em>æ ¸å‡½æ•°</em>ï¼Œæ ¸å‡½æ•°å¤šç§å¤šæ ·ã€‚æˆ‘ä»¬è¿™é‡Œå†™çš„æ˜¯å¾ˆå¸¸ç”¨çš„ $\exp(-\frac{\sum_{j=1}^n(x_j-l_j^{(i)})^2}{2\sigma^2})$ ï¼Œç§°ä¸º Gaussian Kernelï¼Œä»–çš„ä»£ç å®ç°å¦‚ä¸‹ï¼š</p>
<p>ç”±è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬çŸ¥é“ï¼Œç»™å®š $x$ï¼Œå¯¹æ¯ä¸ª $l^{(i)}$ æˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ª $f_i$ï¼Œæ»¡è¶³ï¼š</p>
<ol>
<li>å½“ $x$ æ¥è¿‘ $l^{(i)}$ æ—¶</li>
</ol>
<p>$$<br>f_i \approx \lim_{x\to l^{(i)}}\exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2}) = \exp(-\frac{0^2}{2\sigma^2}) = 1<br>$$</p>
<ol start="2">
<li>å½“ $x$ è¿œç¦» $l^{(i)}$ æ—¶</li>
</ol>
<p>$$<br>f_i \approx \lim_{||x-l^{(i)}||\to +\infin}\exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2}) = \exp(-\frac{\infin^2}{2\sigma^2}) = 0<br>$$</p>
<p>$\sigma$ çš„é€‰æ‹©ä¼šå½±å“ $f_i$ å€¼éš $x$ è¿œç¦» $l^{(i)}$ è€Œä¸‹é™çš„é€Ÿåº¦ï¼Œ$\sigma^2$ è¶Šå¤§ï¼Œ$f_i$å‡å°åœ°è¶Šæ…¢ï¼š</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jmrwo86pj30ph0dwwm1.jpg" alt="image-20191102132852867"></p>
<p>ä½¿ç”¨æ ¸æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥åšå‡ºè¿™ç§é¢„æµ‹ï¼š</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jmzc2qq4j30ot0ditej.jpg" alt="image-20191102133600603"></p>
<p>å½“ä¸”ä»…å½“ç»™å®š$x$ä¸´è¿‘$l^{(1)}$ æˆ– $l^{(2)}$ æ—¶é¢„æµ‹ 1ï¼Œå¦åˆ™é¢„æµ‹ 0.</p>
<h5 id="SVM-ä¸­ä½¿ç”¨æ ¸"><a href="#SVM-ä¸­ä½¿ç”¨æ ¸" class="headerlink" title="SVM ä¸­ä½¿ç”¨æ ¸"></a>SVM ä¸­ä½¿ç”¨æ ¸</h5><ol>
<li><p>ç»™å®šè®­ç»ƒé›† $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),â€¦,(x^{(m)},y^{(m)})$</p>
</li>
<li><p>é€‰æ‹©æ ‡è®°ç‚¹ï¼š$l^{(i)}=x^{(i)} \quad \textrm{for }i=1,2,\cdots,m$</p>
</li>
<li><p>å¯¹äºæ ·æœ¬ $x$ï¼Œè®¡ç®—æ ¸ï¼š$f_i=\mathop{\textrm{similarity}}(x,l^{(i)}) \quad \textrm{for }i=1,2,\cdots,m$</p>
</li>
<li><p>ä»¤ $f=[f_0,f_1,f_2,\cdots,f_m]^T$ï¼Œå…¶ä¸­ $f_0 \equiv 1$ã€‚</p>
</li>
</ol>
<p>é¢„æµ‹ï¼š</p>
<ul>
<li>ç»™å®š $x$ï¼Œè®¡ç®— $f\in\R^{m+1}$</li>
<li>é¢„æµ‹ $y=1$ å¦‚æœ $\theta^Tf=\theta_0f_0+\theta_1f_1+\cdots\ge 0$</li>
</ul>
<p>è®­ç»ƒï¼š<br>$$<br>\min_\theta<br>C\sum_{i=1}^m \large[ y^{(i)} \mathop{\textrm{cost}_1}(\theta^Tf^{(i)})</p>
<ul>
<li>(1 - y^{(i)})\ \mathop{\textrm{cost}_0}(\theta^Tf^{(i)})\large]</li>
<li>\frac{1}{2}\sum_{j=1}^n \theta_j^2<br>$$<br>å®ç°ï¼š</li>
</ul>
<p>æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¯¸å¦‚ liblinear, libsvm ä¹‹ç±»çš„åº“æ¥å¾—åˆ° SVM çš„ å‚æ•° $\theta$ï¼Œè¦ä½¿ç”¨è¿™äº›åº“ï¼Œæˆ‘ä»¬ä¸€èˆ¬éœ€è¦åšä»¥ä¸‹å·¥ä½œï¼š</p>
<ul>
<li>é€‰æ‹©å‚æ•° $C$</li>
<li>é€‰æ‹©æ ¸å‡½æ•°<ul>
<li><strong>No kernel</strong>ï¼ˆå³çº¿æ€§æ ¸ï¼Œäº¦å³åšé€»è¾‘å›å½’ï¼šPredict $y=1$ if $\theta^Tx\ge0$ï¼‰ï¼Œé€‚ç”¨äº <strong>nå¤§ må°</strong> çš„æƒ…å†µï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰</li>
<li><strong>Gaussian kernel</strong>ï¼ˆ$f_i=\exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2})\textrm{ where } l^{(i)}=x^{(i)} \textrm{ for } i=1,\cdots,m$ï¼‰é€‚ç”¨äº <strong>må¤§ nå°</strong> çš„æƒ…å†µï¼ˆå¯æ‹Ÿåˆæ›´å¤æ‚çš„éçº¿æ€§è¾¹ç•Œï¼‰</li>
</ul>
</li>
<li>æä¾›æ ¸å‡½æ•°ï¼ˆGaussian kernel ä¸ºä¾‹ï¼‰ï¼š</li>
</ul>
<p>$$<br>\begin{array}{l}<br>\textrm{function f = kernel(x1, x2)}\<br>\qquad \textrm{f} = \exp(-\frac{||\textrm{x1}-\textrm{x2}||^2}{2\sigma^2})\<br>\textrm{return}<br>\end{array}<br>$$</p>
<p>æ³¨æ„ï¼šä½¿ç”¨ Gaussian Kernel å‰åŠ¡å¿…åšç‰¹å¾ç¼©æ”¾ï¼</p>
<p>ğŸ‘‰æ— æ ¸å‡½æ•°çš„ä»£ç å®ç°ï¼š</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">sim</span> = <span class="title">linearKernel</span><span class="params">(x1, x2)</span></span></span><br><span class="line"><span class="comment">%LINEARKERNEL returns a linear kernel between x1 and x2</span></span><br><span class="line"><span class="comment">%   sim = linearKernel(x1, x2) returns a linear kernel between x1 and x2</span></span><br><span class="line"><span class="comment">%   and returns the value in sim</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Ensure that x1 and x2 are column vectors</span></span><br><span class="line">x1 = x1(:); x2 = x2(:);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Compute the kernel</span></span><br><span class="line">sim = x1&#x27; * x2;  <span class="comment">% dot product</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>ğŸ‘‰é«˜æ–¯æ ¸çš„ä»£ç å®ç°ï¼š</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">sim</span> = <span class="title">gaussianKernel</span><span class="params">(x1, x2, sigma)</span></span></span><br><span class="line"><span class="comment">%RBFKERNEL returns a radial basis function kernel between x1 and x2</span></span><br><span class="line"><span class="comment">%   sim = gaussianKernel(x1, x2) returns a gaussian kernel between x1 and x2</span></span><br><span class="line"><span class="comment">%   and returns the value in sim</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Ensure that x1 and x2 are column vectors</span></span><br><span class="line">x1 = x1(:); x2 = x2(:);</span><br><span class="line"></span><br><span class="line">sim = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">sim = <span class="built_in">exp</span>(-sum((x1 - x2).^<span class="number">2</span>) / (<span class="number">2</span> * sigma ^ <span class="number">2</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>ğŸ‘‰SVM ç¤ºä¾‹ï¼š</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% Load X, y, Xtest and ytest</span></span><br><span class="line">load(<span class="string">&#x27;data.mat&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% SVM Parameters</span></span><br><span class="line">C = <span class="number">1</span>;         <span class="comment">% C = 1 ~ 100 is fine</span></span><br><span class="line">sigma = <span class="number">0.1</span>;    <span class="comment">% sigma = 0.03 ~ 0.1 gives somewhat good boundary, less is better</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% We set the tolerance and max_passes lower here so that the code will run</span></span><br><span class="line"><span class="comment">% faster. However, in practice, you will want to run the training to</span></span><br><span class="line"><span class="comment">% convergence.</span></span><br><span class="line">model= svmTrain(X, y, C, @(x1, x2) gaussianKernel(x1, x2, sigma)); </span><br><span class="line">p = svmPredict(model, Xtest);</span><br><span class="line"></span><br><span class="line">fprintf(<span class="string">&#x27;Test Accuracy: %f\n&#x27;</span>, <span class="built_in">mean</span>(double(p == ytest)) * <span class="number">100</span>);</span><br></pre></td></tr></table></figure>
<h5 id="å¤šå…ƒåˆ†ç±»-2"><a href="#å¤šå…ƒåˆ†ç±»-2" class="headerlink" title="å¤šå…ƒåˆ†ç±»"></a>å¤šå…ƒåˆ†ç±»</h5><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jul8myufj30mv0cz0xu.jpg" alt="image-20191102171432072"></p>
<h4 id="é€»è¾‘å›å½’-vs-ç¥ç»ç½‘ç»œ-vs-SVM"><a href="#é€»è¾‘å›å½’-vs-ç¥ç»ç½‘ç»œ-vs-SVM" class="headerlink" title="é€»è¾‘å›å½’ vs ç¥ç»ç½‘ç»œ vs SVM"></a>é€»è¾‘å›å½’ vs ç¥ç»ç½‘ç»œ vs SVM</h4><p>$n$ = ç‰¹å¾æ•°ï¼ˆ$x\in\R^{n+1}$ï¼‰</p>
<p>$m$ = è®­ç»ƒæ ·æœ¬æ•°</p>
<ul>
<li>nç›¸å¯¹äºmå¤§ ï¼ˆe.g. $n=10,000, m=10 \sim 1000$ï¼‰<ul>
<li>é€»è¾‘å›å½’ï¼Œæˆ– æ— æ ¸SVM</li>
</ul>
</li>
<li>nå°ã€mé€‚ä¸­ï¼ˆe.g. $n=1\sim1000,m=50,000$ï¼‰<ul>
<li>ç”¨ Gaussian æ ¸ SVM</li>
</ul>
</li>
<li>nå°ã€må¤§<ul>
<li>åˆ›é€ /æ·»åŠ ç‰¹å¾ï¼Œç„¶åç”¨ é€»è¾‘å›å½’æˆ–æ— æ ¸SVM</li>
</ul>
</li>
</ul>
<p>ç¥ç»ç½‘ç»œé€šå¸¸å¯ä»¥è§£å†³ä¸Šè¿°ä»»ä½•ä¸€ç§æƒ…å†µï¼Œä½†å¯èƒ½ç›¸å¯¹è¾ƒæ…¢ã€‚</p>
<h2 id="æ— ç›‘ç£å­¦ä¹ "><a href="#æ— ç›‘ç£å­¦ä¹ " class="headerlink" title="æ— ç›‘ç£å­¦ä¹ "></a>æ— ç›‘ç£å­¦ä¹ </h2><p>æ— ç›‘ç£å­¦ä¹ æ˜¯åªç»™xæ•°æ®çš„ï¼Œä¸ç»™yã€‚</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8n62mi54pj30lw0catay.jpg" alt="å±å¹•å¿«ç…§ 2019-11-05 14.49.05"></p>
<h3 id="K-Means-èšç±»"><a href="#K-Means-èšç±»" class="headerlink" title="K-Means èšç±»"></a>K-Means èšç±»</h3><blockquote>
<p>æŠŠä¸€å †ä¸œè¥¿è‡ªåŠ¨åˆ†æˆKå †ã€‚</p>
</blockquote>
<p>è¾“å…¥ï¼š</p>
<ul>
<li>$K$ï¼šèšç±»çš„ä¸ªæ•°</li>
<li>${x^{(1)},x^{(2)},\cdots,x^{(m)}}$ï¼šè®­ç»ƒé›†</li>
</ul>
<p>è¾“å‡ºï¼š</p>
<ul>
<li>$K$ ä¸ªç±»</li>
</ul>
<p>K-Meansç®—æ³•ï¼š<br>$$<br>\begin{array}{l}</p>
<p>\textrm{Randomly initialize $K$ cluster centroids $\mu_1, \mu_2,â€¦\mu_k \in \R^n$}\<br>\textrm{Repeat }{\<br>\qquad \textrm{for $i=1$ to $m$:}\qquad\textrm{// Cluster assignment step}\<br>\qquad\qquad c^{(i)} := k \ \textrm{ s.t. } \min_k||x^{(i)}-\mu_k||^2 \<br>\qquad \textrm{for $k=1$ to $K$:}\qquad\textrm{// Move centroid step}\<br>\qquad\qquad \mu_k:= \textrm{average (mean) of points assigned to cluster $k$}\<br>}\</p>
<p>\end{array}<br>$$<br>ä»£ä»·å‡½æ•°ï¼š<br>$$<br>J(c^{(1)},\cdots,c^{(m)},\mu_1,\cdots,\mu_K)=\frac{1}{m}\sum_{i=1}^{m}||x^{(i)}-\mu_{c^{(i)}}||^2<br>$$<br>ä¼˜åŒ–ç›®æ ‡ï¼š<br>$$<br>\min_{<br>\begin{array}{c}<br>    {1}c^{(1)},\cdots,c^{(m)},\<br>    \mu_1,\cdots,\mu_K<br>\end{array}}<br>J(c^{(1)},\cdots,c^{(m)},\mu_1,\cdots,\mu_K)<br>$$<br>å¾—åˆ°è¾ƒä¼˜è§£(ä¸ä¸€å®šèƒ½å¾—åˆ°æœ€ä¼˜è§£)çš„ç®—æ³•ï¼š<br>$$<br>\begin{array}{l}<br>\textrm{For $i=1$ to $100$ &lt;or 50~1000&gt; {}\<br>\qquad\textrm{Randomly initialize K-means.}\<br>\qquad\textrm{Run K-means. Get $c^{(1)},\cdots,c^{(m)},\mu_1,\cdots,\mu_k$}\<br>\qquad\textrm{Compute cost function (distortion):}\<br>\qquad\qquad J(c^{(1)},\cdots,c^{(m)},\mu_1,\cdots,\mu_K)\<br>\textrm{}}\<br>\textrm{pick clustering that gave lowest $J$.}<br>\end{array}<br>$$<br>$K$çš„é€‰æ‹©ï¼š</p>
<ol>
<li>æ›´å…·å®é™…é—®é¢˜çš„éœ€æ±‚æ˜“å¾—ï¼›</li>
<li>é€‰æ‹©æ‹ç‚¹ï¼š<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaxja4d68ej30cb0c3jro.jpg" alt="image-20191106171612460"></li>
</ol>
<p>ğŸ‘‰<strong>ä»£ç å®ç°</strong></p>
<ol>
<li>æ‰¾æœ€è¿‘çš„ç±»ä¸­å¿ƒï¼š</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">idx</span> = <span class="title">findClosestCentroids</span><span class="params">(X, centroids)</span></span></span><br><span class="line"><span class="comment">%FINDCLOSESTCENTROIDS computes the centroid memberships for every example</span></span><br><span class="line"><span class="comment">%   idx = FINDCLOSESTCENTROIDS (X, centroids) returns the closest centroids</span></span><br><span class="line"><span class="comment">%   in idx for a dataset X where each row is a single example. idx = m x 1 </span></span><br><span class="line"><span class="comment">%   vector of centroid assignments (i.e. each entry in range [1..K])</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Set K</span></span><br><span class="line">K = <span class="built_in">size</span>(centroids, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">idx = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X,<span class="number">1</span>), <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">size</span>(X, <span class="number">1</span>)</span><br><span class="line">    min_j = <span class="number">0</span>;</span><br><span class="line">    min_l = Inf;</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : <span class="built_in">size</span>(centroids, <span class="number">1</span>)</span><br><span class="line">        l = sum((X(<span class="built_in">i</span>, :) - centroids(<span class="built_in">j</span>, :)) .^ <span class="number">2</span>);</span><br><span class="line">        <span class="keyword">if</span> l &lt;= min_l</span><br><span class="line">            min_j = <span class="built_in">j</span>;</span><br><span class="line">            min_l = l;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    idx(<span class="built_in">i</span>) = min_j;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>è®¡ç®—ä¸­å¿ƒ:</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">centroids</span> = <span class="title">computeCentroids</span><span class="params">(X, idx, K)</span></span></span><br><span class="line"><span class="comment">%COMPUTECENTROIDS returns the new centroids by computing the means of the </span></span><br><span class="line"><span class="comment">%data points assigned to each centroid.</span></span><br><span class="line"><span class="comment">%   centroids = COMPUTECENTROIDS(X, idx, K) returns the new centroids by </span></span><br><span class="line"><span class="comment">%   computing the means of the data points assigned to each centroid. It is</span></span><br><span class="line"><span class="comment">%   given a dataset X where each row is a single data point, a vector</span></span><br><span class="line"><span class="comment">%   idx of centroid assignments (i.e. each entry in range [1..K]) for each</span></span><br><span class="line"><span class="comment">%   example, and K, the number of centroids. You should return a matrix</span></span><br><span class="line"><span class="comment">%   centroids, where each row of centroids is the mean of the data points</span></span><br><span class="line"><span class="comment">%   assigned to it.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Useful variables</span></span><br><span class="line">[m n] = <span class="built_in">size</span>(X);</span><br><span class="line"></span><br><span class="line">centroids = <span class="built_in">zeros</span>(K, n);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : K</span><br><span class="line">    ck = <span class="built_in">find</span>(idx == <span class="built_in">i</span>);</span><br><span class="line">    centroids(<span class="built_in">i</span>, :) = sum(X(ck,:)) / <span class="built_in">size</span>(ck, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ol start="3">
<li>è¿è¡ŒK-Means</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[centroids, idx]</span> = <span class="title">runkMeans</span><span class="params">(X, initial_centroids, ...</span></span></span><br><span class="line"><span class="function"><span class="params">                                      max_iters, plot_progress)</span></span></span><br><span class="line"><span class="comment">%RUNKMEANS runs the K-Means algorithm on data matrix X, where each row of X</span></span><br><span class="line"><span class="comment">%is a single example</span></span><br><span class="line"><span class="comment">%   [centroids, idx] = RUNKMEANS(X, initial_centroids, max_iters, ...</span></span><br><span class="line"><span class="comment">%   plot_progress) runs the K-Means algorithm on data matrix X, where each </span></span><br><span class="line"><span class="comment">%   row of X is a single example. It uses initial_centroids used as the</span></span><br><span class="line"><span class="comment">%   initial centroids. max_iters specifies the total number of interactions </span></span><br><span class="line"><span class="comment">%   of K-Means to execute. plot_progress is a true/false flag that </span></span><br><span class="line"><span class="comment">%   indicates if the function should also plot its progress as the </span></span><br><span class="line"><span class="comment">%   learning happens. This is set to false by default. runkMeans returns </span></span><br><span class="line"><span class="comment">%   centroids, a Kxn matrix of the computed centroids and idx, a m x 1 </span></span><br><span class="line"><span class="comment">%   vector of centroid assignments (i.e. each entry in range [1..K])</span></span><br><span class="line"><span class="comment">% è‹¥ä½¿ç”¨ plot_progress éœ€è¦é¢å¤–çš„ç”»å›¾å‡½æ•°å®ç°ï¼Œè¿™é‡Œæ²¡æœ‰ç»™å‡º.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Set default value for plot progress</span></span><br><span class="line"><span class="keyword">if</span> ~exist(<span class="string">&#x27;plot_progress&#x27;</span>, <span class="string">&#x27;var&#x27;</span>) || <span class="built_in">isempty</span>(plot_progress)</span><br><span class="line">    plot_progress = <span class="built_in">false</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Plot the data if we are plotting progress</span></span><br><span class="line"><span class="keyword">if</span> plot_progress</span><br><span class="line">    <span class="built_in">figure</span>;</span><br><span class="line">    <span class="built_in">hold</span> on;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize values</span></span><br><span class="line">[m n] = <span class="built_in">size</span>(X);</span><br><span class="line">K = <span class="built_in">size</span>(initial_centroids, <span class="number">1</span>);</span><br><span class="line">centroids = initial_centroids;</span><br><span class="line">previous_centroids = centroids;</span><br><span class="line">idx = <span class="built_in">zeros</span>(m, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Run K-Means</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:max_iters</span><br><span class="line">    </span><br><span class="line">    <span class="comment">% Output progress</span></span><br><span class="line">    fprintf(<span class="string">&#x27;K-Means iteration %d/%d...\n&#x27;</span>, <span class="built_in">i</span>, max_iters);</span><br><span class="line">    <span class="keyword">if</span> exist(<span class="string">&#x27;OCTAVE_VERSION&#x27;</span>)</span><br><span class="line">        fflush(stdout);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">% For each example in X, assign it to the closest centroid</span></span><br><span class="line">    idx = findClosestCentroids(X, centroids);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">% Optionally, plot progress here</span></span><br><span class="line">    <span class="keyword">if</span> plot_progress</span><br><span class="line">        plotProgresskMeans(X, centroids, previous_centroids, idx, K, <span class="built_in">i</span>);</span><br><span class="line">        previous_centroids = centroids;</span><br><span class="line">        fprintf(<span class="string">&#x27;Press enter to continue.\n&#x27;</span>);</span><br><span class="line">        input(<span class="string">&quot;...&quot;</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">% Given the memberships, compute new centroids</span></span><br><span class="line">    centroids = computeCentroids(X, idx, K);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Hold off if we are plotting progress</span></span><br><span class="line"><span class="keyword">if</span> plot_progress</span><br><span class="line">    <span class="built_in">hold</span> off;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ol start="4">
<li>é©±åŠ¨è„šæœ¬ï¼š</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% Load an example dataset</span></span><br><span class="line">load(<span class="string">&#x27;data.mat&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Settings for running K-Means</span></span><br><span class="line">K = <span class="number">3</span>;</span><br><span class="line">max_iters = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% For consistency, here we set centroids to specific values</span></span><br><span class="line"><span class="comment">% but in practice you want to generate them automatically, such as by</span></span><br><span class="line"><span class="comment">% settings them to be random examples (as can be seen in</span></span><br><span class="line"><span class="comment">% kMeansInitCentroids).</span></span><br><span class="line">initial_centroids = [<span class="number">3</span> <span class="number">3</span>; <span class="number">6</span> <span class="number">2</span>; <span class="number">8</span> <span class="number">5</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">% Run K-Means algorithm. The &#x27;true&#x27; at the end tells our function to plot</span></span><br><span class="line"><span class="comment">% the progress of K-Means</span></span><br><span class="line">[centroids, idx] = runkMeans(X, initial_centroids, max_iters, <span class="built_in">true</span>);</span><br><span class="line">fprintf(<span class="string">&#x27;\nK-Means Done.\n\n&#x27;</span>);</span><br></pre></td></tr></table></figure>
<h3 id="PCA-ç»´æ•°çº¦å‡"><a href="#PCA-ç»´æ•°çº¦å‡" class="headerlink" title="PCA ç»´æ•°çº¦å‡"></a>PCA ç»´æ•°çº¦å‡</h3><blockquote>
<p>ä¸»æˆåˆ†åˆ†æï¼šæŠŠnç»´çš„æ•°æ®(æŠ•å½±)é™åˆ°kç»´ï¼Œç•¥å»ä¸é‡è¦çš„éƒ¨åˆ†(k&lt;=n)ã€‚</p>
</blockquote>
<p><strong>PCAç®—æ³•</strong>ï¼š</p>
<ol>
<li><p>æ•°æ®é¢„å¤„ç†</p>
<p>è®­ç»ƒé›†ï¼š$x^{(1)},x^{(2)},\cdots,x^{(m)}$</p>
<p>é¢„å¤„ç†(feature scaling &amp; mean normalization):</p>
<ul>
<li><p>$\mu_j=\frac{1}{m}\sum_{i=1}^m x_j^{(i)},\qquad s_j=\textrm{standard deviation of feature }j$</p>
</li>
<li><p>Replace each $x_j^{(i)}$ with $\frac{x_j-\mu_j}{s_j}$</p>
</li>
</ul>
</li>
</ol>
<p>2)é™ç»´</p>
<ol>
<li>è®¡ç®—åæ–¹å·®çŸ©é˜µ$\Sigma$ï¼ˆè¿™ä¸ªçŸ©é˜µè®°åšå¤§Sigmaï¼Œæ³¨æ„å’Œæ±‚å’Œå·åŒºåˆ†ï¼‰ï¼š<br>$$<br>\Sigma = \frac{1}{m}\sum_{i=1}^n(x^{(i)})(x^{(i)})^T<br>$$</li>
<li>æ±‚$\Sigma$çš„ç‰¹å¾å€¼(å®é™…ä¸Šæ˜¯å¥‡å¼‚å€¼åˆ†è§£)ï¼š<code>[U, S, V] = svd(Sigma);</code></li>
<li>ä»ä¸Šä¸€æ­¥svdå¾—åˆ°:<br>$$<br>U = \left[\begin{array}{cccc}<br>| &amp; | &amp;  &amp; |\<br>u^{(1)} &amp; u^{(2)} &amp; \cdots &amp; u^{(n)}\<br>| &amp; | &amp;  &amp; |<br>\end{array}\right]<br>\in \R^{n\times n}<br>\Rightarrow<br>U_{reduce}=\left[\begin{array}{cccc}<br>| &amp; | &amp;  &amp; |\<br>u^{(1)} &amp; u^{(2)} &amp; \cdots &amp; u^{(k)}\<br>| &amp; | &amp;  &amp; |<br>\end{array}\right]<br>$$</li>
<li>å®Œæˆé™ç»´ï¼š$x\in\R^n\to z\in\R^k$:<br>$$<br>z = U_{reduce}^Tx<br>=\left[\begin{array}{ccc}</li>
</ol>
<p>   â€“ &amp; (u^{(1)})^T &amp; â€“\<br>    &amp; \vdots &amp; \<br>   â€“ &amp; (u^{(k)})^T &amp; â€“\<br>   \end{array}\right]x<br>$$</p>
<p>ğŸ‘‰ä»£ç å®ç°ï¼š</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% do feature scaling &amp; mean normalization</span></span><br><span class="line"></span><br><span class="line">Sigma = <span class="number">1</span>/m * X&#x27; * X;</span><br><span class="line">[U, S, V] = svd(Sigma);</span><br><span class="line"></span><br><span class="line">Ureduce = U(:, <span class="number">1</span>:K);</span><br><span class="line">Z = X * Ureduce;</span><br></pre></td></tr></table></figure>
<p><strong>æ•°æ®å¤åŸ</strong>ï¼šå°†æ•°æ®è¿˜åŸåˆ°åŸæ¥çš„ç»´åº¦ï¼ˆ$z\in\R^k \to x_{approx}\in\R^n$ï¼‰ï¼š<br>$$<br>x_{approx}=U_{reduce}z<br>$$<br>ä¸€èˆ¬æƒ…å†µä¸‹ $x \neq x_{approx}$ï¼Œæˆ‘ä»¬åªèƒ½æœŸæœ› $x_{approx}$ å°½é‡æ¥è¿‘ $x$. </p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8ta88gafkj30oz0djaeo.jpg" alt="image-20191110215011122"></p>
<p><strong>$k$(ä¸»æˆåˆ†ä¸ªæ•°)çš„é€‰æ‹©</strong></p>
<p>ä¸€èˆ¬ï¼Œé€‰æ‹© $k$ ä¸ºä½¿å¾—ä¸‹å¼æˆç«‹çš„æœ€å°å€¼ï¼š<br>$$<br>\frac{\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\frac{1}{m}\sum_{i=1}^{m}||x^{(i)}||^2}\le0.01<br>$$<br>ç®—æ³•ï¼š<br>$$<br>\begin{array}{l}<br>\textrm{Try PCA with } k=1,\cdots,n:\<br>\quad \textrm{Compute } U_{reduce},z^{(1)},\cdots,z^{(m)},x_{approx}^{(1)},\cdots,x_{approx}^{m}\<br>\quad \textrm{Check if } \frac{\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\frac{1}{m}\sum_{i=1}^{m}||x^{(i)}||^2}\le0.01<br>\end{array}<br>$$</p>
<h3 id="å¼‚å¸¸æ£€æµ‹"><a href="#å¼‚å¸¸æ£€æµ‹" class="headerlink" title="å¼‚å¸¸æ£€æµ‹"></a>å¼‚å¸¸æ£€æµ‹</h3><blockquote>
<p>ä»ä¸€å †æ•°æ®ä¸­æ‰¾å‡ºå¼‚å¸¸äºå…¶ä»–çš„ã€‚</p>
</blockquote>
<p>é—®é¢˜æè¿°ï¼šç»™å®šæ•°æ®é›† ${x^{(1)},x^{(2)},\cdots,x^{(m)}}$ï¼Œé€šè¿‡è®­ç»ƒï¼Œåˆ¤æ–­ $x_{test}$ æ˜¯å¦å¼‚å¸¸ã€‚</p>
<p>è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹$p(x)$ï¼ˆæ¦‚ç‡ï¼‰å»ºç«‹ä¸€ä¸ªæ¨¡å‹ï¼Œé€‰æ‹©ä¸€ä¸ªä¸´ç•Œå€¼ $\epsilon$ï¼Œä½¿ï¼š<br>$$<br>\begin{array}{l}<br>p(x_{test})&lt;\epsilon \Rightarrow \textrm{anomaly}\<br>p(x_{test})\ge\epsilon \Rightarrow \textrm{OK}<br>\end{array}<br>$$<br>è¿™æ ·é—®é¢˜å¯ä»¥è½¬åŒ–ä¸º<em>å¯†åº¦å€¼ä¼°è®¡</em>ã€‚æˆ‘ä»¬å¸¸ç”¨é«˜æ–¯åˆ†å¸ƒè§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p>
<h4 id="é«˜æ–¯åˆ†å¸ƒ"><a href="#é«˜æ–¯åˆ†å¸ƒ" class="headerlink" title="é«˜æ–¯åˆ†å¸ƒ"></a>é«˜æ–¯åˆ†å¸ƒ</h4><p>$x$ æœä»é«˜æ–¯åˆ†å¸ƒï¼š$x \sim \mathcal{N}(\mu,\sigma^2)$</p>
<p>åˆ™ï¼Œ$x$ çš„æ¦‚ç‡ä¸ºï¼š<br>$$<br>p(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)<br>$$<br>å…¶ä¸­å‚æ•° $\mu$ å’Œ $\sigma$ ç”±ä¸‹å¼ç¡®å®šï¼ˆè¿™æ˜¯åœ¨æœºå™¨å­¦ä¹ é‡Œå¸¸ç”¨çš„æ ¼å¼ï¼Œä¸ä¸€å®šå’Œæ•°å­¦é‡Œçš„ä¸€æ ·ï¼‰ï¼š<br>$$<br>\mu=\frac{1}{m}\sum_{i=1}^{m}x^{(i)}<br>$$</p>
<p>$$<br>\sigma^2=\frac{1}{m}\sum_{i=1}^{m}\left(x^{(i)}-\mu\right)^2<br>$$</p>
<p>ğŸ‘‰ä»£ç å®ç°ï¼š</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[mu sigma2]</span> = <span class="title">estimateGaussian</span><span class="params">(X)</span></span></span><br><span class="line"><span class="comment">%ESTIMATEGAUSSIAN This function estimates the parameters of a </span></span><br><span class="line"><span class="comment">%Gaussian distribution using the data in X</span></span><br><span class="line"><span class="comment">%   [mu sigma2] = estimateGaussian(X), </span></span><br><span class="line"><span class="comment">%   The input X is the dataset with each n-dimensional data point in one row</span></span><br><span class="line"><span class="comment">%   The output is an n-dimensional vector mu, the mean of the data set</span></span><br><span class="line"><span class="comment">%   and the variances sigma^2, an n x 1 vector</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Useful variables</span></span><br><span class="line">[m, n] = <span class="built_in">size</span>(X);</span><br><span class="line"></span><br><span class="line">mu = <span class="built_in">zeros</span>(n, <span class="number">1</span>);</span><br><span class="line">sigma2 = <span class="built_in">zeros</span>(n, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">mu = <span class="built_in">mean</span>(X);</span><br><span class="line">sigma2 = var(X) * (m - <span class="number">1</span>) / m;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>


<p>å€Ÿæ­¤æˆ‘ä»¬ä¾¿å¯å¾—åˆ°å¼‚å¸¸æ£€æŸ¥ç®—æ³•ï¼š</p>
<h5 id="å¼‚å¸¸æ£€æŸ¥ç®—æ³•"><a href="#å¼‚å¸¸æ£€æŸ¥ç®—æ³•" class="headerlink" title="å¼‚å¸¸æ£€æŸ¥ç®—æ³•"></a>å¼‚å¸¸æ£€æŸ¥ç®—æ³•</h5><ol>
<li>é€‰æ‹©è®¤ä¸ºå¯èƒ½è¡¨ç°å‡ºæ ·æœ¬å¼‚å¸¸çš„æ•°æ®ç‰¹å¾ $x_i$</li>
<li>è®¡ç®—å‚æ•° $\mu_1,\cdots,\mu_n,\sigma_1^2,\cdots,\sigma_n^2$ </li>
</ol>
<p>$$<br>\mu=\frac{1}{m}\sum_{i=1}^{m}x^{(i)}<br>$$</p>
<p>$$<br>\sigma^2=\frac{1}{m}\sum_{i=1}^{m}\left(x^{(i)}-\mu\right)^2<br>$$</p>
<ol start="3">
<li>å¯¹äºæ–°ç»™çš„æ ·æœ¬ $x$ï¼Œè®¡ç®— $p(x)$ï¼š</li>
</ol>
<p>$$<br>p(x)=\prod_{j=1}^{n}p(x_j;\mu_j,\sigma_j^2)=\prod_{j=1}^{n}\frac{1}{\sqrt{2\pi}\sigma_j}\exp\left(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2}\right)<br>$$</p>
<ol start="4">
<li>å¦‚æœ$p(x)&lt;\epsilon$ï¼Œåˆ™é¢„æµ‹å¼‚å¸¸ã€‚</li>
</ol>
<h4 id="å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ"><a href="#å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ" class="headerlink" title="å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ"></a>å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ</h4><p>$$<br>p(x;\mu,\Sigma)=\frac<br>{\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)}<br>{\sqrt{(2\pi)^{n}|\Sigma|}}<br>$$</p>
<p>å‚æ•°ï¼š</p>
<ul>
<li>$\mu\in\R^n$</li>
<li>$\Sigma\in\R^{n\times n}$ (covariance matrix, <code>Sigma = 1/m * X&#39; * X;</code>)</li>
</ul>
<p>å‚æ•°çš„è®¡ç®—ï¼š<br>$$<br>\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)} \qquad<br>\Sigma=\frac{1}{m}\sum_{i=1}^m\left(x^{(i)}-\mu\right)\left(x^{(i)}-\mu\right)^T<br>$$</p>
<p>ğŸ‘‰ä»£ç å®ç°ï¼š</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">p</span> = <span class="title">multivariateGaussian</span><span class="params">(X, mu, Sigma2)</span></span></span><br><span class="line"><span class="comment">%MULTIVARIATEGAUSSIAN Computes the probability density function of the</span></span><br><span class="line"><span class="comment">%multivariate gaussian distribution.</span></span><br><span class="line"><span class="comment">%    p = MULTIVARIATEGAUSSIAN(X, mu, Sigma2) Computes the probability </span></span><br><span class="line"><span class="comment">%    density function of the examples X under the multivariate gaussian </span></span><br><span class="line"><span class="comment">%    distribution with parameters mu and Sigma2. If Sigma2 is a matrix, it is</span></span><br><span class="line"><span class="comment">%    treated as the covariance matrix. If Sigma2 is a vector, it is treated</span></span><br><span class="line"><span class="comment">%    as the \sigma^2 values of the variances in each dimension (a diagonal</span></span><br><span class="line"><span class="comment">%    covariance matrix)</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line">k = <span class="built_in">length</span>(mu);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">size</span>(Sigma2, <span class="number">2</span>) == <span class="number">1</span>) || (<span class="built_in">size</span>(Sigma2, <span class="number">1</span>) == <span class="number">1</span>)</span><br><span class="line">    Sigma2 = <span class="built_in">diag</span>(Sigma2);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">X = <span class="built_in">bsxfun</span>(@minus, X, mu(:)&#x27;);</span><br><span class="line">p = (<span class="number">2</span> * <span class="built_in">pi</span>) ^ (- k / <span class="number">2</span>) * det(Sigma2) ^ (<span class="number">-0.5</span>) * ...</span><br><span class="line">    <span class="built_in">exp</span>(<span class="number">-0.5</span> * sum(<span class="built_in">bsxfun</span>(@times, X * pinv(Sigma2), X), <span class="number">2</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<h5 id="ç”¨å¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„å¼‚å¸¸æ£€æŸ¥"><a href="#ç”¨å¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„å¼‚å¸¸æ£€æŸ¥" class="headerlink" title="ç”¨å¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„å¼‚å¸¸æ£€æŸ¥"></a>ç”¨å¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„å¼‚å¸¸æ£€æŸ¥</h5><ol>
<li>æ‹Ÿåˆå¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„ $p(x)$ æ¨¡å‹ï¼Œé€šè¿‡å‚æ•°ï¼š</li>
</ol>
<p>$$<br>\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)} \qquad<br>\Sigma=\frac{1}{m}\sum_{i=1}^m\left(x^{(i)}-\mu\right)\left(x^{(i)}-\mu\right)^T<br>$$</p>
<ol start="2">
<li>å¯¹äºæ–°ç»™ $x$ï¼Œè®¡ç®—ï¼š</li>
</ol>
<p>$$<br>p(x)=\frac<br>{\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)}<br>{\sqrt{(2\pi)^{n}|\Sigma|}}<br>$$</p>
<ol start="3">
<li>å¦‚æœ$p(x)&lt;\epsilon$ï¼Œåˆ™é¢„æµ‹å¼‚å¸¸ã€‚</li>
</ol>
<h4 id="é—¨æ§›é€‰æ‹©"><a href="#é—¨æ§›é€‰æ‹©" class="headerlink" title="é—¨æ§›é€‰æ‹©"></a>é—¨æ§›é€‰æ‹©</h4><p>é€šè¿‡è®¡ç®— $F_1$ å€¼å¯ä»¥å¾—åˆ°æœ€é€‚åˆçš„ $\epsilon$ã€‚</p>
<p>$F_1$ å€¼ç”± precision ($prec$) å’Œ recall ($rec$) ç»™å‡ºï¼š<br>$$<br>F_1=\frac{2\cdot prec \cdot rec}{prec+rec}<br>$$<br>å…¶ä¸­ï¼š<br>$$<br>prec = \frac{tp}{tp+fp}<br>$$</p>
<p>$$<br>rec = \frac{tp}{tp+fn}<br>$$</p>
<ul>
<li>$tp$ æ˜¯ true positivesï¼šé¢„æµ‹ä¸ºæ­£ï¼Œå®é™…ä¹Ÿä¸ºæ­£</li>
<li>$fp$ æ˜¯ false positivesï¼šé¢„æµ‹ä¸ºæ­£ï¼Œå®é™…ä¸ºè´Ÿ</li>
<li>$fn$ æ˜¯ false negativesï¼šé¢„æµ‹ä¸ºè´Ÿï¼Œå®é™…ä¸ºæ­£</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8bmwqbebxj30po0dgahc.jpg" alt="image-20191026152903469"></p>
<p>ğŸ‘‰ä»£ç å®ç°ï¼š</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[bestEpsilon bestF1]</span> = <span class="title">selectThreshold</span><span class="params">(yval, pval)</span></span></span><br><span class="line"><span class="comment">%SELECTTHRESHOLD Find the best threshold (epsilon) to use for selecting</span></span><br><span class="line"><span class="comment">%outliers</span></span><br><span class="line"><span class="comment">%   [bestEpsilon bestF1] = SELECTTHRESHOLD(yval, pval) finds the best</span></span><br><span class="line"><span class="comment">%   threshold to use for selecting outliers based on the results from a</span></span><br><span class="line"><span class="comment">%   validation set (pval) and the ground truth (yval).</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line">bestEpsilon = <span class="number">0</span>;</span><br><span class="line">bestF1 = <span class="number">0</span>;</span><br><span class="line">F1 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">stepsize = (<span class="built_in">max</span>(pval) - <span class="built_in">min</span>(pval)) / <span class="number">1000</span>;</span><br><span class="line"><span class="keyword">for</span> epsilon = <span class="built_in">min</span>(pval):stepsize:<span class="built_in">max</span>(pval)</span><br><span class="line"></span><br><span class="line">    cvPredictions = pval &lt; epsilon;</span><br><span class="line">    </span><br><span class="line">    tp = sum((cvPredictions == <span class="number">1</span>) &amp; (yval == <span class="number">1</span>));</span><br><span class="line">    fp = sum((cvPredictions == <span class="number">1</span>) &amp; (yval == <span class="number">0</span>));</span><br><span class="line">    fn = sum((cvPredictions == <span class="number">0</span>) &amp; (yval == <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    prec = tp / (tp + fp);</span><br><span class="line">    rec = tp / (tp + fn);</span><br><span class="line"></span><br><span class="line">    F1 = (<span class="number">2</span> * prec * rec) / (prec + rec);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> F1 &gt; bestF1</span><br><span class="line">       bestF1 = F1;</span><br><span class="line">       bestEpsilon = epsilon;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>


<h3 id="æ¨èç³»ç»Ÿ"><a href="#æ¨èç³»ç»Ÿ" class="headerlink" title="æ¨èç³»ç»Ÿ"></a>æ¨èç³»ç»Ÿ</h3><blockquote>
<p>é€šè¿‡è¯„åˆ†ï¼Œæ¨èç”¨æˆ·æ–°å†…å®¹ã€‚</p>
</blockquote>
<p>ç¬¦å·è¯´æ˜ï¼šï¼ˆå‡è®¾æˆ‘ä»¬è¦æ¨èçš„ä¸œè¥¿æ˜¯ç”µå½±ï¼‰</p>
<ul>
<li>$n_u$ = ç”¨æˆ·æ•°</li>
<li>$n_m$ = ç”µå½±æ•°</li>
<li>$r(i,j)=1$ è‹¥ç”¨æˆ· $j$ å¯¹ç”µå½± $i$ è¯„è¿‡åˆ†ï¼Œå¦åˆ™ä¸º 0</li>
<li>$y^{(i,j)}$ = ç”¨æˆ· $j$ ç»™ç”µå½± $i$ çš„è¯„åˆ†(åªæœ‰å½“ $r(i,j)=1$ æ—¶æ‰æœ‰å®šä¹‰)</li>
</ul>
<h4 id="åŸºäºå†…å®¹æ¨è"><a href="#åŸºäºå†…å®¹æ¨è" class="headerlink" title="åŸºäºå†…å®¹æ¨è"></a>åŸºäºå†…å®¹æ¨è</h4><p><strong>é¢„æµ‹æ¨¡å‹</strong>ï¼š</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9bi5bv4etj30nz06lac1.jpg" alt="å±å¹•å¿«ç…§ 2019-11-26 16.04.26"></p>
<ul>
<li>$r(i,j)=1$ è‹¥ç”¨æˆ· $j$ å¯¹ç”µå½± $i$ è¯„è¿‡åˆ†</li>
<li>$y^{(i,j)}$ = ç”¨æˆ· $j$ ç»™ç”µå½± $i$ çš„è¯„åˆ†(å¦‚æœæœ‰å®šä¹‰)</li>
<li>$\theta^{(j)}$ ç”¨æˆ· $j$ çš„å‚æ•°ï¼ˆå‘é‡ï¼‰</li>
<li>$x^{(i)}$ ç”µå½± $i$ çš„ç‰¹å¾ï¼ˆå‘é‡ï¼‰</li>
</ul>
<p>å¯¹äºç”¨æˆ· $j$ï¼Œç”µå½± $i$ï¼Œé¢„æµ‹è¯„åˆ†ï¼š$(\theta^{(j)})^T(x^{(i)})$ã€‚</p>
<p><strong>ä¼˜åŒ–ç›®æ ‡</strong>ï¼š</p>
<ol>
<li>ä¼˜åŒ– $\theta^{(j)}$ ï¼ˆå¯¹äºå•ä¸ªç”¨æˆ· $j$ çš„å‚æ•°ï¼‰</li>
</ol>
<p>$$<br>\min_{\theta^{(j)}}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{k=1}^n \left(\theta_k^{(j)}\right)^2<br>$$</p>
<ol start="2">
<li>ä¼˜åŒ– $\theta^{(1)},\theta^{(2)},\cdots,\theta^{(n_u)}$ï¼ˆå¯¹æ‰€æœ‰ç”¨æˆ·ï¼‰</li>
</ol>
<p>$$<br>\min_{\theta^{(1)},\cdots,\theta^{(n_u)}}<br>\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2 +<br>\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n \left(\theta_k^{(j)}\right)^2<br>$$</p>
<p>æˆ‘ä»¬å¯ä»¥ç”¨æ¢¯åº¦ä¸‹é™è§£å†³é—®é¢˜ï¼š<br>$$<br>\begin{array}{l}<br>Repeat\quad{\<br>\qquad \theta_0^{(j)}:=\theta_0^{(j)}-\alpha\sum_{i:r(i,j)=1} \big((\theta^{(j)})^T(x^{(i)})-y^{(i,j)}\big)x_0^{(i)}\<br>\qquad \theta_k^{(j)}:=\theta_k^{(j)}-\alpha\Big[\Big(\sum_{i:r(i,j)=1}\big((\theta^{(j)})^T(x^{(i)})-y^{(i)}\big)x_k^{(i)}\Big)+\lambda\theta_k^{(j)}\Big]\qquad (\textrm{for } k \neq 0)\<br>}<br>\end{array}<br>$$</p>
<h4 id="ååŒè¿‡æ»¤"><a href="#ååŒè¿‡æ»¤" class="headerlink" title="ååŒè¿‡æ»¤"></a>ååŒè¿‡æ»¤</h4><p>åœ¨åŸºäºå†…å®¹æ¨èä¸­æˆ‘ä»¬æœ‰æ—¶ä¼šå¾ˆéš¾æŠŠæ¡ç”µå½±ï¼ˆæˆ‘ä»¬è¦æ¨èçš„ä¸œè¥¿ï¼‰æœ‰å“ªäº›ç‰¹å¾ï¼ˆ$x^{(i)}$ï¼‰ï¼Œæˆ‘ä»¬æƒ³è®©æœºå™¨å­¦ä¹ è‡ªå·±æ‰¾ç‰¹å¾ï¼Œè¿™å°±ç”¨åˆ°ååŒè¿‡æ»¤ã€‚</p>
<p><strong>æ–°åŠ çš„ä¼˜åŒ–ç›®æ ‡</strong>ï¼šï¼ˆä¹‹å‰åœ¨åŸºäºå†…å®¹æ¨èé‡Œé¢çš„ä¼˜åŒ–ç›®æ ‡ä»éœ€è€ƒè™‘ï¼‰</p>
<ul>
<li>ç»™å®š $\theta^{(1)},\theta^{(2)},\cdots,\theta^{(n_u)}$ï¼Œå­¦ä¹  $x^{(i)}$:</li>
</ul>
<p>$$<br>\min_{x^{(i)}}\frac{1}{2}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{k=1}^n \left(x_k^{(i)}\right)^2<br>$$</p>
<ul>
<li>ç»™å®š $\theta^{(1)},\theta^{(2)},\cdots,\theta^{(n_u)}$ ï¼Œå­¦ä¹  $x^{(1)},\cdots,x^{(n_m)}$ ï¼š</li>
</ul>
<p>$$<br>\min_{x^{(1)},\cdots,x^{(n_m)}}\frac{1}{2}<br>\sum_{i=1}^{n_m}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2 +<br>\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n \left(x_k^{(i)}\right)^2<br>$$</p>
<p><strong>ååŒè¿‡æ»¤</strong>ï¼š</p>
<p>ç°åœ¨æˆ‘ä»¬çš„é—®é¢˜æ˜¯å³æ²¡æœ‰è®­ç»ƒå¥½çš„ $\theta$ï¼Œåˆæ²¡æœ‰ä¸€ç»„å……åˆ†ä¼˜åŒ–çš„ $x$ï¼Œä½†å­¦ä¹  $\theta$ è¦å…ˆæœ‰ $x$ï¼Œå­¦ä¹  $x$ è¦å…ˆæœ‰ $\theta$ã€‚è¿™å°±å˜æˆäº†ä¸€ä¸ªç±»ä¼¼é¸¡ç”Ÿè›‹ã€è›‹ç”Ÿé¸¡çš„é—®é¢˜ã€‚</p>
<p>æˆ‘ä»¬å¯ä»¥è€ƒè™‘è¿™æ ·è§£å†³è¿™ä¸ªéš¾é¢˜ï¼š</p>
<p>é¦–å…ˆéšæœºçŒœä¸€ç»„ $\theta$ï¼Œç„¶åç”¨è¿™ç»„ $\theta$ å°±å¯ä»¥å¾—åˆ°ä¸€ç»„ $x$ï¼›ç”¨è¿™ç»„å¾—åˆ°çš„ $x$ åˆå¯ä»¥ä¼˜åŒ– $\theta$ï¼Œä¼˜åŒ–åçš„ $\theta$ åˆæ‹¿æ¥ä¼˜åŒ– $x$ â€¦â€¦ ä¸æ–­é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥æœŸæœ›å¾—åˆ°ä¸€ç»„ $x$ å’Œ $\theta$ éƒ½å……åˆ†ä¼˜åŒ–çš„è§£ï¼ˆäº‹å®ä¸Šå®ƒä»¬æœ€ç»ˆæ˜¯ä¼šæ”¶æ•›çš„ï¼‰ã€‚</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g9ds8xkjatj30ob0bd0w0.jpg" alt="å±å¹•å¿«ç…§ 2019-11-28 15.20.59"><br>$$<br>\begin{array}{l}</p>
<p>\textrm{Given }<br>x^{(1)},\cdots,x^{(n_m)}<br>\textrm{ , estimate }<br>\theta^{(1)},\cdots,\theta^{(n_u)}:\<br>\quad<br>\min_{\theta^{(1)},\cdots,\theta^{(n_u)}}<br>\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2 +<br>\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n \left(\theta_k^{(j)}\right)^2\</p>
<p>\textrm {Given }<br>\theta^{(1)},\cdots,\theta^{(n_u)}<br>\textrm{ , estimate }<br>x^{(1)},\cdots,x^{(n_m)}:\<br>\quad<br>\min_{x^{(1)},\cdots,x^{(n_m)}}\frac{1}{2}<br>\sum_{i=1}^{n_m}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2 +<br>\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n \left(x_k^{(i)}\right)^2<br>\end{array}<br>$$<br>æˆ‘ä»¬éšæœºåˆå§‹åŒ–ä¸€ç»„å‚æ•°ï¼Œç„¶åé‡å¤æ¥å›è®¡ç®— $\theta$ å’Œ $x$ï¼Œæœ€ç»ˆä¼šå¾—åˆ°è§£ï¼Œä½†è¿™æ ·æ¯”è¾ƒéº»çƒ¦ï¼Œæˆ‘ä»¬å¯ä»¥åšçš„æ›´é«˜æ•ˆï¼š</p>
<p><strong>åŒæ—¶</strong>ä¼˜åŒ– $x^{(1)},\cdots,x^{(n_m)}$ å’Œ $\theta^{(1)},\cdots,\theta^{(n_u)}$:<br>$$<br>J(x^{(1)},\cdots,x^{(n_m)},\theta^{(1)},\cdots,\theta^{(n_u)})=<br>\frac{1}{2}<br>\sum_{(i,j):r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+<br>\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n \left(x_k^{(i)}\right)^2+<br>\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n \left(x_k^{(i)}\right)^2<br>$$</p>
<p>$$<br>\min_{\begin{array}{c}x^{(1)},\cdots,x^{(n_m)}\\theta^{(1)},\cdots,\theta^{(n_u)}\end{array}}<br>J(x^{(1)},\cdots,x^{(n_m)},\theta^{(1)},\cdots,\theta^{(n_u)})<br>$$</p>
<p><strong>ååŒè¿‡æ»¤ç®—æ³•</strong>ï¼š</p>
<ol>
<li>å°† $x^{(1)},\cdots,x^{(n_m)},\theta^{(1)},\cdots,\theta^{(n_u)}$ éšæœºåˆå§‹åŒ–ä¸ºä¸€äº›æ¯”è¾ƒå°çš„éšæœºå€¼</li>
<li>ä¼˜åŒ– $J(x^{(1)},\cdots,x^{(n_m)},\theta^{(1)},\cdots,\theta^{(n_u)})$</li>
<li>å¯¹äºç»™å®šç”¨æˆ·ï¼Œè¯¥ç”¨æˆ·çš„å‚æ•°æ˜¯ $\theta$ï¼Œåˆ™ç”¨è®­ç»ƒå¾—åˆ°çš„æŸç”µå½±çš„ç‰¹å¾ $x$ ï¼Œæˆ‘ä»¬å¯ä»¥é¢„æµ‹è¯¥ç”¨æˆ·å¯èƒ½ä¸ºæ­¤ç”µå½±è¯„åˆ†ï¼š$\theta^Tx$ã€‚</li>
</ol>
<p><strong>ä½ç§©çŸ©é˜µåˆ†è§£</strong>ï¼š</p>
<p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬æœ€ç»ˆçš„é¢„æµ‹æ˜¯è¿™æ ·çš„ï¼š<br>$$<br>Predict = \left[\begin{array}{ccccc} (x^{(1)})^T(\theta^{(1)}) &amp; \ldots &amp; (x^{(1)})^T(\theta^{(n_u)})\ \vdots &amp; \ddots &amp; \vdots \ (x^{(n_m)})^T(\theta^{(1)}) &amp; \ldots &amp; (x^{(n_m)})^T(\theta^{(n_u)})\end{array}\right]<br>$$<br>è€ƒè™‘åˆ°å‡ ä¹ä¸å¯èƒ½æœ‰ç”¨æˆ·æŠŠæ¥è¿‘æ‰€æœ‰çš„ç”µå½±éƒ½è¯„åˆ†ï¼Œè¿™ä¸ªé¢„æµ‹çŸ©é˜µæ˜¯ç¨€ç–çš„ï¼Œå­˜å‚¨è¿™ä¸ªçŸ©é˜µä¼šé€ æˆå¤§é‡æµªè´¹ï¼Œä¸å¦¨ä»¤ï¼š<br>$$<br>X = \left[\begin{array}{ccc}</p>
<ul>
<li>&amp; (x^{(1)})^T &amp; - \<br>&amp; \vdots &amp; \</li>
<li>&amp; (x^{(n_m)})^T &amp; - \<br>\end{array}\right],<br>\qquad<br>\Theta = \left[\begin{array}{ccc}</li>
<li>&amp; (\theta^{(1)})^T &amp; - \<br>&amp; \vdots &amp; \</li>
<li>&amp; (\theta^{(n_u)})^T &amp; - \<br>\end{array}\right]<br>$$<br>åˆ™æœ‰ï¼š<br>$$<br>Predict=X\Theta^T<br>$$<br>æˆ‘ä»¬ä¾¿å°†å®ƒåˆ†ä¸ºäº†ä¸¤éƒ¨åˆ†ã€‚ç”¨è¿™ä¸ªæ–¹æ³•æ±‚å– $X$ å’Œ $\Theta$ï¼Œè·å¾—æ¨èç³»ç»Ÿéœ€è¦çš„å‚æ•°ï¼Œç§°ä¹‹ä¸º<strong>ä½ç§©çŸ©é˜µåˆ†è§£</strong>ã€‚è¯¥æ–¹æ³•ä¸ä»…èƒ½åœ¨ç¼–ç¨‹æ—¶ç›´æ¥é€šè¿‡å‘é‡åŒ–çš„æ‰‹æ³•è·å¾—å‚æ•°ï¼Œè¿˜é€šè¿‡çŸ©é˜µåˆ†è§£èŠ‚çœäº†å†…å­˜ç©ºé—´ã€‚</li>
</ul>
<p><strong>å¯»æ‰¾ç›¸å…³ç”µå½±</strong>ï¼š</p>
<p>æˆ‘ä»¬å¸¸éœ€è¦æ¨èä¸ç”µå½± $i$ ç›¸å…³çš„ç”µå½± $j$ï¼Œå¯ä»¥è¿™æ ·æ‰¾åˆ°ï¼š<br>$$<br>\mathop{\textrm{smallest}} ||x^{(i)}-x^{(j)}||<br>$$<br><strong>å‡å€¼å½’ä¸€åŒ–å¤„ç†</strong>ï¼š</p>
<p>å†ç”µå½±æ¨èé—®é¢˜ä¸­ï¼Œç”±äºè¯„åˆ†æ€»æ˜¯1åˆ°5åˆ†ï¼ˆæˆ–å…¶ä»–èŒƒå›´ï¼‰ï¼Œæ•…ä¸ç”¨ç‰¹å¾ç¼©æ”¾ï¼Œä½†å¯ä»¥åš mean normalizationï¼š<br>$$<br>\mu_i=\mathop{\textrm{average}} y^{(i,:)}<br>$$</p>
<p>$$<br>Y_i = Y_i-\mu_i<br>$$</p>
<p>å¯¹ç”¨æˆ· $j$, ç”µå½± $i$, é¢„æµ‹:<br>$$<br>\left(\Theta^{(j)}\right)^T\left(x^{(i)}\right)+\mu_i<br>$$<br>ğŸ‘‰<strong>ä»£ç å®ç°</strong>ï¼š</p>
<ol>
<li>ä»£ä»·å‡½æ•°ï¼š</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J, grad]</span> = <span class="title">cofiCostFunc</span><span class="params">(params, Y, R, num_users, num_movies, ...</span></span></span><br><span class="line"><span class="function"><span class="params">                                  num_features, lambda)</span></span></span><br><span class="line"><span class="comment">%COFICOSTFUNC Collaborative filtering cost function</span></span><br><span class="line"><span class="comment">%   [J, grad] = COFICOSTFUNC(params, Y, R, num_users, num_movies, ...</span></span><br><span class="line"><span class="comment">%   num_features, lambda) returns the cost and gradient for the</span></span><br><span class="line"><span class="comment">%   collaborative filtering problem.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Unfold the U and W matrices from params</span></span><br><span class="line">X = <span class="built_in">reshape</span>(params(<span class="number">1</span>:num_movies*num_features), num_movies, num_features);</span><br><span class="line">Theta = <span class="built_in">reshape</span>(params(num_movies*num_features+<span class="number">1</span>:<span class="keyword">end</span>), ...</span><br><span class="line">                num_users, num_features);</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line">X_grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X));</span><br><span class="line">Theta_grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(Theta));</span><br><span class="line"></span><br><span class="line">h = X * Theta&#x27;;</span><br><span class="line">er = (h - Y) .* R;</span><br><span class="line"></span><br><span class="line">J = <span class="number">1</span>/<span class="number">2</span> * sum(sum(er.^<span class="number">2</span>));</span><br><span class="line">X_grad = er * Theta;</span><br><span class="line">Theta_grad = er&#x27; * X; </span><br><span class="line"></span><br><span class="line"><span class="comment">% Regularized</span></span><br><span class="line"></span><br><span class="line">J += lambda/<span class="number">2</span> *(sum(sum(Theta.^<span class="number">2</span>)) + sum(sum(X.^<span class="number">2</span>)));</span><br><span class="line">X_grad += lambda * X;</span><br><span class="line">Theta_grad += lambda * Theta;</span><br><span class="line">grad = [X_grad(:); Theta_grad(:)];</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li>å‡å€¼å½’ä¸€</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[Ynorm, Ymean]</span> = <span class="title">normalizeRatings</span><span class="params">(Y, R)</span></span></span><br><span class="line"><span class="comment">%NORMALIZERATINGS Preprocess data by subtracting mean rating for every </span></span><br><span class="line"><span class="comment">%movie (every row)</span></span><br><span class="line"><span class="comment">%   [Ynorm, Ymean] = NORMALIZERATINGS(Y, R) normalized Y so that each movie</span></span><br><span class="line"><span class="comment">%   has a rating of 0 on average, and returns the mean rating in Ymean.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line">[m, n] = <span class="built_in">size</span>(Y);</span><br><span class="line">Ymean = <span class="built_in">zeros</span>(m, <span class="number">1</span>);</span><br><span class="line">Ynorm = <span class="built_in">zeros</span>(<span class="built_in">size</span>(Y));</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:m</span><br><span class="line">    idx = <span class="built_in">find</span>(R(<span class="built_in">i</span>, :) == <span class="number">1</span>);</span><br><span class="line">    Ymean(<span class="built_in">i</span>) = <span class="built_in">mean</span>(Y(<span class="built_in">i</span>, idx));</span><br><span class="line">    Ynorm(<span class="built_in">i</span>, idx) = Y(<span class="built_in">i</span>, idx) - Ymean(<span class="built_in">i</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ol start="3">
<li>é©±åŠ¨è„šæœ¬</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%  Normalize Ratings</span></span><br><span class="line">[Ynorm, Ymean] = normalizeRatings(Y, R);</span><br><span class="line"></span><br><span class="line"><span class="comment">%  Useful Values</span></span><br><span class="line">num_users = <span class="built_in">size</span>(Y, <span class="number">2</span>);</span><br><span class="line">num_movies = <span class="built_in">size</span>(Y, <span class="number">1</span>);</span><br><span class="line">num_features = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Set Initial Parameters (Theta, X)</span></span><br><span class="line">X = <span class="built_in">randn</span>(num_movies, num_features);</span><br><span class="line">Theta = <span class="built_in">randn</span>(num_users, num_features);</span><br><span class="line"></span><br><span class="line">initial_parameters = [X(:); Theta(:)];</span><br><span class="line"></span><br><span class="line"><span class="comment">% Set options for fmincg</span></span><br><span class="line">options = optimset(<span class="string">&#x27;GradObj&#x27;</span>, <span class="string">&#x27;on&#x27;</span>, <span class="string">&#x27;MaxIter&#x27;</span>, <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Set Regularization</span></span><br><span class="line">lambda = <span class="number">10</span>;</span><br><span class="line">theta = fmincg (@(t)(cofiCostFunc(t, Ynorm, R, num_users, num_movies, ...</span><br><span class="line">                                num_features, lambda)), ...</span><br><span class="line">                initial_parameters, options);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Unfold the returned theta back into U and W</span></span><br><span class="line">X = <span class="built_in">reshape</span>(theta(<span class="number">1</span>:num_movies*num_features), num_movies, num_features);</span><br><span class="line">Theta = <span class="built_in">reshape</span>(theta(num_movies*num_features+<span class="number">1</span>:<span class="keyword">end</span>), ...</span><br><span class="line">                num_users, num_features);</span><br><span class="line"></span><br><span class="line">fprintf(<span class="string">&#x27;Recommender system learning completed.\n&#x27;</span>);</span><br><span class="line"></span><br><span class="line">p = X * Theta&#x27;;</span><br><span class="line">my_predictions = p(:,<span class="number">1</span>) + Ymean;</span><br></pre></td></tr></table></figure>
<hr>
<p>EOF</p>

  </div>
</article>
<!--Disqus-->


<!--Livere-->

    <div class="blog-post-comments">
        <div id="lv-container" data-id="city" data-uid="MTAyMC80NjEzMi8yMjY0Mw==">
            <noscript>ä¸å¯ç”¨ JavaScript æ”¯æŒçš„äººæ˜¯çœ‹ä¸åˆ°å¯çˆ±çš„è¯„è®ºåŒºçš„ã€‚ğŸ˜¥</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">é¦–é¡µ</a></li>
         
          <li><a href="/about/">å…³äº</a></li>
         
          <li><a href="/archives/">å½’æ¡£</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">é¡¹ç›®</a></li>
         
          <li><a href="/search/">æœç´¢</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">æœºå™¨å­¦ä¹ </span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.</span> <span class="toc-text">ç›‘ç£å­¦ä¹ </span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.1.</span> <span class="toc-text">å›å½’é—®é¢˜</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#h-theta-x"><span class="toc-number">2.</span> <span class="toc-text">h_\theta(x)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sum-i-0-m-theta-ix-i"><span class="toc-number">3.</span> <span class="toc-text">\sum_{i&#x3D;0}^m \theta_ix_i</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#left-begin-array-c-x-0-x-1-vdots-x-n-end-array-right"><span class="toc-number">4.</span> <span class="toc-text">\left[\begin{array}{c}x_0 \ x_1 \ \vdots \ x_n \end{array}\right]</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">4.0.0.1.</span> <span class="toc-text">æ¢¯åº¦ä¸‹é™</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="toc-number">4.0.0.2.</span> <span class="toc-text">æ­£è§„æ–¹ç¨‹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8Dvs%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="toc-number">4.0.0.3.</span> <span class="toc-text">æ¢¯åº¦ä¸‹é™vsæ­£è§„æ–¹ç¨‹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%EF%BC%9A%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="toc-number">4.0.0.4.</span> <span class="toc-text">æ³¨ï¼šç‰¹å¾ç¼©æ”¾</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">4.0.1.</span> <span class="toc-text">åˆ†ç±»é—®é¢˜</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">4.0.1.1.</span> <span class="toc-text">é€»è¾‘å›å½’</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB"><span class="toc-number">4.0.1.1.1.</span> <span class="toc-text">å¤šå…ƒåˆ†ç±»</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%EF%BC%9A%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.0.1.2.</span> <span class="toc-text">æ³¨ï¼šè¿‡æ‹Ÿåˆ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">4.0.1.3.</span> <span class="toc-text">ç¥ç»ç½‘ç»œ</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB-1"><span class="toc-number">4.0.1.3.1.</span> <span class="toc-text">å¤šå…ƒåˆ†ç±»</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%8B%9F%E5%90%88"><span class="toc-number">4.0.1.3.2.</span> <span class="toc-text">ç¥ç»ç½‘ç»œçš„æ‹Ÿåˆ</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">4.0.1.4.</span> <span class="toc-text">æ”¯æŒå‘é‡æœº</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E6%96%B9%E6%B3%95"><span class="toc-number">4.0.1.4.1.</span> <span class="toc-text">æ ¸æ–¹æ³•</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#SVM-%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%A0%B8"><span class="toc-number">4.0.1.4.2.</span> <span class="toc-text">SVM ä¸­ä½¿ç”¨æ ¸</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB-2"><span class="toc-number">4.0.1.4.3.</span> <span class="toc-text">å¤šå…ƒåˆ†ç±»</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-vs-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-vs-SVM"><span class="toc-number">4.0.1.5.</span> <span class="toc-text">é€»è¾‘å›å½’ vs ç¥ç»ç½‘ç»œ vs SVM</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.1.</span> <span class="toc-text">æ— ç›‘ç£å­¦ä¹ </span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means-%E8%81%9A%E7%B1%BB"><span class="toc-number">4.1.1.</span> <span class="toc-text">K-Means èšç±»</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PCA-%E7%BB%B4%E6%95%B0%E7%BA%A6%E5%87%8F"><span class="toc-number">4.1.2.</span> <span class="toc-text">PCA ç»´æ•°çº¦å‡</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B"><span class="toc-number">4.1.3.</span> <span class="toc-text">å¼‚å¸¸æ£€æµ‹</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="toc-number">4.1.3.1.</span> <span class="toc-text">é«˜æ–¯åˆ†å¸ƒ</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BC%82%E5%B8%B8%E6%A3%80%E6%9F%A5%E7%AE%97%E6%B3%95"><span class="toc-number">4.1.3.1.1.</span> <span class="toc-text">å¼‚å¸¸æ£€æŸ¥ç®—æ³•</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="toc-number">4.1.3.2.</span> <span class="toc-text">å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%94%A8%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E5%BC%82%E5%B8%B8%E6%A3%80%E6%9F%A5"><span class="toc-number">4.1.3.2.1.</span> <span class="toc-text">ç”¨å¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„å¼‚å¸¸æ£€æŸ¥</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%A8%E6%A7%9B%E9%80%89%E6%8B%A9"><span class="toc-number">4.1.3.3.</span> <span class="toc-text">é—¨æ§›é€‰æ‹©</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="toc-number">4.1.4.</span> <span class="toc-text">æ¨èç³»ç»Ÿ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E6%8E%A8%E8%8D%90"><span class="toc-number">4.1.4.1.</span> <span class="toc-text">åŸºäºå†…å®¹æ¨è</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4"><span class="toc-number">4.1.4.2.</span> <span class="toc-text">ååŒè¿‡æ»¤</span></a></li></ol></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&text=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&title=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&is_video=false&description=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“&body=Check out this article: https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&title=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&title=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&title=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&title=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2020/01/16/AndrewNgML/AndrewNgMLNotes/&name=Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> èœå•</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> ç›®å½•</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> åˆ†äº«</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> è¿”å›é¡¶éƒ¨</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2021 CDFMLR
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">é¦–é¡µ</a></li>
         
          <li><a href="/about/">å…³äº</a></li>
         
          <li><a href="/archives/">å½’æ¡£</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">é¡¹ç›®</a></li>
         
          <li><a href="/search/">æœç´¢</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"å¤åˆ¶åˆ°ç²˜è´´æ¿!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "å¤åˆ¶æˆåŠŸ!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-146911386-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?9a0d2e6fde93dad496ac79f04f3aba97";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->


<!--Livere Comments-->

    <script type="text/javascript">
      (function (d, s) {
        var j, e = d.getElementsByTagName(s)[0];

        if (typeof LivereTower === 'function') { return; }

        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;

        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>

</body>
</html>
