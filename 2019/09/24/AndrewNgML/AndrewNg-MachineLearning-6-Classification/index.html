<!DOCTYPE html>
<html lang=zh>
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Notes of Andrew Ng’s Machine Learning —— (6) ClassificationIntro of ClassificationClassification problems are something like giving a patient with a tumor, we have to predict whether the tumor is mali">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="分类问题">
<meta property="og:url" content="https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/index.html">
<meta property="og:site_name" content="clownote">
<meta property="og:description" content="Notes of Andrew Ng’s Machine Learning —— (6) ClassificationIntro of ClassificationClassification problems are something like giving a patient with a tumor, we have to predict whether the tumor is mali">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g72ld52460j30ur0g8ta7.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g72ng9ovwsj306e04u74d.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g72niix9vaj30m20cin2i.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g72nku2lrxj30mv08xtcy.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g757sf5lm8j30f504w3zi.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g758q7xincj31lc0mq76b.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g78nj8fvy4j30d507agmp.jpg">
<meta property="og:updated_time" content="2021-01-10T03:27:05.525Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="分类问题">
<meta name="twitter:description" content="Notes of Andrew Ng’s Machine Learning —— (6) ClassificationIntro of ClassificationClassification problems are something like giving a patient with a tumor, we have to predict whether the tumor is mali">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g72ld52460j30ur0g8ta7.jpg">
    
    
        
          
              <link rel="shortcut icon" href="/images/rabbit.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/rabbit_192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/rabbit_180.png">
          
        
    
    <!-- title -->
    <title>分类问题</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
    <!--Google search varification (PRIVATE)-->
    <meta name="google-site-verification" content="MrqlpFAD8nDanw3Ypv7ZsIWHLnTdhRuLa4QhSVwxIvc">
    <!--Google AdSense 关联 (PRIVATE)-->
    <script data-ad-client="ca-pub-1510963483941114" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2019/09/26/AndrewNgML/AndrewNg-MachineLearning-7-Overfitting/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/09/17/blog/MathematicalModelingWithPython/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&text=分类问题"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&title=分类问题"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&is_video=false&description=分类问题"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=分类问题&body=Check out this article: https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&title=分类问题"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&title=分类问题"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&title=分类问题"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&title=分类问题"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&name=分类问题&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Notes-of-Andrew-Ng’s-Machine-Learning-——-6-Classification"><span class="toc-number">1.</span> <span class="toc-text">Notes of Andrew Ng’s Machine Learning —— (6) Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Intro-of-Classification"><span class="toc-number">1.1.</span> <span class="toc-text">Intro of Classification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression"><span class="toc-number">1.2.</span> <span class="toc-text">Logistic Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hypothesis-Representation"><span class="toc-number">1.2.1.</span> <span class="toc-text">Hypothesis Representation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decision-Boundary"><span class="toc-number">1.2.2.</span> <span class="toc-text">Decision Boundary</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Logistic-Regression-Model"><span class="toc-number">1.2.3.</span> <span class="toc-text">Logistic Regression Model</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Cost-Function"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">Cost Function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Simplified-Cost-Function"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">Simplified Cost Function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Gradient-Descent"><span class="toc-number">1.2.3.3.</span> <span class="toc-text">Gradient Descent</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Advanced-Optimization"><span class="toc-number">1.3.</span> <span class="toc-text">Advanced Optimization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multiclass-Classification"><span class="toc-number">1.4.</span> <span class="toc-text">Multiclass Classification</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        分类问题
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">clownote</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-09-24T16:03:36.000Z" itemprop="datePublished">2019-09-24</time>
        
        (Updated: <time datetime="2021-01-10T03:27:05.525Z" itemprop="dateModified">2021-01-10</time>)
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a> › <a class="category-link" href="/categories/Machine-Learning/AndrewNg/">AndrewNg</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/Machine-Learning/">Machine Learning</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="Notes-of-Andrew-Ng’s-Machine-Learning-——-6-Classification"><a href="#Notes-of-Andrew-Ng’s-Machine-Learning-——-6-Classification" class="headerlink" title="Notes of Andrew Ng’s Machine Learning —— (6) Classification"></a>Notes of Andrew Ng’s Machine Learning —— (6) Classification</h1><h2 id="Intro-of-Classification"><a href="#Intro-of-Classification" class="headerlink" title="Intro of Classification"></a>Intro of Classification</h2><p>Classification problems are something like giving a patient with a tumor, we have to predict whether the tumor is malignant or benign. It’s expected to output <strong>discrete</strong> values.</p>
<p>The classification problem is just like the regression problem, except that the values we want to predict take on only a small number of discrete values. For now, we will focus on the <strong>binary classification problem</strong> in which <code>y</code> can take only two values, <code>0</code> and <code>1</code>. </p>
<p>For instance, if we are trying to build a spam classifier for email, then $x^{(i)}$ may be some features of a piece of email, and $y$ may be <code>1</code> if it is a piece of spam mail, and <code>0</code> otherwise. Hence, $y \in {0,1}$. <code>0</code> is also called the <em>negative class</em>, and <code>1</code> the <em>positive class</em>, and they are sometimes also denoted by the symbols <code>-</code> and <code>+</code>. Given $x^{(i)}$, the corresponding $y^{(i)}$ is also called <em>the label for the training example</em>.</p>
<p>To attempt classification, one method is to use linear refression and map all predictions greater than <code>0.5</code> as a <code>1</code> and all less than <code>0.5</code> as a <code>0</code>. However, this method doesn’t work because classification is not actually a linear function. So what we actually do to solve classification problems is an algorithm named <strong>logistic regression</strong>. Note that even be called <em>regression</em>, it is actually to do classification.</p>
<h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><h3 id="Hypothesis-Representation"><a href="#Hypothesis-Representation" class="headerlink" title="Hypothesis Representation"></a>Hypothesis Representation</h3><p>We could approach the classification problem ignoring the fact that <code>y</code> is discrete-valued, and use our old linear regression algorithm to try to predict <code>y</code> given <code>x</code>. However it is easy to construct examples where this method performs very poorly.</p>
<p>Intuitively, it also doesn’t make sense for $h_\theta(x)$ to take values larger than <code>1</code> or smaller then <code>0</code> when we know that $y\in{0,1}$. To fix this, let’s change our hypotheses $h_\theta(x)$ to satisfy $0 \le h_\theta(x) \le 1$. This accomplished by plugging $\theta^Tx$ into the <strong>Logistic Function</strong>.</p>
<p>The the <strong>Simoid Function</strong>, also called the <strong>Logistic Function</strong> is this:<br>$$<br>y = \frac{1}{1+e^{-x}}<br>$$<br>It looks like the following image:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g72ld52460j30ur0g8ta7.jpg" alt="image-20190917162504420"></p>
<p>Our new form uses the Simoid Function:<br>$$<br>\begin{array}{l}h_\theta(x) = g(\theta^Tx)\z = \theta^Tx\g(z) = \frac{1}{1+e^{-z}}\end{array}<br>$$<br>The logistic function $g(z)$ maps any real numbers to the $(0,1)$ interval, making it useful for transforming a arbitrary-valued function into a function better suited for classfication.</p>
<p>We can also simply write $h_\theta(x)$ like this:<br>$$<br>h_\theta(X) = \frac{1}{1+e^{-\theta^Tx}}<br>$$<br><strong>$h_\theta(x)$ will output the probalility that our output is <code>1</code>.</strong> For example, $h_\theta(x)=0.7$ gives us a probability of $70%$ that our output is $1$ (then the probability that it is 0 is 30%):<br>$$<br>\begin{array}{l}<br>h_\theta(x)=P(y=1 \mid x;\theta)=1-P(y=0 \mid x; \theta)\<br>P(y=0 \mid x;\theta) + P(y=1 \mid x;\theta) = 1<br>\end{array}<br>$$</p>
<h3 id="Decision-Boundary"><a href="#Decision-Boundary" class="headerlink" title="Decision Boundary"></a>Decision Boundary</h3><p>In order to get our discrete <code>0</code> or <code>1</code> classification, we can translate the output of the hyporithesis function as follows:<br>$$<br>\begin{array}{rcl}<br>h_\theta(x) \ge 0.5 &amp;\Rightarrow&amp; y=1\<br>h_\theta(x) &lt; 0.5 &amp;\Rightarrow&amp; y=0<br>\end{array}<br>$$<br>The way our logistic function $g$ behaves is that when its input is $\ge 0$, its output is $\ge 0.5$:<br>$$<br>g(z) \ge 0.5 \quad when \quad z \ge 0<br>$$<br>In fact, we know that:<br>$$<br>\begin{array}{lclcl}<br>z \to -\infin &amp;,&amp; e^{\infin} \to \infin &amp; \Rightarrow &amp; g(z) \to 0\<br>z \to 0 &amp;,&amp; e^{0} \to 1 &amp; \Rightarrow &amp; g(z) \to 0.5\<br>z \to +\infin &amp;,&amp; e^{-\infin} \to 0 &amp; \Rightarrow &amp; g(z) \to 1<br>\end{array}<br>$$<br>So if our input to $g$ is $\theta^TX$, then that means:<br>$$<br>h_\theta(X) = g(\theta^TX) \ge 0.5 \quad when \quad \theta^TX \ge 0<br>$$<br>From these statements we can now say:<br>$$<br>\begin{array}{rcl}<br>\theta^TX \ge 0 &amp;\Rightarrow&amp; y=1\<br>\theta^TX \le 0 &amp;\Rightarrow&amp; y=0\<br>\end{array}<br>$$<br>The <strong>decision boundary</strong> is the line that separates the area where $y=0$ and where $y=1$. It is created by our hypothesis function.</p>
<p>Example:<br>$$<br>\theta=\left[ \begin{array}{c}<br>5\<br>-1\<br>0<br>\end{array} \right]<br>$$<br>In this case, $y=1$ if $5+(-1)x_1+0x_2 \ge 0$ , i.e. $x_1 \le 5$. Our boundary is a straight vertical line placed on the graph where $x_1=5$, and everything to the left of that denotes $y=1$, while everything to the right denotes $y=0$:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g72ng9ovwsj306e04u74d.jpg" alt="image-20190917173722734"></p>
<p>Another example:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g72niix9vaj30m20cin2i.jpg" alt="image-20190917173931467"></p>
<p>Non-linear decision boundaries:</p>
<p>The input to the sigmoid function g(z) (e.g. $\theta^TX$) doesn’t need to be linear, and could be a function that describes a circle (e.g. $z=\theta_0+\theta_1x_1^2+\theta_2x_2^2$) or any shape to fit our data.</p>
<p>Example:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g72nku2lrxj30mv08xtcy.jpg" alt="image-20190917174144868"></p>
<h3 id="Logistic-Regression-Model"><a href="#Logistic-Regression-Model" class="headerlink" title="Logistic Regression Model"></a>Logistic Regression Model</h3><p>In this part, we will implement the logistic regression model.<br>$$<br>\begin{array}{rcl}<br>\textrm{Training set} &amp;:&amp; {(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \ldots, (x^{(m)},y^{(m)})}\<br>\<br>\textrm{m examples} &amp;:&amp;<br>x \in \left[\begin{array}{c}<br>x_0\x_1\ \vdots \ x_n<br>\end{array}\right] \textrm{where }(x_0=1)<br>,\quad y \in {0,1}\<br>\<br>\textrm{Hypothesis} &amp;:&amp; h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}<br>\end{array}<br>$$</p>
<h4 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h4><p>If we use the same cost function that we use for <em>linear regression</em> ($J(\theta)=\frac{1}{m}\sum_{i=1}^m\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$) for logistic regression, it will be <strong>non-convex</strong> (that looks wavy), which is causing many local optima and hard to find the global minimum.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g757sf5lm8j30f504w3zi.jpg" alt="屏幕快照 2019-09-19 22.49.53"></p>
<p>So, what we actually need is our new <strong>Logistic Regression Cost Function</strong>, which guarantees that $J(\theta)$ is convex for logistic regression.<br>$$<br>\begin{array}{l}<br>J(\theta)=\frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)},y^{(i)}))\<br>\<br>Cost(h_\theta(x),y)=\left{\begin{array}{rl}<br>-log(h_\theta(x)) &amp; \textrm{ if}\quad y=1\<br>-log(1-h_\theta(x)) &amp; \textrm{ if}\quad y=0<br>\end{array}\right.<br>\end{array}<br>$$<br>The “$J(\theta)$-$h_\theta(x)$” plots is like this:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g758q7xincj31lc0mq76b.jpg" alt="屏幕快照 2019-09-19 23.23.39"></p>
<p>Let’s take a look at the $Cost(h_\theta(x),y)$ :<br>$$<br>\begin{array}{lcl}Cost(h_\theta(x),y) = 0 &amp;\textrm{if}&amp; h_\theta(x)=y\Cost(h_\theta(x),y) \to \infin &amp;\textrm{if}&amp; y=0 \quad&amp;\quad h_\theta(x) \to 1\Cost(h_\theta(x),y) \to \infin &amp;\textrm{if}&amp; y=1 \quad&amp;\quad h_\theta(x) \to 0\\end{array}<br>$$</p>
<blockquote>
<p> If our correct answer ‘y’ is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity.</p>
<p>If our correct answer ‘y’ is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity.</p>
</blockquote>
<h4 id="Simplified-Cost-Function"><a href="#Simplified-Cost-Function" class="headerlink" title="Simplified Cost Function"></a>Simplified Cost Function</h4><p>We can simplify the $Cost$ function by compressing the two conditional cases into one case:<br>$$<br>Cost(h_\theta(x),y)=-y \cdot log(h_\theta(x))-(1-y) \cdot log(1-h_\theta(x))<br>$$<br>In this definition, when $y=0$, the term $-(1-y) \cdot log(1-h_\theta(x))$ will be $0$; when $y=0$, the term $-y \cdot log(h_\theta(x))$ will be $0$. Obviously, this is equal to the previous one but more easy to implement.</p>
<p>Now, we can fully write out our entire cost function as follow:<br>$$<br>J(\theta)=-\frac{1}{m}\sum_{i=1}^m\Bigg[y^{(i)}log\Big(h_\theta(x)\Big)+(1-y^{(i)})log\Big(1-h_\theta(x^{(i)})\Big)\Bigg]<br>$$<br>And a <strong>vectorized</strong> implementation is:<br>$$<br>\begin{array}{l}<br>h=g(X\theta)\<br>J(\theta)=\frac{1}{m}\cdot\big(-y^T log(h) -(1-y)^T log(1-h)\big)<br>\end{array}<br>$$</p>
<h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><p>The general form of gradient descent is:<br>$$<br>\begin{array}{l}<br>Repeat \quad {\<br>\qquad \theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)\<br>}<br>\end{array}<br>$$<br>Work out the derivative part using calculus to get:<br>$$<br>\begin{array}{l}<br>Repeat \quad {\<br>\qquad \theta_j:=\theta_j-\frac{\alpha}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)}\<br>}<br>\end{array}<br>$$<br>Actually, this algorithm is identical to the one we used in linear regression. And notice that we still have to simultaneously update all values in theta.</p>
<p><strong>Vectorized implementation</strong>:<br>$$<br>\theta:=\theta-\frac{\alpha}{m}X^T(g(X\theta)-\overrightarrow{y})<br>$$</p>
<h2 id="Advanced-Optimization"><a href="#Advanced-Optimization" class="headerlink" title="Advanced Optimization"></a>Advanced Optimization</h2><p>There are many more sophisticated, faster ways to optimize $\theta$ that can be used instead of gradient descent, such as “<em>Conjugate gradient</em>“, “<em>BFGS</em>“ and “<em>L-BFGS</em>“.</p>
<p>We are not suggest to write these more sophisticated algorithms ourself but use the libraries instead, as they’re already tested and highly optimized. Octave provides them.</p>
<p>We can use octave’s <code>fminunc()</code> optimization algorithm to do that.</p>
<p>To use this advanced optimization, we first need to provide a function that evaluates the following two functions for a given inpuit value $\theta$:<br>$$<br>J(\theta), \qquad \frac{\partial}{\partial\theta_j}J(\theta)<br>$$<br>We can write a single function that retunrs both of these:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">function [jVal, gradient] = costFunction(theta)</span><br><span class="line">	jVal = &lt;code to compute J(theta)&gt;</span><br><span class="line">	gradient = &lt;code to compute derivative of J(theta)&gt;</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p>Then we are going to set a <code>optimset</code> and a initial theta as well, then send them to <code>fminunc()</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">options = optimset(&apos;GradObj&apos;, &apos;on&apos;, &apos;MaxIter&apos;, 100);</span><br><span class="line">initialTheta = zeros(2, 1);</span><br><span class="line"></span><br><span class="line">[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);</span><br></pre></td></tr></table></figure>

<h2 id="Multiclass-Classification"><a href="#Multiclass-Classification" class="headerlink" title="Multiclass Classification"></a>Multiclass Classification</h2><p>Now we will approach the classification of data when we have more than two categories. Instead of $y={0,1}$ we will expand our definition so that $y={0,1…n}$.</p>
<p>Since $y={0,1…n}$, we divide our problem into $n+1$ ($+1$ because the index starts at $0$) binary classification problems; in each one, we predict the probability that ‘$y$’ is a member of one of our classes.<br>$$<br>\begin{array}{l}<br>y \in {0,1,\cdots,n}\\<br>h_\theta^{(0)}(x)=P(y=0|x;\theta)\<br>h_\theta^{(0)}(x)=P(y=0|x;\theta)\<br>\vdots\<br>h_\theta^{(0)}(x)=P(y=0|x;\theta)\\<br>prediction = \mathop{max}\limits_{\theta}\big(h_\theta^{(i)}(x)\big)<br>\end{array}<br>$$<br>We are basically choosing one class and then lumping all the others into a single second class. We do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.</p>
<p>The following image shows how one could classify 3 classes:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g78nj8fvy4j30d507agmp.jpg" alt="img"></p>
<p><strong>To summarize:</strong></p>
<p>Train a logistic regression classifier $h_\theta(x)$ for each class￼ to predict the probability that ￼ ￼$y=i$￼ ￼. To make a prediction on a new $x$, pick the class ￼that maximizes $h_\theta(x)$.</p>

  </div>
</article>
<!--Disqus-->


<!--Livere-->

    <div class="blog-post-comments">
        <div id="lv-container" data-id="city" data-uid="MTAyMC80NjEzMi8yMjY0Mw==">
            <noscript>不启用 JavaScript 支持的人是看不到可爱的评论区的。😥</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Notes-of-Andrew-Ng’s-Machine-Learning-——-6-Classification"><span class="toc-number">1.</span> <span class="toc-text">Notes of Andrew Ng’s Machine Learning —— (6) Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Intro-of-Classification"><span class="toc-number">1.1.</span> <span class="toc-text">Intro of Classification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression"><span class="toc-number">1.2.</span> <span class="toc-text">Logistic Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hypothesis-Representation"><span class="toc-number">1.2.1.</span> <span class="toc-text">Hypothesis Representation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decision-Boundary"><span class="toc-number">1.2.2.</span> <span class="toc-text">Decision Boundary</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Logistic-Regression-Model"><span class="toc-number">1.2.3.</span> <span class="toc-text">Logistic Regression Model</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Cost-Function"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">Cost Function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Simplified-Cost-Function"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">Simplified Cost Function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Gradient-Descent"><span class="toc-number">1.2.3.3.</span> <span class="toc-text">Gradient Descent</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Advanced-Optimization"><span class="toc-number">1.3.</span> <span class="toc-text">Advanced Optimization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multiclass-Classification"><span class="toc-number">1.4.</span> <span class="toc-text">Multiclass Classification</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&text=分类问题"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&title=分类问题"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&is_video=false&description=分类问题"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=分类问题&body=Check out this article: https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&title=分类问题"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&title=分类问题"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&title=分类问题"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&title=分类问题"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/09/24/AndrewNgML/AndrewNg-MachineLearning-6-Classification/&name=分类问题&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2021 CDFMLR
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<!-- clipboard -->

  <script src="/lib/clipboard/clipboard.min.js"></script>
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功!");
      e.clearSelection();
    })
  })
  </script>

<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-146911386-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?9a0d2e6fde93dad496ac79f04f3aba97";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->


<!--Livere Comments-->

    <script type="text/javascript">
      (function (d, s) {
        var j, e = d.getElementsByTagName(s)[0];

        if (typeof LivereTower === 'function') { return; }

        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;

        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>

</body>
</html>
