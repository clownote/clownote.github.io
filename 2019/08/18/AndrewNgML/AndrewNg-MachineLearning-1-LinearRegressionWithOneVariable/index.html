<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Notes of Andrew Ng’s Machine Learning —— (1) Linear Regression with One VariableModel RepresentationNotation $x^{(i)}$ denote the input variables, called input features $y^{(i)}$ denote the output var">
<meta property="og:type" content="article">
<meta property="og:title" content="单变量线性回归">
<meta property="og:url" content="https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/index.html">
<meta property="og:site_name" content="clownote">
<meta property="og:description" content="Notes of Andrew Ng’s Machine Learning —— (1) Linear Regression with One VariableModel RepresentationNotation $x^{(i)}$ denote the input variables, called input features $y^{(i)}$ denote the output var">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/006tNc79gy1g652fmlvx6j30az078glq.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/006tNc79gy1g65bp99j97j307o05gq2z.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/006tNc79gy1g65byqwhjxj307o05gwee.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/006tNc79gy1g65cm1aucyj30js0a60v0.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/006tNc79ly1g65ddr26d9j30ja0a4778.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/006tNc79ly1g65dnu9hj7j309606iwen.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/006tNc79ly1g65ejfyhsbj30rs08cmzd.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006tNc79ly1g65dzrkce0j30b209ydh6.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/006y8mN6gy1g673ule5evj30p40cqafx.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/006y8mN6gy1g673vbnti4j30o20bggqp.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/006y8mN6ly1g67fcs8k0mj30gs09gjsz.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/006y8mN6gy1g67l1ajh02j30no0con68.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/006y8mN6ly1g67magihpyj30wo0hu7iy.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g6de48b49qj30i50af0ui.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g6de6eemuhj30im0astb9.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g6de8nmhaoj30ia0a4why.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g6dedxiq58j30ej052aay.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g6dhmaf0chj308q06qgml.jpg">
<meta property="article:published_time" content="2019-08-18T21:09:53.000Z">
<meta property="article:modified_time" content="2021-05-01T03:32:03.245Z">
<meta property="article:author" content="CDFMLR">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://ww4.sinaimg.cn/large/006tNc79gy1g652fmlvx6j30az078glq.jpg">
    
    
        
          
              <link rel="shortcut icon" href="/images/rabbit.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/rabbit_192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/rabbit_180.png">
          
        
    
    <!-- title -->
    <title>单变量线性回归</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
    <!--Google search varification (PRIVATE)-->
    <meta name="google-site-verification" content="MrqlpFAD8nDanw3Ypv7ZsIWHLnTdhRuLa4QhSVwxIvc" />
    <!--Google AdSense 关联 (PRIVATE)-->
    <script data-ad-client="ca-pub-1510963483941114" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<meta name="generator" content="Hexo 5.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2019/08/31/Golang/go-0-begining/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-0-Introduction/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&text=单变量线性回归"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&title=单变量线性回归"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&is_video=false&description=单变量线性回归"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=单变量线性回归&body=Check out this article: https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&title=单变量线性回归"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&title=单变量线性回归"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&title=单变量线性回归"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&title=单变量线性回归"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&name=单变量线性回归&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Notes-of-Andrew-Ng%E2%80%99s-Machine-Learning-%E2%80%94%E2%80%94-1-Linear-Regression-with-One-Variable"><span class="toc-number">1.</span> <span class="toc-text">Notes of Andrew Ng’s Machine Learning —— (1) Linear Regression with One Variable</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Representation"><span class="toc-number">1.1.</span> <span class="toc-text">Model Representation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Notation"><span class="toc-number">1.1.1.</span> <span class="toc-text">Notation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-more-formal-description-of-supervised-learning"><span class="toc-number">1.1.2.</span> <span class="toc-text">A more formal description of supervised learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hypothesis-amp-Model-Regression"><span class="toc-number">1.1.3.</span> <span class="toc-text">Hypothesis &amp; Model Regression</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cost-Function"><span class="toc-number">1.2.</span> <span class="toc-text">Cost Function</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Cost-function"><span class="toc-number">1.2.1.</span> <span class="toc-text">Cost function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Goal"><span class="toc-number">1.2.2.</span> <span class="toc-text">Goal</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cost-Function-Intuition-I"><span class="toc-number">1.3.</span> <span class="toc-text">Cost Function - Intuition I</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cost-Function-Intuition-II"><span class="toc-number">1.4.</span> <span class="toc-text">Cost Function - Intuition II</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent"><span class="toc-number">1.5.</span> <span class="toc-text">Gradient Descent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent-Intuition"><span class="toc-number">1.6.</span> <span class="toc-text">Gradient Descent Intuition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent-for-Linear-Regression"><span class="toc-number">1.7.</span> <span class="toc-text">Gradient Descent for Linear Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experiment"><span class="toc-number">1.8.</span> <span class="toc-text">Experiment</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        单变量线性回归
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">clownote</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-08-18T21:09:53.000Z" itemprop="datePublished">2019-08-18</time>
        
        (Updated: <time datetime="2021-05-01T03:32:03.245Z" itemprop="dateModified">2021-05-01</time>)
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a> › <a class="category-link" href="/categories/Machine-Learning/AndrewNg/">AndrewNg</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="Notes-of-Andrew-Ng’s-Machine-Learning-——-1-Linear-Regression-with-One-Variable"><a href="#Notes-of-Andrew-Ng’s-Machine-Learning-——-1-Linear-Regression-with-One-Variable" class="headerlink" title="Notes of Andrew Ng’s Machine Learning —— (1) Linear Regression with One Variable"></a>Notes of Andrew Ng’s Machine Learning —— (1) Linear Regression with One Variable</h1><h2 id="Model-Representation"><a href="#Model-Representation" class="headerlink" title="Model Representation"></a>Model Representation</h2><h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h3><ul>
<li><a href="http://clownote.github.io/about/index.html#%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F">$x^{(i)}$</a> denote the <em>input</em> variables, called <strong>input features</strong></li>
<li>$y^{(i)}$ denote the <em>output</em> variables, called <strong>target variable</strong></li>
<li>$(x^{(i)}, y^{(i)})$ is called a <strong>training example</strong></li>
<li>$(x^{(i)}, y^{(i)}); i=1, …, m$ —— is called a <strong>training set</strong></li>
</ul>
<p>⚠️【Note】The superscript “$^{(i)}$” in the notations is simply an <strong>index</strong> into the train set.</p>
<ul>
<li>$X$ denote the <strong>space</strong> of <em>input values</em></li>
<li>$Y$ denote the <strong>space</strong> of <em>output values</em></li>
</ul>
<p>In many examples, $X = Y =\R$.</p>
<h3 id="A-more-formal-description-of-supervised-learning"><a href="#A-more-formal-description-of-supervised-learning" class="headerlink" title="A more formal description of supervised learning"></a>A more formal description of supervised learning</h3><p>Given a training set, to learn a <strong>function</strong> $h: X \to Y$ so that $h(x)$ is a “good” predictor for the corresponding value of $y$.</p>
<p>For historical reasons, this function $h$ is called a <strong><code>hypothesis</code></strong>.</p>
<p>Pictorially:</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g652fmlvx6j30az078glq.jpg" alt="page1image20344816.png"> </p>
<p>With our new notations, the categories of supervised learning problems can be this:</p>
<ul>
<li><code>regression problem</code>: when the <strong>target variable</strong> is continuous.</li>
<li><code>classification problem</code>: $y$ can take on only a small number of discrete values</li>
</ul>
<h3 id="Hypothesis-amp-Model-Regression"><a href="#Hypothesis-amp-Model-Regression" class="headerlink" title="Hypothesis &amp; Model Regression"></a>Hypothesis &amp; Model Regression</h3><p>To represent the hypothesis $h$, for example, if we want to fit a data set like this:</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g65bp99j97j307o05gq2z.jpg" alt="image-20190819214710193"></p>
<p>Simply, we may make the $h_\theta(x)=\theta_0+\theta_1x$. This is means that we are going to predict that $y$ is a linear function of $x$. </p>
<p>Plotting this in the picture, it will be something like this:</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g65byqwhjxj307o05gwee.jpg" alt="006tNc79gy1g65bp99j97j307o05gq2z"></p>
<p>And what this function is doing is predicting that $y$ is some straight line function of $x$.</p>
<p>In this case, this <strong>model</strong> is called <strong><code>linear regression with one variable</code></strong>(or <code>Univariate linear regression</code>).</p>
<p>P.S. we can also fit a more complicated (perhaps non-linear) functions, but this linear case is the simplest.</p>
<h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><p>Well, now we have got this:</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g65cm1aucyj30js0a60v0.jpg" alt="image-20190819221840291"></p>
<p>To get a “good” hypothesis function, we need to choose $\theta_0$, $\theta_1$ so that $h_\theta(x)$ is close to $y$ for our training exmaples $(x, y)$.  A cost function is of great help with this goal.</p>
<h3 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h3><p>A <strong><code>cost function</code></strong> can help us to measure the accuracy of our hypothesis function.</p>
<p>This takes an <strong>average difference</strong> of all the results of the hypothesis with inputs from x’s and the actual output y’s:<br>$$<br>J(\theta_0, \theta_1) = \frac{1}{2m}\sum^m_{i=1}[h_\theta(x^{(i)})-y^{(i)}]^2 \tag{1}<br>$$<br>The function $(1)$ is our <em>cost function</em> exactly. Take a look at it:</p>
<ul>
<li><p>$h_\theta(x^{(i)})-y^{(i)}$ shows the difference between the predicted value and the actual value.</p>
</li>
<li><p>$\frac{1}{m}\sum^{m}_{i=1}…$ offers the mean of the squares of $h_\theta(x^{(i)})-y^{(i)}$</p>
</li>
<li><p>The mean is halved ($\frac{1}{2}$) as a convenience for the computati::on of the <strong>gradient descent</strong>, as the $\frac{1}{2}f^2 = f$.</p>
</li>
</ul>
<p>P.S. This function is otherwise called the “<em>Squared error function</em>“, or “<em>Mean squared error</em>“.</p>
<h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><p>To make $h_\theta(x)$ closing to $y$, we are just expected to minimize our cost function by adjusting the value of $\theta_0$, $\theta_1$.</p>
<p>We describe this <strong>goal</strong> like this:<br>$$<br>\mathop{minimize}\limits_{\theta_0, \theta_1} J(\theta_0, \theta_1) \tag{2}<br>$$<br>Or, more directly:<br>$$<br>\mathop{minimize}\limits_{\theta_0, \theta_1}<br>\frac{1}{2m}\sum^m_{i=1}[h_\theta(x^{(i)})-y^{(i)}]^2<br>\tag{3}<br>$$</p>
<p>And if we are working with a linear regression with one variable, the $h_\theta(x)=\theta_0+\theta_1x$.</p>
<p>⚠️【Note】Hypothesis Function &amp; Cost Function</p>
<ul>
<li>$h_\theta(x)$ for fixed $\theta_i$, is a function of $x$.</li>
<li>$J(\theta_0, \theta_1)$ is a function of the parameter $\theta_i$ .</li>
</ul>
<h2 id="Cost-Function-Intuition-I"><a href="#Cost-Function-Intuition-I" class="headerlink" title="Cost Function - Intuition I"></a>Cost Function - Intuition I</h2><p>Let’s draw some pictures for better understanding of what the values of the cost function.</p>
<p>To getting start, we are going to work with a simplified hypothesis function:</p>
<p><img src="http://ww2.sinaimg.cn/large/006tNc79ly1g65ddr26d9j30ja0a4778.jpg" alt="屏幕快照 2019-08-19 22.44.10"></p>
<p>Our training data set is scattered on the x-y plane. We are trying to make a straight line (defined by $h_\theta(x)$) which passes through these scattered data points.</p>
<p>Our objective is to get the best possible line. The best possible line will be such:</p>
<blockquote>
<p>So that the <em>average squared vertical distances of the scattered points from the line</em> ($\frac{1}{m}\sum^m_{i=1}[h_\theta(x^{(i)})-y^{(i)}]^2$) will be the least.</p>
</blockquote>
<p>Ideally, the line should pass through all the points of our training data set. In such a case, the value of cost function $J(\theta_0, \theta_1)$ will be <code>0</code>. </p>
<p>E.g. A ideal situation where $J=0$:</p>
<p>Let this be our training set: Only three points <code>(1, 1)</code>, <code>(2, 2)</code> &amp; <code>(3, 3)</code></p>
<p><img src="http://ww2.sinaimg.cn/large/006tNc79ly1g65dnu9hj7j309606iwen.jpg" alt="屏幕快照 2019-08-19 22.54.07"></p>
<p>Now, we try setting $\theta_1$ to different values: <code>-0.5</code>,  <code>0</code>, <code>0.5</code>, <code>1</code>, <code>1.5</code>……</p>
<p>When $\theta_1=1$, we get a slope of 1 which goes through every single data point in our model. Conversely, when $\theta_1=0.5$, we see the vertical distance from our fit to the data points increase:</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79ly1g65ejfyhsbj30rs08cmzd.jpg" alt="新建项目"></p>
<p>By doing this, we got a series of graph of $h_\theta(x)$ in x-y plane as well as yield to the following $\theta_1$-$J(\theta_1)$ graph:</p>
<p><img src="http://ww1.sinaimg.cn/large/006tNc79ly1g65dzrkce0j30b209ydh6.jpg" alt="image-20190819230628708"></p>
<p>Thus as a goal, we should try to <strong>minimize the cost function</strong>. In this case, $\theta_1=1$ is our global minimum.</p>
<h2 id="Cost-Function-Intuition-II"><a href="#Cost-Function-Intuition-II" class="headerlink" title="Cost Function - Intuition II"></a>Cost Function - Intuition II</h2><p>Unlike before, this time, we won’t continue with the simplified hypothesis, we are going to keep both of parameters $\theta_0$ and $\theta_1$. So the hypithesis function will be $h_\theta(x)=\theta_0+\theta_1x$.</p>
<p>Here’s our problem formulation as usual:<br>$$<br>\begin{array}{rl}<br> \textrm{Hypothesis: } &amp; h_\theta(x)=\theta_0+\theta_1x\<br>  &amp; \<br> \textrm{Parameters: } &amp; \theta_0, \theta_1\<br>   &amp; \<br> \textrm{Cost Function: } &amp; J(\theta_0, \theta_1) = \frac{1}{2m}\sum^m_{i=1}[h_\theta(x^{(i)})-y^{(i)}]^2\<br>   &amp; \<br> \textrm{Goal: } &amp; \mathop{minimize}\limits_{\theta_0, \theta_1} J(\theta_0, \theta_1)<br>\end{array}<br>$$<br>Same as last time, we want to unserstand the hypothesis $h$ and the cost function $J$ via a series of graph. However, we’d like to use a <em><code>contour plot</code></em> to describe our $J(\theta_0, \theta_1)$.</p>
<blockquote>
<p>A <em>contour plot</em> is a graph that contains many contour lines.</p>
<p>A <em>contour line</em> of a two variable function has a constant value at all points of the same line. </p>
</blockquote>
<p>An example:</p>
<p><img src="http://ww3.sinaimg.cn/large/006y8mN6gy1g673ule5evj30p40cqafx.jpg" alt="屏幕快照 2019-08-21 09.29.48"></p>
<p>Taking any color and going along the ‘circle’, one would expect to get the same value of the cost function. </p>
<p>To touch our optimization objective, we can try to setting the parameters $\theta_i$ to different values.</p>
<p>When $\theta_0 = 360$ and $\theta_1 = 0$, the value of $J(\theta_0, \theta_1)$ in the contour plot <strong>gets closer to the center thus reducing the cost function error</strong>. Now we get a result in a better fit of the data:</p>
<p><img src="http://ww3.sinaimg.cn/large/006y8mN6gy1g673vbnti4j30o20bggqp.jpg" alt="屏幕快照 2019-08-21 09.29.58"></p>
<p>Minimizing the cost function as much as possible and consequently, the result of $\theta_1$ and $\theta_0$ tend to be around 0.12 and 250 respectively. Plotting those values on our graph to the right seems to put our point in the center of the inner most ‘circle’.</p>
<p>Obviously, we dislike to write a software to just plot out a contour plot and then try to manually read off the numbers to reach our goal. We want <strong>an efficient algorithm for automatically finding the value of $\theta_0$ and $\theta_1$ that minimizes the cost function $J$</strong>. Actually, the <em>gradient descent</em> algorithm that we will talk about works great on this question.</p>
<h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>There is a algorithm called <strong>gradient descent</strong> for <strong>minimizing the cost function $J$</strong>. And we can use it not only in linear regression as it’s actually used all over the place in machine learning.</p>
<p>Let’s talk about gradient descent for minimizing some arbitrary function $J$. So here’s the problem setup:</p>
<p><img src="http://ww2.sinaimg.cn/large/006y8mN6ly1g67fcs8k0mj30gs09gjsz.jpg" alt="image-20190821172439919"></p>
<p>We put $\theta_0$ on the <code>x</code> axis and $\theta_1$ on the <code>y</code> axis, with the <strong>cost function</strong> on the vertical <code>z</code> axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup:</p>
<p><img src="http://ww2.sinaimg.cn/large/006y8mN6gy1g67l1ajh02j30no0con68.jpg" alt="image-20190821204112216"></p>
<p>We will know that we have succeeded when our cost function is at the very bottom of the pits in our graph, i.e. when its value is the minimum. The red arrows show the minimum points in the graph.</p>
<p>Image that we are physically standing at a point on a hill, in gradient descent, what we’re going to do is to spin 360 degrees around and just look all around us, and ask, “If I were to take a little step in some direction, and I want to go down the hill as quickly as possible, what direction should I take?” then you take a step in that direction. Repeat doing this until you converge to a local minimum. Like the black line in the picture above shows.</p>
<p>Notice that if we choose different points to grandient descent, we may reach different local optimums.</p>
<p>Mathematically, this is the definition of the gradient descent algorithm:</p>
<blockquote>
<p><strong>Gradient Descent Algorithm</strong></p>
<p>repeat until convergence {</p>
<p>$\qquad\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta_0, \theta_1)\qquad \textrm{(for } j=0 \textrm{ and } j=1 \textrm{)}$</p>
<p>}</p>
</blockquote>
<p>The $\alpha$ is a number that is called the <strong><code>learning rate</code></strong>. It basically controls how big a step we take downhill with gradient descent.</p>
<p>At each iteration $j$, one should simultaneously update the parameters θ1,θ2,…,θn. Updating a specific parameter prior to calculating another one on the <code>j(th)</code> iteration would yield to a wrong implementation:</p>
<p><img src="http://ww3.sinaimg.cn/large/006y8mN6ly1g67magihpyj30wo0hu7iy.jpg" alt="屏幕快照 2019-08-21 21.24.15"></p>
<h2 id="Gradient-Descent-Intuition"><a href="#Gradient-Descent-Intuition" class="headerlink" title="Gradient Descent Intuition"></a>Gradient Descent Intuition</h2><p>Let’s explore the scenario where we used **one parameter $\theta_1$ ** and plotted its cost function to implement a gradient descent. Our formula for a single parameter was : Repeat until convergence:<br>$$<br>\theta_1:=\alpha\frac{d}{d\theta_1}J(\theta_1)<br>$$</p>
<p>Regardless of the slope’s sign for $\frac{d}{d\theta_1}J(\theta_1)$ eventually converges to its minimum value. </p>
<p>The following graph shows that when the slope is negative, the value of $\theta_1$ increases and when it is positive, the value of $\theta_1$ decreases:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6de48b49qj30i50af0ui.jpg" alt="image-20190826211520863"></p>
<p>On a side note, we should adjust our parameter $\alpha$ to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6de6eemuhj30im0astb9.jpg" alt="image-20190826211727240"></p>
<p>How does gradient descent converge with a fixed step size $\alpha$?<br>The intuition behind the convergence is that $\frac{d}{d\theta_1}J(\theta_1)$ approaches 0 as we approach the bottom of our convex function. At the minimum, the derivative will always be 0 and thus we get:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6de8nmhaoj30ia0a4why.jpg" alt="image-20190826211936126"></p>
<h2 id="Gradient-Descent-for-Linear-Regression"><a href="#Gradient-Descent-for-Linear-Regression" class="headerlink" title="Gradient Descent for Linear Regression"></a>Gradient Descent for Linear Regression</h2><p>Now, we have learnt the gradient descent, the linear regression model and the squared error cost function as well. This time, we are going to put together gradient descent with our cost function that will give us an algorithm for linear regression for fitting a straight line to our data.</p>
<p>This is what we worked out:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6dedxiq58j30ej052aay.jpg" alt="image-20190826212442071"></p>
<p>We are going to apply gradient descent algorithm to minimize our squared error cost function.</p>
<p>The key term is the derivative term:</p>
<p>$$<br>\frac{\partial}{\partial\theta_j}J(\theta_0, \theta_1) = \frac{\partial}{\partial\theta_j}\frac{1}{2m}\sum^m_{i=1}[h_\theta(x^{(i)})-y^{(i)}]^2<br>=<br>\frac{\partial}{\partial\theta_j}\frac{1}{2m}\sum^m_{i=1}[\theta_0+\theta_1x^{(i)}-y^{(i)}]^2<br>$$</p>
<p>$$<br>\begin{array}{rl}<br>j=0: &amp; \frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)=\frac{1}{m}\sum^m_{i=1}[h_\theta(x^{(i)})-y^{(i)}]\<br> &amp; \<br>j=1: &amp; \frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)=\frac{1}{m}\sum^m_{i=1}[h_\theta(x^{(i)})-y^{(i)}] \cdot x^{(i)}<br>\end{array}<br>$$</p>
<p>Plug them back into our gradient descent algorithm:</p>
<blockquote>
<p>repeat until convergence {</p>
<p>$\qquad\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum^m_{i=1}[h_\theta(x^{(i)})-y^{(i)}]$</p>
<p>$\qquad\theta_1 := \theta_1 - \alpha\frac{1}{m}\sum^m_{i=1}[h_\theta(x^{(i)})-y^{(i)}] \cdot x^{(i)}$</p>
<p>}</p>
</blockquote>
<p>Notice: update $\theta_0$ and $\theta_1$ simultaneously.</p>
<p>The point of all this is that if we start with a guess for our hypothesis and then repeatedly apply these gradient descent equations, our hypothesis will become more and more accurate. So, this is simply gradient descent on the original cost function J. </p>
<p>This method looks at every example in the entire training set on every step, and is called <strong>batch gradient descent</strong>. </p>
<p>Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have     posed here for linear regression has only one global, and no other local,optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum. Indeed, J is a convex quadratic function. Here is an example of gradient descent as it is run to minimize a quadratic function.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6dhmaf0chj308q06qgml.jpg" alt="image-20190826231632514"></p>
<p>The ellipses shown above are the contours of a quadratic function. Also shown is the trajectory taken by gradient descent, which was initialized at (48,30). The x’s in the figure (joined by straight lines) mark the successive values of θ that gradient descent went through as itconverged to its minimum.</p>
<hr>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>A Wild Implement of Linear Regression with One Variable via Gradient Descent in Python Made by Myself:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># linregress.py</span></span><br><span class="line"><span class="comment"># Linear Regression with one variable via a Batch Gradient Descent</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Created by CDFMLR on 2019/8/28.</span></span><br><span class="line"><span class="comment"># Copyright © CDFMLR. All right reserved.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegressionWithOneVariable</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    # Linear regression with one variable</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &gt; Given a training set, to find a set of parameters (theta_0, theta_1) of hypothesis function `h(x) = theta_0 + theta_1 * x` via gradient descent so that h(x) is a &quot;good&quot; predictor for the corresponding value of y.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Properties</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        - `training_set`</span></span><br><span class="line"><span class="string">        - `theta`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Methods</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        - `regress`: to find a set of thetas to make hypothesis a &quot;good&quot; predictor</span></span><br><span class="line"><span class="string">        - `hypothesis`: to get a predicted value</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ``python</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; model = LinearRegressionWithOneVariable([(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6)])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; model.regress(verbose=True, epsilon=0.0001)</span></span><br><span class="line"><span class="string">    theta_0:        0.025486336182825836</span></span><br><span class="line"><span class="string">    theta_1:        0.9940305813471573</span></span><br><span class="line"><span class="string">    cost:           9.99815680487604e-05</span></span><br><span class="line"><span class="string">    hypothesis:     h(x) = 0.025486336182825836 + 0.9940305813471573*x</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; model.hypothesis(10)</span></span><br><span class="line"><span class="string">    9.9657921496544</span></span><br><span class="line"><span class="string">		``</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, training_set</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        training_set: training set</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.training_set = training_set    <span class="comment"># [(x: int, y: int), ...]</span></span><br><span class="line">        self.theta = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_hypothesis</span>(<span class="params">self, x, theta</span>):</span></span><br><span class="line">        <span class="keyword">return</span> theta[<span class="number">0</span>] + theta[<span class="number">1</span>] * x</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_cost</span>(<span class="params">self, dataset, theta</span>):</span></span><br><span class="line">        s = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> dataset:</span><br><span class="line">            s += (self._hypothesis(i[<span class="number">0</span>], theta) - i[<span class="number">1</span>]) ** <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> s / (<span class="number">2</span> * <span class="built_in">len</span>(dataset))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_gradient_descent</span>(<span class="params">self, dataset, starting_theta, learning_rate, epsilon, max_count=<span class="number">4294967296</span></span>):</span></span><br><span class="line">        theta = <span class="built_in">list</span>.copy(starting_theta)</span><br><span class="line">        last_theta = <span class="built_in">list</span>.copy(starting_theta)</span><br><span class="line">        cost = self._cost(dataset, theta)</span><br><span class="line">        last_cost = cost * <span class="number">2</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        epsilon = <span class="built_in">abs</span>(epsilon)</span><br><span class="line">        diff = epsilon + <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> (diff &gt; epsilon):</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> count &gt;= max_count:</span><br><span class="line">                <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;Failed in gradient descent: cannot convergence after &#123;mc&#125; iterations.&quot;</span>.<span class="built_in">format</span>(mc=max_count))</span><br><span class="line">    </span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                t_sum= <span class="built_in">sum</span>((self._hypothesis(i[<span class="number">0</span>], theta) - i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> dataset))</span><br><span class="line">                theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - learning_rate * t_sum / <span class="built_in">len</span>(dataset)</span><br><span class="line">    </span><br><span class="line">                t_sum = <span class="built_in">sum</span>(</span><br><span class="line">                    ((self._hypothesis(i[<span class="number">0</span>], theta) - i[<span class="number">1</span>]) * i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> dataset)</span><br><span class="line">                    )</span><br><span class="line">                theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - learning_rate * t_sum / <span class="built_in">len</span>(dataset)</span><br><span class="line">    </span><br><span class="line">                last_cost = cost</span><br><span class="line">                cost = self._cost(dataset, theta)</span><br><span class="line">    </span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>((math.isnan(x) <span class="keyword">or</span> math.isinf(x) <span class="keyword">for</span> x <span class="keyword">in</span> theta)) <span class="keyword">and</span> <span class="built_in">abs</span>(cost) &lt;= <span class="built_in">abs</span>(last_cost):</span><br><span class="line">                    diff = <span class="built_in">max</span>((<span class="built_in">abs</span>(last_theta[i] - theta[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(theta))))</span><br><span class="line">                    last_theta = <span class="built_in">list</span>.copy(theta)</span><br><span class="line">                    learning_rate += learning_rate * <span class="number">4</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    theta = <span class="built_in">list</span>.copy(last_theta)</span><br><span class="line">                    learning_rate /= <span class="number">10</span></span><br><span class="line">                    <span class="keyword">if</span> (learning_rate == <span class="number">0</span>):</span><br><span class="line">                        learning_rate = self._get_learning_rate(self.training_set)</span><br><span class="line">    </span><br><span class="line">                <span class="comment"># print(&#x27;[DEBUG] (%s) theta: %s, diff=%s, learning_rate=%s, cost=%s&#x27; % (count, theta, diff, learning_rate, cost))</span></span><br><span class="line">            <span class="keyword">except</span> OverflowError:</span><br><span class="line">                theta = <span class="built_in">list</span>.copy(last_theta)</span><br><span class="line">                learning_rate /= <span class="number">10</span></span><br><span class="line">                <span class="keyword">if</span> (learning_rate == <span class="number">0</span>):</span><br><span class="line">                    learning_rate = self._get_learning_rate(self.training_set)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> theta, count</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_learning_rate</span>(<span class="params">self, dataset</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> / <span class="built_in">max</span>((i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> dataset))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">regress</span>(<span class="params">self, epsilon=<span class="number">1</span>, learning_rate=<span class="number">0</span>, verbose=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        To find a set of thetas to make hypothesis a &quot;good&quot; predictor</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Parms:</span></span><br><span class="line"><span class="string">            - epsilon: when the difference between new theta and last theta less then epsilon, finish regressing</span></span><br><span class="line"><span class="string">            - learning_rate: about the &quot;step length&quot; of gtadient descent. Too small will take a long time to regress, and too big will raise a Overflow error. `0` to allow the algorithm to select an appropriate value automatically.</span></span><br><span class="line"><span class="string">            - verbose: true to print the result of regression</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        init_theta = [random.random(), random.random()]</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> learning_rate == <span class="number">0</span>:</span><br><span class="line">            learning_rate = self._get_learning_rate(self.training_set)</span><br><span class="line">        </span><br><span class="line">        self.theta, count = self._gradient_descent(self.training_set, init_theta, learning_rate, epsilon)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> verbose:</span><br><span class="line">            print(<span class="string">&#x27;Gradient descent finished after &#123;count&#125; iterations:\ntheta_0:\t&#123;t0&#125;\ntheta_1:\t&#123;t1&#125;\ncost:\t\t&#123;cost&#125;\nhypothesis:\th(x) = &#123;t0&#125; + &#123;t1&#125;*x&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    count=count, t0=self.theta[<span class="number">0</span>], t1=self.theta[<span class="number">1</span>], cost=self._cost(self.training_set, self.theta)))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hypothesis</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        To get a predicted y value of giving x</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parms:</span></span><br><span class="line"><span class="string">            - x: x value of the point you want to predict</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> self._hypothesis(x, self.theta)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = LinearRegressionWithOneVariable([(<span class="number">0</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">3</span>), (<span class="number">4</span>, <span class="number">4</span>), (<span class="number">5</span>, <span class="number">5</span>), (<span class="number">6</span>, <span class="number">6</span>)])</span><br><span class="line">    model.regress(verbose=<span class="literal">True</span>, epsilon=<span class="number">0.0000000001</span>)</span><br><span class="line">    model.hypothesis(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>





  </div>
</article>
<!--Disqus-->


<!--Livere-->

    <div class="blog-post-comments">
        <div id="lv-container" data-id="city" data-uid="MTAyMC80NjEzMi8yMjY0Mw==">
            <noscript>不启用 JavaScript 支持的人是看不到可爱的评论区的。😥</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Notes-of-Andrew-Ng%E2%80%99s-Machine-Learning-%E2%80%94%E2%80%94-1-Linear-Regression-with-One-Variable"><span class="toc-number">1.</span> <span class="toc-text">Notes of Andrew Ng’s Machine Learning —— (1) Linear Regression with One Variable</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Representation"><span class="toc-number">1.1.</span> <span class="toc-text">Model Representation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Notation"><span class="toc-number">1.1.1.</span> <span class="toc-text">Notation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-more-formal-description-of-supervised-learning"><span class="toc-number">1.1.2.</span> <span class="toc-text">A more formal description of supervised learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hypothesis-amp-Model-Regression"><span class="toc-number">1.1.3.</span> <span class="toc-text">Hypothesis &amp; Model Regression</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cost-Function"><span class="toc-number">1.2.</span> <span class="toc-text">Cost Function</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Cost-function"><span class="toc-number">1.2.1.</span> <span class="toc-text">Cost function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Goal"><span class="toc-number">1.2.2.</span> <span class="toc-text">Goal</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cost-Function-Intuition-I"><span class="toc-number">1.3.</span> <span class="toc-text">Cost Function - Intuition I</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cost-Function-Intuition-II"><span class="toc-number">1.4.</span> <span class="toc-text">Cost Function - Intuition II</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent"><span class="toc-number">1.5.</span> <span class="toc-text">Gradient Descent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent-Intuition"><span class="toc-number">1.6.</span> <span class="toc-text">Gradient Descent Intuition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent-for-Linear-Regression"><span class="toc-number">1.7.</span> <span class="toc-text">Gradient Descent for Linear Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experiment"><span class="toc-number">1.8.</span> <span class="toc-text">Experiment</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&text=单变量线性回归"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&title=单变量线性回归"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&is_video=false&description=单变量线性回归"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=单变量线性回归&body=Check out this article: https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&title=单变量线性回归"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&title=单变量线性回归"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&title=单变量线性回归"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&title=单变量线性回归"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/08/18/AndrewNgML/AndrewNg-MachineLearning-1-LinearRegressionWithOneVariable/&name=单变量线性回归&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2021 CDFMLR
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-146911386-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?9a0d2e6fde93dad496ac79f04f3aba97";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->


<!--Livere Comments-->

    <script type="text/javascript">
      (function (d, s) {
        var j, e = d.getElementsByTagName(s)[0];

        if (typeof LivereTower === 'function') { return; }

        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;

        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>

</body>
</html>
