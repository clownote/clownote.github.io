<!DOCTYPE html>
<html lang=zh>
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Deep Learning with Python 这篇文章是我学习《Deep Learning with Python》(第二版，François Chollet 著) 时写的系列笔记之一。文章的内容是从  Jupyter notebooks 转成 Markdown 的，当我完成所以文章后，会在 GitHub 发布我写的所有  Jupyter notebooks。 你可以在这个网址在线阅读这本">
<meta name="keywords" content="Machine Learning,Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Python深度学习之神经网络入门">
<meta property="og:url" content="https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/index.html">
<meta property="og:site_name" content="clownote">
<meta property="og:description" content="Deep Learning with Python 这篇文章是我学习《Deep Learning with Python》(第二版，François Chollet 著) 时写的系列笔记之一。文章的内容是从  Jupyter notebooks 转成 Markdown 的，当我完成所以文章后，会在 GitHub 发布我写的所有  Jupyter notebooks。 你可以在这个网址在线阅读这本">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge8cohvw31j31hi0lu49o.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjeapcizwj30at07qjri.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjeasgbdsj30az07q3ym.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjed34amjj30at07qdfx.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjed4tgtzj30at07qglp.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjefsh6f1j30at07qwel.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjeg0o13pj30at07qaa5.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjem9vu9zj30b007qmxh.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggje0e4s1qj30aq07r3yo.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gert24t4p8j317u0mstci.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggje37vwedj31fw0u0dmn.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggje0zqitmj30b007eq35.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggje11t08ij30b007egls.jpg">
<meta property="og:updated_time" content="2020-09-10T15:05:23.339Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Python深度学习之神经网络入门">
<meta name="twitter:description" content="Deep Learning with Python 这篇文章是我学习《Deep Learning with Python》(第二版，François Chollet 著) 时写的系列笔记之一。文章的内容是从  Jupyter notebooks 转成 Markdown 的，当我完成所以文章后，会在 GitHub 发布我写的所有  Jupyter notebooks。 你可以在这个网址在线阅读这本">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge8cohvw31j31hi0lu49o.jpg">
    
    
        
          
              <link rel="shortcut icon" href="/images/rabbit.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/rabbit_192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/rabbit_180.png">
          
        
    
    <!-- title -->
    <title>Python深度学习之神经网络入门</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
    <!--Google search varification (PRIVATE)-->
    <meta name="google-site-verification" content="MrqlpFAD8nDanw3Ypv7ZsIWHLnTdhRuLa4QhSVwxIvc">
    <!--Google AdSense 关联 (PRIVATE)-->
    <script data-ad-client="ca-pub-1510963483941114" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2020/07/10/DeepLearningWithPython/Deep-Learning with-Python-ch4/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2020/06/24/SwiftUI/SwiftUI_Essentials_2_Building_List_and_navigation/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&text=Python深度学习之神经网络入门"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&title=Python深度学习之神经网络入门"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&is_video=false&description=Python深度学习之神经网络入门"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Python深度学习之神经网络入门&body=Check out this article: https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&title=Python深度学习之神经网络入门"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&title=Python深度学习之神经网络入门"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&title=Python深度学习之神经网络入门"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&title=Python深度学习之神经网络入门"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&name=Python深度学习之神经网络入门&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#deep-learning-with-python"><span class="toc-number">1.</span> <span class="toc-text"> Deep Learning with Python</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#电影评论分类二分类问题"><span class="toc-number">1.1.</span> <span class="toc-text"> 电影评论分类:二分类问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#imdb-数据集"><span class="toc-number">1.1.1.</span> <span class="toc-text"> IMDB 数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据准备"><span class="toc-number">1.1.2.</span> <span class="toc-text"> 数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#建立网络"><span class="toc-number">1.1.3.</span> <span class="toc-text"> 建立网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#激活函数的作用"><span class="toc-number">1.1.3.1.</span> <span class="toc-text"> 激活函数的作用</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编译模型"><span class="toc-number">1.1.4.</span> <span class="toc-text"> 编译模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练模型"><span class="toc-number">1.1.5.</span> <span class="toc-text"> 训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#进一步实验"><span class="toc-number">1.1.6.</span> <span class="toc-text"> 进一步实验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#新闻分类-多分类问题"><span class="toc-number">1.2.</span> <span class="toc-text"> 新闻分类: 多分类问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#路透社数据集"><span class="toc-number">1.2.1.</span> <span class="toc-text"> 路透社数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据准备-2"><span class="toc-number">1.2.2.</span> <span class="toc-text"> 数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#构建网络"><span class="toc-number">1.2.3.</span> <span class="toc-text"> 构建网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编译模型-2"><span class="toc-number">1.2.4.</span> <span class="toc-text"> 编译模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#验证效果"><span class="toc-number">1.2.5.</span> <span class="toc-text"> 验证效果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练模型-2"><span class="toc-number">1.2.6.</span> <span class="toc-text"> 训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#处理标签和损失的另一种方法"><span class="toc-number">1.2.7.</span> <span class="toc-text"> 处理标签和损失的另一种方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#中间层维度足够大的重要性"><span class="toc-number">1.2.8.</span> <span class="toc-text"> 中间层维度足够大的重要性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#尝试使用更少更多的层"><span class="toc-number">1.2.9.</span> <span class="toc-text"> 尝试使用更少/更多的层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#预测房价-回归问题"><span class="toc-number">1.3.</span> <span class="toc-text"> 预测房价: 回归问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#波士顿房间数据集"><span class="toc-number">1.3.1.</span> <span class="toc-text"> 波士顿房间数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据准备-3"><span class="toc-number">1.3.2.</span> <span class="toc-text"> 数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#构建网络-2"><span class="toc-number">1.3.3.</span> <span class="toc-text"> 构建网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#拟合验证-k-fold-验证"><span class="toc-number">1.3.4.</span> <span class="toc-text"> 拟合验证 —— K-fold 验证</span></a></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Python深度学习之神经网络入门
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">clownote</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2020-07-08T11:35:00.000Z" itemprop="datePublished">2020-07-08</time>
        
        (Updated: <time datetime="2020-09-10T15:05:23.339Z" itemprop="dateModified">2020-09-10</time>)
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a> › <a class="category-link" href="/categories/Machine-Learning/Deep-Learning-with-Python/">Deep Learning with Python</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a>, <a class="tag-link" href="/tags/Machine-Learning/">Machine Learning</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="deep-learning-with-python"><a class="markdownIt-Anchor" href="#deep-learning-with-python"></a> Deep Learning with Python</h1>
<p>这篇文章是我学习《Deep Learning with Python》(第二版，François Chollet 著) 时写的系列笔记之一。文章的内容是从  Jupyter notebooks 转成 Markdown 的，当我完成所以文章后，会在 GitHub 发布我写的所有  Jupyter notebooks。</p>
<p>你可以在这个网址在线阅读这本书的正版原文(英文)：<a href="https://livebook.manning.com/book/deep-learning-with-python" target="_blank" rel="noopener">https://livebook.manning.com/book/deep-learning-with-python</a></p>
<p>这本书的作者也给出了一套 Jupyter notebooks：<a href="https://github.com/fchollet/deep-learning-with-python-notebooks" target="_blank" rel="noopener">https://github.com/fchollet/deep-learning-with-python-notebooks</a></p>
<hr>
<p>本文为 <strong>第3章 神经网络入门</strong> (Chapter 3. Getting started with neural networks) 的笔记整合。</p>
<p>本文目录：</p>
<p>[TOC]</p>
<h2 id="电影评论分类二分类问题"><a class="markdownIt-Anchor" href="#电影评论分类二分类问题"></a> 电影评论分类:二分类问题</h2>
<p><a href="https://livebook.manning.com/book/deep-learning-with-python/chapter-3/101" target="_blank" rel="noopener">原文链接</a></p>
<h3 id="imdb-数据集"><a class="markdownIt-Anchor" href="#imdb-数据集"></a> IMDB 数据集</h3>
<p>IMDB 数据集里是 50,000 条电影评论。一半是训练集，一半是测试集。<br>
数据里 50% 是积极评价，50% 是消极评价。</p>
<p>Keras 内置了做过预处理的 IMDB 数据集，把单词序列转化成了整数序列（一个数字对应字典里的词）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(</span><br><span class="line">    num_words=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure>
<p><code>num_words=10000</code> 是只保留出现频率前 10000 的单词。</p>
<p>先来随便看一条评论，这是条好评：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 字典</span></span><br><span class="line">index_word = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> imdb.get_word_index().items()&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 还原一条评论看看</span></span><br><span class="line">text = <span class="string">' '</span>.join([index_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> train_data[<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"<span class="subst">&#123;train_labels[<span class="number">0</span>]&#125;</span>:"</span>, text)</span><br></pre></td></tr></table></figure>
<pre><code>1: the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an
</code></pre>
<h3 id="数据准备"><a class="markdownIt-Anchor" href="#数据准备"></a> 数据准备</h3>
<p>先看一下 train_data 现在的形状：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.shape</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>(25000,)
</code></pre>
<p>我们要把它变成 <code>(samples, word_indices)</code> 的样子，大概是下面这种：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[0, 0, ..., 1, ..., 0, ..., 1],</span><br><span class="line"> [0, 1, ..., 0, ..., 1, ..., 0],</span><br><span class="line"> ...</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>有这个词就是 1，没有就是 0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_train</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>array([[0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.],
       ...,
       [0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.]])
</code></pre>
<p>labels 也随便搞一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_labels</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>array([1, 0, 0, ..., 0, 1, 0])
</code></pre>
<p>处理一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train = np.asarray(train_labels).astype(<span class="string">'float32'</span>)</span><br><span class="line">y_test = np.asarray(test_labels).astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>array([1., 0., 0., ..., 0., 1., 0.], dtype=float32)
</code></pre>
<p>现在这些数据就可以安全投喂我们一会儿建的神经网络了。</p>
<h3 id="建立网络"><a class="markdownIt-Anchor" href="#建立网络"></a> 建立网络</h3>
<p>对于这种输入是向量、标签是标量（甚至是 0 或 1）的问题：<br>
使用 relu 激活的 Dense (全连接)堆起来的网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>)</span><br></pre></td></tr></table></figure>
<p>这种层的作用是 <code>output = relu(dot(W, input) + b)</code>。</p>
<p>16 是每层里隐藏单元(hidden unit)的个数。一个 hidden unit 就是在这层的表示空间里的一个维度。<br>
W 的形状也是 <code>(input_dimension, 16)</code>，dot 出来就是个 16 维的向量，也就把数据投影到了 16 维的表示空间。</p>
<p>这个维度 (hidden unit 的数量) 可以看成对网络学习的自由度的控制。<br>
维度越高，能学的东西越复杂，但计算消耗也越大，而且可能学到一些不重要的东西导致过拟合。</p>
<p>这里，我们将使用两层这种16个隐藏单元的层，<br>
最后还有一个 sigmoid 激活的层来输出结果（在<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>内的值），<br>
这个结果表示预测有多可能这个数据的标签是1，即一条好评。</p>
<p>relu 是过滤掉负值（把输入的负值输出成0），sigmoid 是把值投到 <code>[0, 1]</code>：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge8cohvw31j31hi0lu49o.jpg" alt="relu and sigmoid"></p>
<p>在 Keras 中实现这个网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<h4 id="激活函数的作用"><a class="markdownIt-Anchor" href="#激活函数的作用"></a> 激活函数的作用</h4>
<p>我们之前在 MNIST 里用过 relu 激活函数，所以<em>激活函数</em>到底是干嘛的？</p>
<p>一个没有激活函数的 Dense 层的作用只是一个线性变换：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = dot(W, input) + b</span><br></pre></td></tr></table></figure>
<p>如果每一层都是这种线性变换，把多个这种层叠在一起，假设空间并不会变大，所以能学到的东西很有限。</p>
<p>而激活函数就是在 <code>dot(W, input) + b</code> 外面套的一个函数，比如 relu 激活是 <code>output = relu(dot(W, input) + b)</code>。<br>
利用这种激活函数，可以拓展表示空间，也就可以让网络学习到更复杂的“知识”。</p>
<h3 id="编译模型"><a class="markdownIt-Anchor" href="#编译模型"></a> 编译模型</h3>
<p>编译模型时，我们还需要选择损失函数、优化器和指标。</p>
<p>对这种最后输出 0 或 1 的二元分类问题，损失函数可以使用 <code>binary_crossentropy</code>(从名字就可以看得出来很合适啦)。</p>
<p>这个 <em>crossentropy</em> 中文叫交叉熵，是信息论里的，是用来衡量概率分布直接的距离的。<br>
所以输出概率的模型经常是用这种 crossentropy 做损失的。</p>
<p>至于优化器，和 MNIST 一样，我们用 <code>rmsprop</code> （书里还没写为什么），指标也还是准确度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<p>因为这几个optimizer、loss、metrics 都是常用的，所以 Keras 内置了，可以直接传字符串。<br>
但也可以传类实例来定制一些参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> optimizers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> losses</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">              loss=losses.binary_crossentropy,</span><br><span class="line">              metrics=[metrics.binary_accuracy])</span><br></pre></td></tr></table></figure>
<h3 id="训练模型"><a class="markdownIt-Anchor" href="#训练模型"></a> 训练模型</h3>
<p>为了在训练的过程中验证模型在它没见过的数据上精度如何，我们从原来的训练数据里分 10,000 个样本出来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_val = x_train[:<span class="number">10000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">10000</span>:]</span><br><span class="line"></span><br><span class="line">y_val = y_train[:<span class="number">10000</span>]</span><br><span class="line">partial_y_train = y_train[<span class="number">10000</span>:]</span><br></pre></td></tr></table></figure>
<p>用一批 512 个样本的 mini-batches，跑 20 轮（所有x_train里的数据跑一遍算一轮），<br>
并用刚分出来的 10,000 的样本做精度验证：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'acc'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                    partial_y_train,</span><br><span class="line">                    epochs=<span class="number">20</span>,</span><br><span class="line">                    batch_size=<span class="number">512</span>,</span><br><span class="line">                    validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<pre><code>Train on 15000 samples, validate on 10000 samples
Epoch 1/20
15000/15000 [==============================] - 3s 205us/sample - loss: 0.5340 - acc: 0.7867 - val_loss: 0.4386 - val_acc: 0.8340
.......
Epoch 20/20
15000/15000 [==============================] - 1s 74us/sample - loss: 0.0053 - acc: 0.9995 - val_loss: 0.7030 - val_acc: 0.8675
</code></pre>
<p><code>fit</code> 阔以返回 history，里面保存了训练过程里每个 Epoch 的黑历史：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">history_dict = history.history</span><br><span class="line">history_dict.keys()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])
</code></pre>
<p>我们可以把这些东西画图出来看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 画训练和验证的损失</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">history_dict = history.history</span><br><span class="line">loss_values = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">val_loss_values = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss_values) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss_values, <span class="string">'ro-'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss_values, <span class="string">'bs-'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjeapcizwj30at07qjri.jpg" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 画训练和验证的准确度</span></span><br><span class="line"></span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">acc = history_dict[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history_dict[<span class="string">'val_acc'</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc, <span class="string">'ro-'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'bs-'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjeasgbdsj30az07q3ym.jpg" alt="png"></p>
<p>我们可以看到，训练集上的精度倒是一直在增（损失一直减），<br>
但在验证集上，到了后面损失反而大了，差不多第4轮左右就到最好的峰值了。</p>
<p>这就是过拟合了，其实从第二轮开始就开始过了。所以，我们其实跑个3、4轮就 ok 了。<br>
要是再跑下去，咱的模型就只“精通”训练集，不认识其他没见过的数据了。</p>
<p>所以，我们重新训练一个模型（要从建立网络开始重写，不然fit是接着刚才已经进行过的这些），那去用测试集测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">             loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">             metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">4</span>, batch_size=<span class="number">512</span>)</span><br><span class="line">result = model.evaluate(x_test, y_test, verbose=<span class="number">2</span>)    <span class="comment"># verbose=2 to avoid a looooong progress bar that fills the screen with '='. https://github.com/tensorflow/tensorflow/issues/32286</span></span><br></pre></td></tr></table></figure>
<pre><code>Train on 25000 samples
Epoch 1/4
25000/25000 [==============================] - 2s 69us/sample - loss: 0.4829 - accuracy: 0.8179
Epoch 2/4
25000/25000 [==============================] - 1s 42us/sample - loss: 0.2827 - accuracy: 0.9054
Epoch 3/4
25000/25000 [==============================] - 1s 42us/sample - loss: 0.2109 - accuracy: 0.9253
Epoch 4/4
25000/25000 [==============================] - 1s 43us/sample - loss: 0.1750 - accuracy: 0.9380
25000/1 - 3s - loss: 0.2819 - accuracy: 0.8836
</code></pre>
<p>把结果输出出来看看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<pre><code>[0.2923990402317047, 0.8836]
</code></pre>
<p>训练完成后，我们当然想实际试一下，对吧。所以我们预测一下测试集，把结果打出来看看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.predict(x_test)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<pre><code>array([[0.17157233],
       [0.99989915],
       [0.79564804],
       ...,
       [0.11750051],
       [0.05890778],
       [0.5040823 ]], dtype=float32)
</code></pre>
<h3 id="进一步实验"><a class="markdownIt-Anchor" href="#进一步实验"></a> 进一步实验</h3>
<ol>
<li>尝试只用一个层</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line"><span class="comment"># model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))</span></span><br><span class="line"><span class="comment"># model.add(layers.Dense(16, activation='relu'))</span></span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">             loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">             metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">4</span>, batch_size=<span class="number">512</span>)</span><br><span class="line">result = model.evaluate(x_test, y_test, verbose=<span class="number">2</span>)    <span class="comment"># verbose=2 to avoid a looooong progress bar that fills the screen with '='. https://github.com/tensorflow/tensorflow/issues/32286</span></span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<pre><code>Train on 25000 samples
Epoch 1/4
25000/25000 [==============================] - 3s 116us/sample - loss: 0.5865 - accuracy: 0.7814
Epoch 2/4
25000/25000 [==============================] - 1s 31us/sample - loss: 0.4669 - accuracy: 0.8608
Epoch 3/4
25000/25000 [==============================] - 1s 32us/sample - loss: 0.3991 - accuracy: 0.8790
Epoch 4/4
25000/25000 [==============================] - 1s 33us/sample - loss: 0.3538 - accuracy: 0.8920
25000/1 - 3s - loss: 0.3794 - accuracy: 0.8732
[0.3726908649635315, 0.8732]
</code></pre>
<p>…这问题比较简单，一个层效果都这么好，但比之前正经的差远了</p>
<ol start="2">
<li>多搞几个层</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">             loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">             metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">4</span>, batch_size=<span class="number">512</span>)</span><br><span class="line">result = model.evaluate(x_test, y_test, verbose=<span class="number">2</span>)    <span class="comment"># verbose=2 to avoid a looooong progress bar that fills the screen with '='. https://github.com/tensorflow/tensorflow/issues/32286</span></span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<pre><code>Train on 25000 samples
Epoch 1/4
25000/25000 [==============================] - 3s 123us/sample - loss: 0.5285 - accuracy: 0.7614
Epoch 2/4
25000/25000 [==============================] - 1s 45us/sample - loss: 0.2683 - accuracy: 0.9072s - loss:
Epoch 3/4
25000/25000 [==============================] - 1s 45us/sample - loss: 0.1949 - accuracy: 0.9297
Epoch 4/4
25000/25000 [==============================] - 1s 47us/sample - loss: 0.1625 - accuracy: 0.9422
25000/1 - 2s - loss: 0.3130 - accuracy: 0.8806
[0.30894253887176515, 0.88056]
</code></pre>
<p>好一点，但也还不如正经的版本</p>
<ol start="3">
<li>多几个隐藏层的单元</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">1024</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">             loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">             metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">4</span>, batch_size=<span class="number">512</span>)</span><br><span class="line">result = model.evaluate(x_test, y_test, verbose=<span class="number">2</span>)    <span class="comment"># verbose=2 to avoid a looooong progress bar that fills the screen with '='. https://github.com/tensorflow/tensorflow/issues/32286</span></span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<pre><code>Train on 25000 samples
Epoch 1/4
25000/25000 [==============================] - 15s 593us/sample - loss: 0.5297 - accuracy: 0.7964
Epoch 2/4
25000/25000 [==============================] - 12s 490us/sample - loss: 0.2233 - accuracy: 0.9109
Epoch 3/4
25000/25000 [==============================] - 12s 489us/sample - loss: 0.1148 - accuracy: 0.9593
Epoch 4/4
25000/25000 [==============================] - 12s 494us/sample - loss: 0.0578 - accuracy: 0.9835
25000/1 - 9s - loss: 0.3693 - accuracy: 0.8812
[0.4772889766550064, 0.8812]
</code></pre>
<p>不是远多越好呀。</p>
<ol start="4">
<li>用 mse 损失</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">             loss=<span class="string">'mse'</span>,</span><br><span class="line">             metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">4</span>, batch_size=<span class="number">512</span>)</span><br><span class="line">result = model.evaluate(x_test, y_test, verbose=<span class="number">2</span>)    <span class="comment"># verbose=2 to avoid a looooong progress bar that fills the screen with '='. https://github.com/tensorflow/tensorflow/issues/32286</span></span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<pre><code>Train on 25000 samples
Epoch 1/4
25000/25000 [==============================] - 3s 119us/sample - loss: 0.1472 - accuracy: 0.8188
Epoch 2/4
25000/25000 [==============================] - 1s 46us/sample - loss: 0.0755 - accuracy: 0.9121
Epoch 3/4
25000/25000 [==============================] - 1s 50us/sample - loss: 0.0577 - accuracy: 0.9319
Epoch 4/4
25000/25000 [==============================] - 1s 47us/sample - loss: 0.0474 - accuracy: 0.9442
25000/1 - 3s - loss: 0.0914 - accuracy: 0.8828
[0.08648386991858482, 0.88276]
</code></pre>
<ol start="5">
<li>用 tanh 激活</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'tanh'</span>, input_shape=(<span class="number">10000</span>, )))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'tanh'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">             loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">             metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">4</span>, batch_size=<span class="number">512</span>)</span><br><span class="line">result = model.evaluate(x_test, y_test, verbose=<span class="number">2</span>)    <span class="comment"># verbose=2 to avoid a looooong progress bar that fills the screen with '='. https://github.com/tensorflow/tensorflow/issues/32286</span></span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<pre><code>Train on 25000 samples
Epoch 1/4
25000/25000 [==============================] - 4s 149us/sample - loss: 0.4237 - accuracy: 0.8241
Epoch 2/4
25000/25000 [==============================] - 1s 46us/sample - loss: 0.2310 - accuracy: 0.9163
Epoch 3/4
25000/25000 [==============================] - 1s 46us/sample - loss: 0.1779 - accuracy: 0.9329
Epoch 4/4
25000/25000 [==============================] - 1s 49us/sample - loss: 0.1499 - accuracy: 0.9458
25000/1 - 3s - loss: 0.3738 - accuracy: 0.8772
[0.3238203083658218, 0.87716]
</code></pre>
<p>所以，这些实验就是说，之前我们书上用的模型是合理的，你改来改去都不如他那个😂。</p>
<h2 id="新闻分类-多分类问题"><a class="markdownIt-Anchor" href="#新闻分类-多分类问题"></a> 新闻分类: 多分类问题</h2>
<p><a href="https://livebook.manning.com/book/deep-learning-with-python/chapter-3/192" target="_blank" rel="noopener">原文链接</a></p>
<p>刚才我们电影评论问题不是把向量输入分成两类嘛，这节我们要把东西分成多类，即做“多分类（multi-class classification）”。</p>
<p>我们要把来自路透社的新闻分到 46 个话题种类里，这里要求一条新闻只能属于一个类，所以具体来说，我们要做的是一个“单标签多分类（single-label, multiclass classification）”问题。</p>
<h3 id="路透社数据集"><a class="markdownIt-Anchor" href="#路透社数据集"></a> 路透社数据集</h3>
<p>the Reuters dataset，路透社在 1986 年（比我老多了😂）发布的数据集，里面有 46 类新闻，训练集里每类至少 10 条数据。</p>
<p>这个玩具数据集和 IMDB、MNIST 一样，也在  Keras 里内置了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> reuters</span><br><span class="line"></span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = reuters.load_data(</span><br><span class="line">    num_words=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz
2113536/2110848 [==============================] - 6s 3us/step
</code></pre>
<p>这个数据集里面的数据和之前的 IMDB 一样，把单词翻译成了数字，然后我们只截取出现频率最高的10000个词。</p>
<p>咱们这个训练集里有 8K+ 条数据，测试集 2K+：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(len(train_data), len(test_data))</span><br></pre></td></tr></table></figure>
<pre><code>8982 2246
</code></pre>
<p>咱们还是像搞 IMDB 时那样，把数据还原会文本看看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode_news</span><span class="params">(data)</span>:</span></span><br><span class="line">    reverse_word_index = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> reuters.get_word_index().items()&#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">' '</span>.join([reverse_word_index.get(i - <span class="number">3</span>, <span class="string">'?'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> data])</span><br><span class="line">    <span class="comment"># i - 3 是因为 0、1、2 为保留词 “padding”(填充)、“start of sequence”(序列开始)、“unknown”(未知词)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">text = decode_news(train_data[<span class="number">0</span>])</span><br><span class="line">print(text)</span><br></pre></td></tr></table></figure>
<pre><code>? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3
</code></pre>
<p>标签是 0~45 的数字：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_labels[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<pre><code>3
</code></pre>
<h3 id="数据准备-2"><a class="markdownIt-Anchor" href="#数据准备-2"></a> 数据准备</h3>
<p>首先，还是把数据位向量化，直接套用我们搞 IMDB 时写的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br></pre></td></tr></table></figure>
<p>然后就是这种效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x_train)</span><br></pre></td></tr></table></figure>
<pre><code>array([[0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.],
       ...,
       [0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.]])
</code></pre>
<p>然后要处理标签。我们可以把标签处理成整数张量，也可以用 <code>One-hot</code> 编码<br>
对于分类这种问题，我们常用 one-hot 编码（也叫<em>分类编码</em>，categorical encoding）。</p>
<p>对于我们当前的问题，使用 one-hot 编码，即用除了标签索引位置为 1 其余位置全为 0 的向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_one_hot</span><span class="params">(labels, dimension=<span class="number">46</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(labels), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">        results[i, label] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">one_hot_train_labels = to_one_hot(train_labels)</span><br><span class="line">one_hot_test_labels = to_one_hot(test_labels)</span><br></pre></td></tr></table></figure>
<p>其实，，，Keras 里自带了一个可以干这个事情的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"><span class="comment"># 书上是 from keras.utils.np_utils import to_categorical 但，，，时代变了，而且咱这用的是 tensorflow.keras，所以稍微有点区别</span></span><br><span class="line"></span><br><span class="line">one_hot_train_labels = to_categorical(train_labels)</span><br><span class="line">one_hot_test_labels = to_categorical(test_labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(one_hot_train_labels)</span><br></pre></td></tr></table></figure>
<pre><code>array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
</code></pre>
<h3 id="构建网络"><a class="markdownIt-Anchor" href="#构建网络"></a> 构建网络</h3>
<p>这个问题和之前的电影评论分类问题还是差不多的，只是最后的解的可能从 2 -&gt; 46，解空间大了太多了。</p>
<p>对于我们用的 Dense 层堆叠，每层都是接收上一层输出的信息作为输入。<br>
所以，如果某一层丢失了一些信息，那么这些信息就再也不能被后面的层找回来了。<br>
如果丢失的信息对分类没用，那这种丢失是好的、我们期望发生的；<br>
但如果这些丢失的信息是对最后分类起作用的，那这种丢失就制约网络的结果了。<br>
也就是说，这可能造成一种“信息瓶颈”。这种瓶颈在每一层都可能发生。</p>
<p>之前的电影评论分类最后只要 2 个结果，所以我们把层里的单元是用了 16 个，<br>
即让机器在一个 16 维空间里学习，以及足够大了，不太会有“信息瓶颈“。</p>
<p>而我们现在的问题，解空间是 46 维的。<br>
直接照搬之前的代码，让它在 16 维空间里学习，肯定有瓶颈！</p>
<p>解决瓶颈的办法很简单，直接增加层里的单元就好。这里我们是 16 -&gt; 64:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br></pre></td></tr></table></figure>
<p>在最后一层，我们的输出是 46 维的，对应 46 种分类，<br>
而这一层的激活函数是 softmax，和我们在训练 MNIST 时用的一样。</p>
<p>用 softmax 可以让网络输出在 46 种分类上的概率分布，即一个 46 维的向量，<br>
其中第 i 个元素代表输入属于第 i 种分类的可能性，<br>
并且这 46 个元素的总和为 <code>1</code>。</p>
<h3 id="编译模型-2"><a class="markdownIt-Anchor" href="#编译模型-2"></a> 编译模型</h3>
<p>编译模型，又要确定损失函数、优化器和优化的目标了。</p>
<ul>
<li>损失函数，分类问题嘛，还是用“分类交叉熵” categorical_crossentropy。</li>
<li>优化器，其实对很多问题我们都是用 rmsprop</li>
<li>目标还是一个，预测的精度 accuracy</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<h3 id="验证效果"><a class="markdownIt-Anchor" href="#验证效果"></a> 验证效果</h3>
<p>我们还是要搞一个验证集来在训练过程中评估模型的。从训练集里分个 1K 条数据出来就好：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_val = x_train[:<span class="number">1000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">1000</span>:]</span><br><span class="line"></span><br><span class="line">y_val = one_hot_train_labels[:<span class="number">1000</span>]</span><br><span class="line">partial_y_train = one_hot_train_labels[<span class="number">1000</span>:]</span><br></pre></td></tr></table></figure>
<h3 id="训练模型-2"><a class="markdownIt-Anchor" href="#训练模型-2"></a> 训练模型</h3>
<p>好了，准备工作完成，又可以看到最迷人的训练过程了！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                    partial_y_train,</span><br><span class="line">                    epochs=<span class="number">20</span>,</span><br><span class="line">                    batch_size=<span class="number">512</span>,</span><br><span class="line">                    validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<pre><code>Train on 7982 samples, validate on 1000 samples
Epoch 1/20
7982/7982 [==============================] - 3s 372us/sample - loss: 2.6180 - accuracy: 0.5150 - val_loss: 1.7517 - val_accuracy: 0.6290
......
Epoch 20/20
7982/7982 [==============================] - 1s 91us/sample - loss: 0.1134 - accuracy: 0.9578 - val_loss: 1.0900 - val_accuracy: 0.8040
</code></pre>
<p>🆗挺快的，照例，还是画图看看训练过程。</p>
<ol>
<li>训练过程中的损失</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo-'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'rs-'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjed34amjj30at07qdfx.jpg" alt="png"></p>
<ol start="2">
<li>训练过程中的精度</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">acc = history.history[<span class="string">'accuracy'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_accuracy'</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo-'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'rs-'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjed4tgtzj30at07qglp.jpg" alt="png"></p>
<p>Emmmm，说，第9轮 epochs 的时候开始过拟合了（你看validation的曲线抖在第9轮了一下）。<br>
所以只要跑 9 轮就够了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(partial_x_train,</span><br><span class="line">          partial_y_train,</span><br><span class="line">          epochs=<span class="number">9</span>,</span><br><span class="line">          batch_size=<span class="number">512</span>,</span><br><span class="line">          validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<pre><code>Train on 7982 samples, validate on 1000 samples
Epoch 1/9
7982/7982 [==============================] - 1s 153us/sample - loss: 2.5943 - accuracy: 0.5515 - val_loss: 1.7017 - val_accuracy: 0.6410
......
Epoch 9/9
7982/7982 [==============================] - 1s 84us/sample - loss: 0.2793 - accuracy: 0.9402 - val_loss: 0.8758 - val_accuracy: 0.8170

&lt;tensorflow.python.keras.callbacks.History at 0x16eb5d6d0&gt;
</code></pre>
<p>然后，用测试集测试一下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">results = model.evaluate(x_test, one_hot_test_labels, verbose=<span class="number">2</span>)</span><br><span class="line">print(results)</span><br></pre></td></tr></table></figure>
<pre><code>2246/1 - 0s - loss: 1.7611 - accuracy: 0.7912
[0.983459981976082, 0.7911843]
</code></pre>
<p>精度差不多 80%，其实还是不错的了，比随机瞎划线去分好多了。</p>
<p>如果随机划线去分类的话，对二元分类问题精度是 50 %，而对这 46 元的分类精度只要不到 19% 了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">test_labels_copy = copy.copy(test_labels)</span><br><span class="line">np.random.shuffle(test_labels_copy)</span><br><span class="line">hits_array = np.array(test_labels) == np.array(test_labels_copy)</span><br><span class="line">float(np.sum(hits_array)) / len(test_labels)</span><br></pre></td></tr></table></figure>
<pre><code>0.18432769367764915
</code></pre>
<p>调用 model 实例的 predict 方法，可以得到对输入在 46 个分类上的概率分布：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predictions = model.predict(x_test)</span><br><span class="line">print(predictions)</span><br></pre></td></tr></table></figure>
<pre><code>array([[4.7181980e-05, 2.0765587e-05, 8.6653872e-06, ..., 3.1266565e-05,
        8.2046267e-07, 6.0611728e-06],
       [5.9005950e-04, 1.3404934e-02, 1.2290048e-03, ..., 4.2919168e-05,
        5.7422225e-05, 4.0201416e-05],
       [8.5751421e-04, 9.2367262e-01, 1.5855590e-03, ..., 4.8341672e-04,
        4.5594123e-05, 2.6183401e-05],
       ...,
       [8.5679676e-05, 2.0081598e-04, 4.1808224e-05, ..., 7.6962686e-05,
        6.5783697e-06, 2.9889508e-05],
       [1.7291466e-03, 2.5600385e-02, 1.8182390e-03, ..., 1.4499390e-03,
        4.8478998e-04, 8.5257640e-04],
       [2.5776261e-04, 8.6797208e-01, 3.9900807e-03, ..., 2.6547859e-04,
        6.5820634e-05, 6.8603881e-06]], dtype=float32)
</code></pre>
<p>predictions 分别代表 46 个分类的可能:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions[<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>
<pre><code>(46,)
</code></pre>
<p>他们的总和为 1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.sum(predictions[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>0.99999994
</code></pre>
<p>其中最大的，即我们认为这条新闻属于这个分类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.argmax(predictions[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>3
</code></pre>
<h3 id="处理标签和损失的另一种方法"><a class="markdownIt-Anchor" href="#处理标签和损失的另一种方法"></a> 处理标签和损失的另一种方法</h3>
<p>前面提到了标签可以使用 one-hot 编码，或者直接把标签处理成整数张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train = np.array(train_labels)</span><br><span class="line">y_test = np.array(test_labels)</span><br></pre></td></tr></table></figure>
<p>用这种的话，损失函数也要跟着改，改成 sparse_categorical_crossentropy，<br>
这个和 categorical_crossentropy 在数学上是一样的，只是接口不同：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'acc'</span>])</span><br></pre></td></tr></table></figure>
<h3 id="中间层维度足够大的重要性"><a class="markdownIt-Anchor" href="#中间层维度足够大的重要性"></a> 中间层维度足够大的重要性</h3>
<p>之前讨论了关于“信息瓶颈”的事，然后我们就说对这个 46 维结果的网络，中间层的维度要足够大！</p>
<p>现在咱试试如果不够大（导致信息瓶颈）会怎么样，咱搞夸张一点，从 64 减到 4：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">4</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(partial_x_train,</span><br><span class="line">          partial_y_train,</span><br><span class="line">          epochs=<span class="number">20</span>,</span><br><span class="line">          batch_size=<span class="number">128</span>,</span><br><span class="line">          validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<pre><code>Train on 7982 samples, validate on 1000 samples
Epoch 1/20
7982/7982 [==============================] - 2s 288us/sample - loss: 2.8097 - accuracy: 0.4721 - val_loss: 2.0554 - val_accuracy: 0.5430
.......
Epoch 20/20
7982/7982 [==============================] - 1s 121us/sample - loss: 0.6443 - accuracy: 0.8069 - val_loss: 1.8962 - val_accuracy: 0.6800

&lt;tensorflow.python.keras.callbacks.History at 0x16f628b50&gt;
</code></pre>
<p>看看这，这训练出来比之前 64 维的差的不是一点半点哈，差距相当明显了。</p>
<p>发生这种效果的下降就是因为你给他学习的空间维度太低了，他把好多对分类有用的信息抛弃了。</p>
<p>那是不是越大越好？我们再试试把中间层加大一些：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">4096</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(partial_x_train,</span><br><span class="line">          partial_y_train,</span><br><span class="line">          epochs=<span class="number">20</span>,</span><br><span class="line">          batch_size=<span class="number">128</span>,</span><br><span class="line">          validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<pre><code>Train on 7982 samples, validate on 1000 samples
Epoch 1/20
7982/7982 [==============================] - 2s 273us/sample - loss: 1.5523 - accuracy: 0.6310 - val_loss: 1.1903 - val_accuracy: 0.7060
......
Epoch 20/20
7982/7982 [==============================] - 2s 296us/sample - loss: 0.0697 - accuracy: 0.9605 - val_loss: 3.5296 - val_accuracy: 0.7850

&lt;tensorflow.python.keras.callbacks.History at 0x1707fcf90&gt;
</code></pre>
<p>可以看到训练用的时间长了一点，电脑更暖手了一点，但效果却没有多大的提升。<br>
这是由于第一层输入到中间层的只有 64 维嘛，中间层再大，也被第一层的瓶颈制约了。</p>
<p>在试试把第一层也加大！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(partial_x_train,</span><br><span class="line">          partial_y_train,</span><br><span class="line">          epochs=<span class="number">20</span>,</span><br><span class="line">          batch_size=<span class="number">128</span>,</span><br><span class="line">          validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<pre><code>Train on 7982 samples, validate on 1000 samples
Epoch 1/20
7982/7982 [==============================] - 5s 662us/sample - loss: 1.3423 - accuracy: 0.6913 - val_loss: 0.9565 - val_accuracy: 0.7920
......
Epoch 20/20
7982/7982 [==============================] - 5s 583us/sample - loss: 0.0648 - accuracy: 0.9597 - val_loss: 2.9887 - val_accuracy: 0.8030

&lt;tensorflow.python.keras.callbacks.History at 0x176fbbd90&gt;
</code></pre>
<p>（稍微小一点，本来是用 4096 的，但太大了，咱乞丐版 mbp 跑的贼慢，跑完要20多分钟，我懒得等）</p>
<p>这个多浪费了好多时间，而且他很快就<s>过泥河</s>过拟合了，过得还过得很严重，画个图看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">loss = _.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = _.history[<span class="string">'val_loss'</span>]</span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo-'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'rs-'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjefsh6f1j30at07qwel.jpg" alt="png"></p>
<p>所以，太大了也不好。还是要有个度！</p>
<h3 id="尝试使用更少更多的层"><a class="markdownIt-Anchor" href="#尝试使用更少更多的层"></a> 尝试使用更少/更多的层</h3>
<ol>
<li>更少的层</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">          partial_y_train,</span><br><span class="line">          epochs=<span class="number">20</span>,</span><br><span class="line">          batch_size=<span class="number">128</span>,</span><br><span class="line">          validation_data=(x_val, y_val))</span><br><span class="line"></span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo-'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'rs-'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>Train on 7982 samples, validate on 1000 samples
Epoch 1/20
7982/7982 [==============================] - 1s 132us/sample - loss: 2.4611 - accuracy: 0.6001 - val_loss: 1.8556 - val_accuracy: 0.6440
......
Epoch 20/20
7982/7982 [==============================] - 1s 85us/sample - loss: 0.1485 - accuracy: 0.9570 - val_loss: 1.2116 - val_accuracy: 0.7960
</code></pre>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjeg0o13pj30at07qaa5.jpg" alt="png"></p>
<p>快呀！结果稍微差了一点点。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Clown0te(<span class="string">"防盗文爬:)虫的追踪标签，读者不必在意"</span>).by(CDFMLR)</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>更多的层</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">          partial_y_train,</span><br><span class="line">          epochs=<span class="number">20</span>,</span><br><span class="line">          batch_size=<span class="number">128</span>,</span><br><span class="line">          validation_data=(x_val, y_val))</span><br><span class="line"></span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo-'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'rs-'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>Train on 7982 samples, validate on 1000 samples
Epoch 1/20
7982/7982 [==============================] - 2s 188us/sample - loss: 1.8340 - accuracy: 0.5829 - val_loss: 1.3336 - val_accuracy: 0.6910
......
Epoch 20/20
7982/7982 [==============================] - 1s 115us/sample - loss: 0.0891 - accuracy: 0.9600 - val_loss: 1.7227 - val_accuracy: 0.7900
</code></pre>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjem9vu9zj30b007qmxh.jpg" alt="png"></p>
<p>所以，这个也不是越多越好呀！</p>
<h2 id="预测房价-回归问题"><a class="markdownIt-Anchor" href="#预测房价-回归问题"></a> 预测房价: 回归问题</h2>
<p><a href="https://livebook.manning.com/book/deep-learning-with-python/chapter-3/271" target="_blank" rel="noopener">原文链接</a></p>
<p>前面两个例子，我们都是在做分类问题（预测离散的标签）。这次看一个回归问题（预测连续的数值）。</p>
<h3 id="波士顿房间数据集"><a class="markdownIt-Anchor" href="#波士顿房间数据集"></a> 波士顿房间数据集</h3>
<p>我们用 Boston Housing Price dataset 这个数据集来预测 70 年代中期的波士顿郊区房价。数据集里有当时那个地方的一些数据，比如犯罪律、税率什么的。</p>
<p>这个数据集比起我们前两个分类的数据集，数据相当少，只有 506 个，404 个在训练集，102 非个在测试集。数据里每种特征（feature）的输入数据数量级也不尽相同。</p>
<p>我们先把数据导进来看看（这个数据集也是 Keras 自带有的）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> boston_housing</span><br><span class="line"></span><br><span class="line">(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()</span><br><span class="line"></span><br><span class="line">print(train_data.shape, test_data.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(404, 13) (102, 13)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="number">0</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([[1.23247e+00, 0.00000e+00, 8.14000e+00, 0.00000e+00, 5.38000e-01,
        6.14200e+00, 9.17000e+01, 3.97690e+00, 4.00000e+00, 3.07000e+02,
        2.10000e+01, 3.96900e+02, 1.87200e+01],
       [2.17700e-02, 8.25000e+01, 2.03000e+00, 0.00000e+00, 4.15000e-01,
        7.61000e+00, 1.57000e+01, 6.27000e+00, 2.00000e+00, 3.48000e+02,
        1.47000e+01, 3.95380e+02, 3.11000e+00],
       [4.89822e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 6.31000e-01,
        4.97000e+00, 1.00000e+02, 1.33250e+00, 2.40000e+01, 6.66000e+02,
        2.02000e+01, 3.75520e+02, 3.26000e+00]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_targets[<span class="number">0</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([15.2, 42.3, 50. ])
</code></pre>
<p>targets 里的数据单位是 <em>千美元</em>， 这个时候的房价还比较便宜：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">min(train_targets), sum(train_targets)/len(train_targets), max(train_targets)</span><br></pre></td></tr></table></figure>
<pre><code>(5.0, 22.395049504950496, 50.0)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = range(len(train_targets))</span><br><span class="line">y = train_targets</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, <span class="string">'o'</span>, label=<span class="string">'data'</span>)</span><br><span class="line">plt.title(<span class="string">'House Prices'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'train_targets'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'prices'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggje0e4s1qj30aq07r3yo.jpg" alt="png"></p>
<h3 id="数据准备-3"><a class="markdownIt-Anchor" href="#数据准备-3"></a> 数据准备</h3>
<p>我们喂给神经网络的数据值的范围不应该差距太大，虽然神经网络是可以处理差距大的数据的，但总归不太好。<br>
对于这种差距大的数据，我们一般都会对每个特征做标准化（feature-wise normalization）。</p>
<p>具体的操作是对每个特征（输入数据矩阵的一列）减去该列的均值，再除以其标准差。<br>
这样做完之后，数据会变成以 0 为中心的，有一个标准差的（标准差为 1 ）。</p>
<p>用 Numpy 可以很容易的做这个：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mean = train_data.mean(axis=<span class="number">0</span>)</span><br><span class="line">std = train_data.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">train_data -= mean</span><br><span class="line">train_data /= std</span><br><span class="line"></span><br><span class="line">test_data -= mean</span><br><span class="line">test_data /= std</span><br></pre></td></tr></table></figure>
<p>注意哈，对测试集的处理用的是训练集的均值和标准差。</p>
<p>处理完数据就可以构建网络、训练了（标签不用处理，比分类方便）</p>
<h3 id="构建网络-2"><a class="markdownIt-Anchor" href="#构建网络-2"></a> 构建网络</h3>
<p>数据越少，越容易过拟合，要减缓过拟合，可以使用比较小的网络。</p>
<p>比如，在这个问题中，我们使用一个只有两个隐藏层的网络，每个 64 单元：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">"relu"</span>, input_shape=(train_data.shape[<span class="number">1</span>], )))</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    model.compile(optimizer=<span class="string">"rmsprop"</span>, loss=<span class="string">"mse"</span>, metrics=[<span class="string">"mae"</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>网络的最后一层只有一个单元，并且没有激活函数（所以是个线性的层）。这种层是我们做连续单值回归问题的最后一步的标配。</p>
<p>如果加了激活函数，输出的值就会有范围限制，比如 sigmoid 会把值限制到 <code>[0, 1]</code>。没有激活函数，这个线性的层输出的值就没有限制了。</p>
<p>我们在编译模型时，使用的损失函数是 <code>mse</code> （mean squared error，均方误差）。这个函数返回预测和真实目标值的差的平方。回归问题多用这个损失。</p>
<p>然后我们还用了以前没用过的训练指标 —— <code>mae</code>（mean absolute error，平均绝对误差），这个东西是预测和真实目标的差的绝对值。</p>
<h3 id="拟合验证-k-fold-验证"><a class="markdownIt-Anchor" href="#拟合验证-k-fold-验证"></a> 拟合验证 —— K-fold 验证</h3>
<p>我们之前一直在做 —— 为了对网络进行评估，来调节网络参数(比如训练的轮数)，我们将数据划分为训练集和验证集。这一次，我们也需要这么做。<br>
但有一点麻烦是，我们这次的数据太少了，所以分出来的验证集就很小（可能才有100条数据）。这种情况下，验证集选择的数据不同，可能对验证的结果有很大影响（即验证集的划分方式不同可能造成验证结果的方差很大），这种情况会影响我们对模型的验证。</p>
<p>在这种尴尬的境地，最佳实践是使用 <em>K-fold</em> 交叉验证（K折交叉验证）。</p>
<p>用 K-fold 验证，我们把数据分成 K 个部分（一般 k = 4 or 5），然后实例化 K 个独立的模型，每个用 K-1 份数据去训练，然后用剩余的一份去验证，最终模型的验证分数使用 K 个部分的平均值。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gert24t4p8j317u0mstci.jpg" alt="K-fold 验证示意图"></p>
<p>K-fold 验证的代码实现：</p>
<p>稍微改一下书上的例子，我们加一点用 TensorBoard 来可视化训练过程的代码。首先在 Jupyter Lab 笔记本里加载 tensorboard：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load the TensorBoard notebook extension</span></span><br><span class="line"><span class="comment"># TensorBoard 可以可视化训练过程</span></span><br><span class="line">%load_ext tensorboard</span><br><span class="line"><span class="comment"># Clear any logs from previous runs</span></span><br><span class="line">!rm -rf ./logs/</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>The tensorboard extension is already loaded. To reload it, use:
  %reload_ext tensorboard
</code></pre>
<p>然后开始写主要的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">num_val_samples = len(train_data) // k</span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line">all_scores = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备 TensorBoard</span></span><br><span class="line">log_dir = <span class="string">"logs/fit/"</span> + datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    print(<span class="string">f'processing fold #<span class="subst">&#123;i&#125;</span> (<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;k&#125;</span>)'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 准备验证数据</span></span><br><span class="line">    val_data = train_data[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    val_targets = train_targets[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 准备训练数据</span></span><br><span class="line">    partial_train_data = np.concatenate(</span><br><span class="line">        [train_data[: i * num_val_samples],</span><br><span class="line">         train_data[(i+<span class="number">1</span>) * num_val_samples :]],</span><br><span class="line">        axis=<span class="number">0</span>)</span><br><span class="line">    partial_train_targets = np.concatenate(</span><br><span class="line">        [train_targets[: i * num_val_samples], </span><br><span class="line">         train_targets[(i+<span class="number">1</span>) *  num_val_samples :]], </span><br><span class="line">        axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#构建、训练模型</span></span><br><span class="line">    model = build_model()</span><br><span class="line">    model.fit(partial_train_data, partial_train_targets, </span><br><span class="line">              epochs=num_epochs, batch_size=<span class="number">1</span>, verbose=<span class="number">0</span>,</span><br><span class="line">              callbacks=[tensorboard_callback])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 有验证集评估模型</span></span><br><span class="line">    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=<span class="number">0</span>)</span><br><span class="line">    all_scores.append(val_mae)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.mean(all_scores)</span><br></pre></td></tr></table></figure>
<pre><code>processing fold #0 (1/4)
processing fold #1 (2/4)
processing fold #2 (3/4)
processing fold #3 (4/4)

2.4046657
</code></pre>
<p>用下面这个命令可以在 Jupyter Lab 笔记本里显示 tensorboard：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%tensorboard --logdir logs/fit</span><br></pre></td></tr></table></figure>
<p>它大概是这样的：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggje37vwedj31fw0u0dmn.jpg" alt="tensorboard的截图"></p>
<p>这个东西也可以直接在你的浏览器里直接打开 <code>http://localhost:6006</code> 得到。</p>
<p>刚才那个只是玩一下哈，现在我们改一下，迭代 500 轮（没独显的mbp跑这个好慢啊啊啊），把训练的结果记下来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">k = <span class="number">4</span></span><br><span class="line">num_val_samples = len(train_data) // k</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">500</span></span><br><span class="line">all_mae_histories = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    print(<span class="string">f'processing fold #<span class="subst">&#123;i&#125;</span> (<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;k&#125;</span>)'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 准备验证数据</span></span><br><span class="line">    val_data = train_data[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    val_targets = train_targets[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 准备训练数据</span></span><br><span class="line">    partial_train_data = np.concatenate(</span><br><span class="line">        [train_data[: i * num_val_samples],</span><br><span class="line">         train_data[(i+<span class="number">1</span>) * num_val_samples :]],</span><br><span class="line">        axis=<span class="number">0</span>)</span><br><span class="line">    partial_train_targets = np.concatenate(</span><br><span class="line">        [train_targets[: i * num_val_samples], </span><br><span class="line">         train_targets[(i+<span class="number">1</span>) *  num_val_samples :]], </span><br><span class="line">        axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#构建、训练模型</span></span><br><span class="line">    model = build_model()</span><br><span class="line">    history = model.fit(partial_train_data, partial_train_targets,</span><br><span class="line">                        validation_data=(val_data, val_targets),</span><br><span class="line">                        epochs=num_epochs, batch_size=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    mae_history = history.history[<span class="string">'val_mae'</span>]</span><br><span class="line">    all_mae_histories.append(mae_history)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Done."</span>)</span><br></pre></td></tr></table></figure>
<pre><code>processing fold #0 (1/4)
processing fold #1 (2/4)
processing fold #2 (3/4)
processing fold #3 (4/4)
Done.
</code></pre>
<p>画图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">average_mae_history = [</span><br><span class="line">    np.mean([x[i] <span class="keyword">for</span> x <span class="keyword">in</span> all_mae_histories]) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.plot(range(<span class="number">1</span>, len(average_mae_history) + <span class="number">1</span>), average_mae_history)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Validation MAE'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggje0zqitmj30b007eq35.jpg" alt="png"></p>
<p>这个图太密了，看不清，所以要处理一下：</p>
<ul>
<li>去掉前十组数据，这端明显和其他的数量差距比较大；</li>
<li>把每个点都换成前面数据点的指数移动平均值（an exponential moving average of the previous points），把曲线变平滑；</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smooth_curve</span><span class="params">(points, factor=<span class="number">0.9</span>)</span>:</span></span><br><span class="line">  smoothed_points = []</span><br><span class="line">  <span class="keyword">for</span> point <span class="keyword">in</span> points:</span><br><span class="line">    <span class="keyword">if</span> smoothed_points:</span><br><span class="line">      previous = smoothed_points[<span class="number">-1</span>]</span><br><span class="line">      smoothed_points.append(previous * factor + point * (<span class="number">1</span> - factor))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      smoothed_points.append(point)</span><br><span class="line">  <span class="keyword">return</span> smoothed_points</span><br><span class="line"></span><br><span class="line">smooth_mae_history = smooth_curve(average_mae_history[<span class="number">10</span>:])</span><br><span class="line"></span><br><span class="line">plt.plot(range(<span class="number">1</span>, len(smooth_mae_history) + <span class="number">1</span>), smooth_mae_history)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Validation MAE'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggje11t08ij30b007egls.jpg" alt="png"></p>
<p>从这个图看，差不多过了 80 个 epochs 后就过拟合了。</p>
<p>在尝试了这些之后，我们找出最佳的参数（轮数啦，网络的层数啦，这些都可以试试），然后用最佳的参数在所有数据上训练，来得出最终的生产模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练最终模型</span></span><br><span class="line"></span><br><span class="line">model = build_model()</span><br><span class="line">model.fit(train_data, train_targets, </span><br><span class="line">          epochs=<span class="number">80</span>, batch_size=<span class="number">16</span>, verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后评估一下</span></span><br><span class="line">test_mse_score, test_mae_score = model.evaluate(test_data, test_targets, verbose=<span class="number">0</span>)</span><br><span class="line">print(test_mse_score, test_mae_score)</span><br></pre></td></tr></table></figure>
<pre><code>17.43332971311083 2.6102107
</code></pre>
<p>这个 test_mae_score 的值说明，我们训练出来的模型的预测和实际大概差了 2k+ 美元。。。😭</p>

  </div>
</article>
<!--Disqus-->


<!--Livere-->

    <div class="blog-post-comments">
        <div id="lv-container" data-id="city" data-uid="MTAyMC80NjEzMi8yMjY0Mw==">
            <noscript>不启用 JavaScript 支持的人是看不到可爱的评论区的。😥</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#deep-learning-with-python"><span class="toc-number">1.</span> <span class="toc-text"> Deep Learning with Python</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#电影评论分类二分类问题"><span class="toc-number">1.1.</span> <span class="toc-text"> 电影评论分类:二分类问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#imdb-数据集"><span class="toc-number">1.1.1.</span> <span class="toc-text"> IMDB 数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据准备"><span class="toc-number">1.1.2.</span> <span class="toc-text"> 数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#建立网络"><span class="toc-number">1.1.3.</span> <span class="toc-text"> 建立网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#激活函数的作用"><span class="toc-number">1.1.3.1.</span> <span class="toc-text"> 激活函数的作用</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编译模型"><span class="toc-number">1.1.4.</span> <span class="toc-text"> 编译模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练模型"><span class="toc-number">1.1.5.</span> <span class="toc-text"> 训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#进一步实验"><span class="toc-number">1.1.6.</span> <span class="toc-text"> 进一步实验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#新闻分类-多分类问题"><span class="toc-number">1.2.</span> <span class="toc-text"> 新闻分类: 多分类问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#路透社数据集"><span class="toc-number">1.2.1.</span> <span class="toc-text"> 路透社数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据准备-2"><span class="toc-number">1.2.2.</span> <span class="toc-text"> 数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#构建网络"><span class="toc-number">1.2.3.</span> <span class="toc-text"> 构建网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编译模型-2"><span class="toc-number">1.2.4.</span> <span class="toc-text"> 编译模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#验证效果"><span class="toc-number">1.2.5.</span> <span class="toc-text"> 验证效果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练模型-2"><span class="toc-number">1.2.6.</span> <span class="toc-text"> 训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#处理标签和损失的另一种方法"><span class="toc-number">1.2.7.</span> <span class="toc-text"> 处理标签和损失的另一种方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#中间层维度足够大的重要性"><span class="toc-number">1.2.8.</span> <span class="toc-text"> 中间层维度足够大的重要性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#尝试使用更少更多的层"><span class="toc-number">1.2.9.</span> <span class="toc-text"> 尝试使用更少/更多的层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#预测房价-回归问题"><span class="toc-number">1.3.</span> <span class="toc-text"> 预测房价: 回归问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#波士顿房间数据集"><span class="toc-number">1.3.1.</span> <span class="toc-text"> 波士顿房间数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据准备-3"><span class="toc-number">1.3.2.</span> <span class="toc-text"> 数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#构建网络-2"><span class="toc-number">1.3.3.</span> <span class="toc-text"> 构建网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#拟合验证-k-fold-验证"><span class="toc-number">1.3.4.</span> <span class="toc-text"> 拟合验证 —— K-fold 验证</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&text=Python深度学习之神经网络入门"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&title=Python深度学习之神经网络入门"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&is_video=false&description=Python深度学习之神经网络入门"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Python深度学习之神经网络入门&body=Check out this article: https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&title=Python深度学习之神经网络入门"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&title=Python深度学习之神经网络入门"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&title=Python深度学习之神经网络入门"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&title=Python深度学习之神经网络入门"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2020/07/08/DeepLearningWithPython/Deep-Learning with-Python-ch3/&name=Python深度学习之神经网络入门&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2020 CDFMLR
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<!-- clipboard -->

  <script src="/lib/clipboard/clipboard.min.js"></script>
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功!");
      e.clearSelection();
    })
  })
  </script>

<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-146911386-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?9a0d2e6fde93dad496ac79f04f3aba97";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->


<!--Livere Comments-->

    <script type="text/javascript">
      (function (d, s) {
        var j, e = d.getElementsByTagName(s)[0];

        if (typeof LivereTower === 'function') { return; }

        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;

        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>

</body>
</html>
