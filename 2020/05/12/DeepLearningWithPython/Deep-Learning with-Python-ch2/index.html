<!DOCTYPE html>
<html lang=zh>
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Deep Learning with Python 这篇文章是我学习《Deep Learning with Python》(第二版，François Chollet 著) 时写的系列笔记之一。文章的内容是从  Jupyter notebooks 转成 Markdown 的，当我完成所以文章后，会在 GitHub 发布我写的所有  Jupyter notebooks。 你可以在这个网址在线阅读这本">
<meta name="keywords" content="Machine Learning,Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Python深度学习之初窥神经网络">
<meta property="og:url" content="https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/index.html">
<meta property="og:site_name" content="clownote">
<meta property="og:description" content="Deep Learning with Python 这篇文章是我学习《Deep Learning with Python》(第二版，François Chollet 著) 时写的系列笔记之一。文章的内容是从  Jupyter notebooks 转成 Markdown 的，当我完成所以文章后，会在 GitHub 发布我写的所有  Jupyter notebooks。 你可以在这个网址在线阅读这本">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gepeqfrnx9j3073070747.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gepeimx6pwj3073070a9v.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gepeiqkaq6j3073070jr7.jpg">
<meta property="og:updated_time" content="2020-08-13T03:55:33.704Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Python深度学习之初窥神经网络">
<meta name="twitter:description" content="Deep Learning with Python 这篇文章是我学习《Deep Learning with Python》(第二版，François Chollet 著) 时写的系列笔记之一。文章的内容是从  Jupyter notebooks 转成 Markdown 的，当我完成所以文章后，会在 GitHub 发布我写的所有  Jupyter notebooks。 你可以在这个网址在线阅读这本">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gepeqfrnx9j3073070747.jpg">
    
    
        
          
              <link rel="shortcut icon" href="/images/rabbit.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/rabbit_192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/rabbit_180.png">
          
        
    
    <!-- title -->
    <title>Python深度学习之初窥神经网络</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
    <!--Google search varification (PRIVATE)-->
    <meta name="google-site-verification" content="MrqlpFAD8nDanw3Ypv7ZsIWHLnTdhRuLa4QhSVwxIvc">
    <!--Google AdSense 关联 (PRIVATE)-->
    <script data-ad-client="ca-pub-1510963483941114" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2020/05/13/School/Leetcode-course-tab-I-II-III/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2020/05/07/blog/pip-install-yourselfs-packages/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&text=Python深度学习之初窥神经网络"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&title=Python深度学习之初窥神经网络"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&is_video=false&description=Python深度学习之初窥神经网络"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Python深度学习之初窥神经网络&body=Check out this article: https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&title=Python深度学习之初窥神经网络"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&title=Python深度学习之初窥神经网络"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&title=Python深度学习之初窥神经网络"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&title=Python深度学习之初窥神经网络"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&name=Python深度学习之初窥神经网络&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#deep-learning-with-python"><span class="toc-number">1.</span> <span class="toc-text"> Deep Learning with Python</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#初窥神经网络"><span class="toc-number">1.1.</span> <span class="toc-text"> 初窥神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#导入mnist数据集"><span class="toc-number">1.1.1.</span> <span class="toc-text"> 导入MNIST数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#网络构建"><span class="toc-number">1.1.2.</span> <span class="toc-text"> 网络构建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编译"><span class="toc-number">1.1.3.</span> <span class="toc-text"> 编译</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#预处理"><span class="toc-number">1.1.4.</span> <span class="toc-text"> 预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#图形处理"><span class="toc-number">1.1.4.1.</span> <span class="toc-text"> 图形处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#标签处理"><span class="toc-number">1.1.4.2.</span> <span class="toc-text"> 标签处理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练网络"><span class="toc-number">1.1.5.</span> <span class="toc-text"> 训练网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络的数据表示"><span class="toc-number">1.2.</span> <span class="toc-text"> 神经网络的数据表示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#认识张量"><span class="toc-number">1.2.1.</span> <span class="toc-text"> 认识张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#标量-0d-tensors"><span class="toc-number">1.2.1.1.</span> <span class="toc-text"> 标量 (0D Tensors)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#向量-1d-tensors"><span class="toc-number">1.2.1.2.</span> <span class="toc-text"> 向量 (1D Tensors)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#矩阵-2d-tensors"><span class="toc-number">1.2.1.3.</span> <span class="toc-text"> 矩阵 (2D Tensors)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#高阶张量"><span class="toc-number">1.2.1.4.</span> <span class="toc-text"> 高阶张量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#张量的三要素"><span class="toc-number">1.2.2.</span> <span class="toc-text"> 张量的三要素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#numpy张量操作"><span class="toc-number">1.2.3.</span> <span class="toc-text"> Numpy张量操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#张量切片"><span class="toc-number">1.2.3.1.</span> <span class="toc-text"> 张量切片：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#数据批量"><span class="toc-number">1.2.3.2.</span> <span class="toc-text"> 数据批量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#常见数据张量表示"><span class="toc-number">1.2.4.</span> <span class="toc-text"> 常见数据张量表示</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络的齿轮-张量运算"><span class="toc-number">1.3.</span> <span class="toc-text"> 神经网络的“齿轮”: 张量运算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#逐元素操作element-wise"><span class="toc-number">1.3.1.</span> <span class="toc-text"> 逐元素操作(Element-wise)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#广播broadcasting"><span class="toc-number">1.3.2.</span> <span class="toc-text"> 广播(Broadcasting)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#张量点积dot"><span class="toc-number">1.3.3.</span> <span class="toc-text"> 张量点积(dot)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#张量变形reshaping"><span class="toc-number">1.3.4.</span> <span class="toc-text"> 张量变形(reshaping)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络的引擎-基于梯度的优化"><span class="toc-number">1.4.</span> <span class="toc-text"> 神经网络的“引擎”: 基于梯度的优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#导数derivative"><span class="toc-number">1.4.1.</span> <span class="toc-text"> 导数(derivative)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度gradient"><span class="toc-number">1.4.2.</span> <span class="toc-text"> 梯度(gradient)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#随机梯度下降stochastic-gradient-descent"><span class="toc-number">1.4.3.</span> <span class="toc-text"> 随机梯度下降(Stochastic gradient descent)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#反向传播算法链式求导"><span class="toc-number">1.4.4.</span> <span class="toc-text"> 反向传播算法：链式求导</span></a></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Python深度学习之初窥神经网络
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">clownote</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2020-05-12T09:09:44.000Z" itemprop="datePublished">2020-05-12</time>
        
        (Updated: <time datetime="2020-08-13T03:55:33.704Z" itemprop="dateModified">2020-08-13</time>)
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a> › <a class="category-link" href="/categories/Machine-Learning/Deep-Learning-with-Python/">Deep Learning with Python</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a>, <a class="tag-link" href="/tags/Machine-Learning/">Machine Learning</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="deep-learning-with-python"><a class="markdownIt-Anchor" href="#deep-learning-with-python"></a> Deep Learning with Python</h1>
<p>这篇文章是我学习《Deep Learning with Python》(第二版，François Chollet 著) 时写的系列笔记之一。文章的内容是从  Jupyter notebooks 转成 Markdown 的，当我完成所以文章后，会在 GitHub 发布我写的所有  Jupyter notebooks。</p>
<p>你可以在这个网址在线阅读这本书的正版原文(英文)：<a href="https://livebook.manning.com/book/deep-learning-with-python" target="_blank" rel="noopener">https://livebook.manning.com/book/deep-learning-with-python</a></p>
<p>这本书的作者也给出了一套 Jupyter notebooks：<a href="https://github.com/fchollet/deep-learning-with-python-notebooks" target="_blank" rel="noopener">https://github.com/fchollet/deep-learning-with-python-notebooks</a></p>
<hr>
<p>本文为 <strong>第2章 开始之前：神经网络背后的数学</strong> (Chapter 2. Before we begin: the mathematical building blocks of neural networks) 的笔记整合。</p>
<p>本文目录：</p>
<p>[TOC]</p>
<h2 id="初窥神经网络"><a class="markdownIt-Anchor" href="#初窥神经网络"></a> 初窥神经网络</h2>
<p>学编程语言从 “Hello World” 开始，学 Deep learning 从 <code>MINST</code> 开始。</p>
<p>MNIST 用来训练手写数字识别， 它包含 28x28 的灰度手写图片，以及每张图片对应的标签(0~9的值)。</p>
<h3 id="导入mnist数据集"><a class="markdownIt-Anchor" href="#导入mnist数据集"></a> 导入MNIST数据集</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading the MNIST dataset in Keras</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br></pre></td></tr></table></figure>
<p>看一下训练集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(train_images.shape)</span><br><span class="line">print(train_labels.shape)</span><br><span class="line">train_labels</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>(60000, 28, 28)
(60000,)

array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)
</code></pre>
<p>这是测试集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(test_images.shape)</span><br><span class="line">print(test_labels.shape)</span><br><span class="line">test_labels</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>(10000, 28, 28)
(10000,)

array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)
</code></pre>
<h3 id="网络构建"><a class="markdownIt-Anchor" href="#网络构建"></a> 网络构建</h3>
<p>我们来构建一个用来学习 MNIST 集的神经网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">network = models.Sequential()</span><br><span class="line">network.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span> * <span class="number">28</span>, )))</span><br><span class="line">network.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br></pre></td></tr></table></figure>
<p>神经网络是一个个「层」组成的。<br>
一个「层」就像是一个“蒸馏过滤器”，它会“过滤”处理输入的数据，从里面“精炼”出需要的信息，然后传到下一层。</p>
<p>这样一系列的「层」组合起来，像流水线一样对数据进行处理。<br>
层层扬弃，让被处理的数据，或者说“数据的表示”对我们最终希望的结果越来越“有用”。</p>
<p>我们刚才这段代码构建的网络包含两个「Dense 层」，这么叫是因为它们是密集连接（densely connected）或者说是 <em>全连接</em> 的。</p>
<p>数据到了最后一层（第二层），是一个 <strong>10路</strong> 的 softmax 层。<br>
这个层输出的是一个数组，包含 10 个概率值（它们的和为1），这个输出「表示」的信息就对我们预测图片对应的数字相当有用了。<br>
事实上这输出中的每一个概率值就分别代表输入图片属于10个数字（0～9）中的一个的概率！</p>
<h3 id="编译"><a class="markdownIt-Anchor" href="#编译"></a> 编译</h3>
<p>接下来，我们要 <em>编译</em> 这个网络，这个步骤需要给3个参数：</p>
<ul>
<li>损失函数：评价你这网络表现的好不好的函数</li>
<li>优化器：怎么更新（优化）你这个网络</li>
<li>训练和测试过程中需要监控的指标，比如这个例子里，我们只关心一个指标 —— 预测的精度</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">network.compile(loss=<span class="string">"categorical_crossentropy"</span>,</span><br><span class="line">                optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">                metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<h3 id="预处理"><a class="markdownIt-Anchor" href="#预处理"></a> 预处理</h3>
<h4 id="图形处理"><a class="markdownIt-Anchor" href="#图形处理"></a> 图形处理</h4>
<p>我们还需要处理一下图形数据，把它变成我们的网络认识的样子。</p>
<p>MNIST 数据集里的图片是 28x28 的，每个值是属于 [0, 255] 的 uint8。<br>
而我们的神经网络想要的是 28x28 的在 [0, 1] 中的 float32。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_images = train_images.reshape((<span class="number">60000</span>, <span class="number">28</span> * <span class="number">28</span>))</span><br><span class="line">train_images = train_images.astype(<span class="string">'float32'</span>) / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">test_images = test_images.reshape((<span class="number">10000</span>, <span class="number">28</span> * <span class="number">28</span>))</span><br><span class="line">test_images = test_images.astype(<span class="string">'float32'</span>) / <span class="number">255</span></span><br></pre></td></tr></table></figure>
<h4 id="标签处理"><a class="markdownIt-Anchor" href="#标签处理"></a> 标签处理</h4>
<p>同样，标签也是需要处理一下的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"></span><br><span class="line">train_labels = to_categorical(train_labels)</span><br><span class="line">test_labels = to_categorical(test_labels)</span><br></pre></td></tr></table></figure>
<h3 id="训练网络"><a class="markdownIt-Anchor" href="#训练网络"></a> 训练网络</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">network.fit(train_images, train_labels, epochs=<span class="number">5</span>, batch_size=<span class="number">128</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>Train on 60000 samples
Epoch 1/5
60000/60000 [==============================] - 3s 49us/sample - loss: 0.2549 - accuracy: 0.9254
Epoch 2/5
60000/60000 [==============================] - 2s 38us/sample - loss: 0.1025 - accuracy: 0.9693
Epoch 3/5
60000/60000 [==============================] - 2s 35us/sample - loss: 0.0676 - accuracy: 0.9800
Epoch 4/5
60000/60000 [==============================] - 2s 37us/sample - loss: 0.0491 - accuracy: 0.9848
Epoch 5/5
60000/60000 [==============================] - 2s 42us/sample - loss: 0.0369 - accuracy: 0.9888

&lt;tensorflow.python.keras.callbacks.History at 0x13a7892d0&gt;
</code></pre>
<p>可以看到，训练很快，一会儿就对训练集有 98%+ 的精度了。</p>
<p>再用测试集去试试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_loss, test_acc = network.evaluate(test_images, test_labels, verbose=<span class="number">2</span>)    <span class="comment"># verbose=2 to avoid a looooong progress bar that fills the screen with '='. https://github.com/tensorflow/tensorflow/issues/32286</span></span><br><span class="line">print(<span class="string">'test_acc:'</span>, test_acc)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>10000/1 - 0s - loss: 0.0362 - accuracy: 0.9789
test_acc: 0.9789
</code></pre>
<p>我们训练好的网络在测试集下的表现并没有之前在训练集中那么好，这是「过拟合」的锅。</p>
<h2 id="神经网络的数据表示"><a class="markdownIt-Anchor" href="#神经网络的数据表示"></a> 神经网络的数据表示</h2>
<p>Tensor，张量，任意维的数组（我的意思是编程的那种数组）。矩阵是二维的张量。</p>
<p>我们常把「张量的维度」说成「轴」。</p>
<h3 id="认识张量"><a class="markdownIt-Anchor" href="#认识张量"></a> 认识张量</h3>
<h4 id="标量-0d-tensors"><a class="markdownIt-Anchor" href="#标量-0d-tensors"></a> 标量 (0D Tensors)</h4>
<p>Scalars，标量是 0 维的张量（0个轴），包含一个数。</p>
<p>标量在 numpy 中可以用 float32 或 float64 表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array(<span class="number">12</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>array(12)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.ndim    <span class="comment"># 轴数（维数）</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>1
</code></pre>
<h4 id="向量-1d-tensors"><a class="markdownIt-Anchor" href="#向量-1d-tensors"></a> 向量 (1D Tensors)</h4>
<p>Vectors，向量是 1 维张量（有1个轴），包含一列标量（就是搞个array装标量）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">x</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>array([1, 2, 3, 4, 5])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.ndim</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>1
</code></pre>
<p>我们把这样有5个元素的向量叫做“5维向量”。<br>
但注意<strong>5D向量</strong>可不是<strong>5D张量</strong>！</p>
<ul>
<li>5D向量：只有1个轴，在这个轴上有5个维度。</li>
<li>5D张量：有5个轴，在每个轴上可以有任意维度。</li>
</ul>
<p>这个就很迷，这“维度”有的时候是指轴数，有的时候是指轴上的元素个数。</p>
<p>所以，我们最好换种说法，用「阶」来表示轴数，说 <strong>5阶张量</strong>。</p>
<h4 id="矩阵-2d-tensors"><a class="markdownIt-Anchor" href="#矩阵-2d-tensors"></a> 矩阵 (2D Tensors)</h4>
<p>Matrices，矩阵是 2 阶张量（2个轴，就是我们说的「行」和「列」），包含一列向量（就是搞个array装向量）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">5</span>, <span class="number">78</span>, <span class="number">2</span>, <span class="number">34</span>, <span class="number">0</span>],</span><br><span class="line">              [<span class="number">6</span>, <span class="number">79</span>, <span class="number">3</span>, <span class="number">35</span>, <span class="number">1</span>],</span><br><span class="line">              [<span class="number">7</span>, <span class="number">80</span>, <span class="number">4</span>, <span class="number">36</span>, <span class="number">2</span>]])</span><br><span class="line">x</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>array([[ 5, 78,  2, 34,  0],
       [ 6, 79,  3, 35,  1],
       [ 7, 80,  4, 36,  2]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.ndim</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>2
</code></pre>
<h4 id="高阶张量"><a class="markdownIt-Anchor" href="#高阶张量"></a> 高阶张量</h4>
<p>你搞个装矩阵的 array 就得到了3阶张量。</p>
<p>再搞个装3阶张量的 array 就得到了4阶张量，依次类推，就有高阶张量了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[[<span class="number">5</span>, <span class="number">78</span>, <span class="number">2</span>, <span class="number">34</span>, <span class="number">0</span>],</span><br><span class="line">               [<span class="number">6</span>, <span class="number">79</span>, <span class="number">3</span>, <span class="number">35</span>, <span class="number">1</span>],</span><br><span class="line">               [<span class="number">7</span>, <span class="number">80</span>, <span class="number">4</span>, <span class="number">36</span>, <span class="number">2</span>]],</span><br><span class="line">              [[<span class="number">5</span>, <span class="number">78</span>, <span class="number">2</span>, <span class="number">34</span>, <span class="number">0</span>],</span><br><span class="line">               [<span class="number">6</span>, <span class="number">79</span>, <span class="number">3</span>, <span class="number">35</span>, <span class="number">1</span>],</span><br><span class="line">               [<span class="number">7</span>, <span class="number">80</span>, <span class="number">4</span>, <span class="number">36</span>, <span class="number">2</span>]],</span><br><span class="line">              [[<span class="number">5</span>, <span class="number">78</span>, <span class="number">2</span>, <span class="number">34</span>, <span class="number">0</span>],</span><br><span class="line">               [<span class="number">6</span>, <span class="number">79</span>, <span class="number">3</span>, <span class="number">35</span>, <span class="number">1</span>],</span><br><span class="line">               [<span class="number">7</span>, <span class="number">80</span>, <span class="number">4</span>, <span class="number">36</span>, <span class="number">2</span>]]])</span><br><span class="line">x.ndim</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>3
</code></pre>
<p>深度学习里，我们一般就用0～4阶的张量。</p>
<h3 id="张量的三要素"><a class="markdownIt-Anchor" href="#张量的三要素"></a> 张量的三要素</h3>
<ul>
<li>阶数（轴的个数）：3，5，…</li>
<li>形状（各轴维数）：(2, 1, 3)，(6, 5, 5, 3, 6)，…</li>
<li>数据类型：float32，uint8，…</li>
</ul>
<p>我们来看看 MNIST 里的张量数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br><span class="line"></span><br><span class="line">print(train_images.ndim)</span><br><span class="line">print(train_images.shape)</span><br><span class="line">print(train_images.dtype)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>3
(60000, 28, 28)
uint8
</code></pre>
<p>所以 train_images 是个8位无符号整数的3阶张量。</p>
<p>打印个里面的图片看看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">digit = train_images[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">print(<span class="string">"image:"</span>)</span><br><span class="line">plt.imshow(digit, cmap=plt.cm.binary)</span><br><span class="line">plt.show()</span><br><span class="line">print(<span class="string">"label: "</span>, train_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gepeqfrnx9j3073070747.jpg" alt="一张图片，显示了一个位于图像中央的数字5"></p>
<pre><code>label:  5
</code></pre>
<h3 id="numpy张量操作"><a class="markdownIt-Anchor" href="#numpy张量操作"></a> Numpy张量操作</h3>
<h4 id="张量切片"><a class="markdownIt-Anchor" href="#张量切片"></a> 张量切片：</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my_slice = train_images[<span class="number">10</span>:<span class="number">100</span>]</span><br><span class="line">print(my_slice.shape)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>(90, 28, 28)
</code></pre>
<p>等价于：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my_slice = train_images[<span class="number">10</span>:<span class="number">100</span>, :, :]</span><br><span class="line">print(my_slice.shape)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>(90, 28, 28)
</code></pre>
<p>也等价于</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my_slice = train_images[<span class="number">10</span>:<span class="number">100</span>, <span class="number">0</span>:<span class="number">28</span>, <span class="number">0</span>:<span class="number">28</span>]</span><br><span class="line">print(my_slice.shape)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>(90, 28, 28)
</code></pre>
<p>选出 <strong>右下角</strong> 14x14 的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_slice = train_images[:, <span class="number">14</span>:, <span class="number">14</span>:]</span><br><span class="line">plt.imshow(my_slice[<span class="number">0</span>], cmap=plt.cm.binary)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gepeimx6pwj3073070a9v.jpg" alt="一张图片，在左上角有数字5的一部分"></p>
<p>选出 中心处 14x14 的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_slice = train_images[:, <span class="number">7</span>:<span class="number">-7</span>, <span class="number">7</span>:<span class="number">-7</span>]</span><br><span class="line">plt.imshow(my_slice[<span class="number">0</span>], cmap=plt.cm.binary)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gepeiqkaq6j3073070jr7.jpg" alt="一张图片，显示了一个数字5，占满了整个图片"></p>
<h4 id="数据批量"><a class="markdownIt-Anchor" href="#数据批量"></a> 数据批量</h4>
<p>深度学习的数据里，一般第一个轴（index=0）叫做「样本轴」（或者说「样本维度」）。</p>
<p>深度学习里，我们一般不会一次性处理整个数据集，我们一批一批地处理。</p>
<p>在 MNIST 中，我们的一个批量是 128 个数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一批</span></span><br><span class="line">batch = train_images[:<span class="number">128</span>]</span><br><span class="line"><span class="comment"># 第二批</span></span><br><span class="line">batch = train_images[<span class="number">128</span>:<span class="number">256</span>]</span><br><span class="line"><span class="comment"># 第n批</span></span><br><span class="line">n = <span class="number">12</span></span><br><span class="line">batch = train_images[<span class="number">128</span> * n : <span class="number">128</span> * (n+<span class="number">1</span>)]</span><br></pre></td></tr></table></figure>
<p>所以，在使用 batch 的时候，我们也把第一个轴叫做「批量轴」。</p>
<h3 id="常见数据张量表示"><a class="markdownIt-Anchor" href="#常见数据张量表示"></a> 常见数据张量表示</h3>
<table>
<thead>
<tr>
<th style="text-align:center">数据</th>
<th style="text-align:center">张量维数</th>
<th style="text-align:left">形状</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">向量数据</td>
<td style="text-align:center">2D</td>
<td style="text-align:left">(samples,features)</td>
</tr>
<tr>
<td style="text-align:center">时间序列</td>
<td style="text-align:center">3D</td>
<td style="text-align:left">(samples, timesteps, features)</td>
</tr>
<tr>
<td style="text-align:center">图像</td>
<td style="text-align:center">4D</td>
<td style="text-align:left">(samples, height, width, channels) 或 (samples, channels, height, width)</td>
</tr>
<tr>
<td style="text-align:center">视频</td>
<td style="text-align:center">5D</td>
<td style="text-align:left">(samples, frames, height, width, channels) 或 (samples, frames, channels, height, width)</td>
</tr>
</tbody>
</table>
<h2 id="神经网络的齿轮-张量运算"><a class="markdownIt-Anchor" href="#神经网络的齿轮-张量运算"></a> 神经网络的“齿轮”: 张量运算</h2>
<p>在我们的第一个神经网络例子中(MNIST)，我们的每一层其实都是对输入数据做了类似如下的运算：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = relu(dot(W, input) + b)</span><br></pre></td></tr></table></figure>
<p>input 是输入，<br>
W 和 b 是层的属性，<br>
output 是输出。</p>
<p>这些东西之间做了 relu、dot、add 运算，<br>
接下来我们会解释这些运算。</p>
<h3 id="逐元素操作element-wise"><a class="markdownIt-Anchor" href="#逐元素操作element-wise"></a> 逐元素操作(Element-wise)</h3>
<p>Element-wise 的操作，就是分别对张量中的每一个元素作用。<br>
比如，我们实现一个简单的 <code>relu</code> （<code>relu(x) = max(x, 0)</code>）:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">2</span>    <span class="comment"># x is a 2D Numpy tensor.</span></span><br><span class="line">    x = x.copy()    <span class="comment"># Avoid overwriting the input tensor.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">            x[i, j] = max(x[i, j], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>加法也是逐元素操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_add</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="comment"># assert x and y are 2D Numpy tensors and have the same shape.</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">2</span></span><br><span class="line">    <span class="keyword">assert</span> x.shape == y.shape</span><br><span class="line">    </span><br><span class="line">    x = x.copy()    <span class="comment"># Avoid overwriting the input tensor.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">            x[i, j] += y[i, j]</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>在 Numpy 里，这些都写好了。 具体的运算是交给 C 或 Fortran 写的 BLAS 进行的，速度很高。</p>
<p>你可以这样查看有没有装 BLAS：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.show_config()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
</code></pre>
<p>下面是如何使用 numpy 的逐元素 relu、add：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">              [<span class="number">-1</span>, <span class="number">2</span>, <span class="number">-3</span>],</span><br><span class="line">              [<span class="number">3</span>, <span class="number">-1</span>, <span class="number">4</span>]])</span><br><span class="line">b = np.array([[<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], </span><br><span class="line">              [<span class="number">-2</span>, <span class="number">-3</span>, <span class="number">1</span>], </span><br><span class="line">              [<span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line">c = a + b    <span class="comment"># Element-wise addition</span></span><br><span class="line">d = np.maximum(c, <span class="number">0</span>)    <span class="comment"># Element-wise relu</span></span><br><span class="line"></span><br><span class="line">print(c)</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>[[ 7  9 11]
 [-3 -1 -2]
 [ 4 -1  8]]
[[ 7  9 11]
 [ 0  0  0]
 [ 4  0  8]]
</code></pre>
<h3 id="广播broadcasting"><a class="markdownIt-Anchor" href="#广播broadcasting"></a> 广播(Broadcasting)</h3>
<p>当进行逐元素运算时，如果两个张量的形状不同，在可行的情况下，较小的张量会「广播」成和较大的张量一样的形状。</p>
<p>具体来说，可以通过广播，对形状为 <code>(a, b, ..., n, n+1, ..., m)</code> 和 <code>(n, n+1, ..., m)</code> 的两个张量进行逐元素运算。</p>
<p>比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.random((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">10</span>))    <span class="comment"># x is a random tensor with shape (64, 3, 32, 10).</span></span><br><span class="line">y = np.random.random((<span class="number">32</span>, <span class="number">10</span>))    <span class="comment"># y is a random tensor with shape (32, 10).</span></span><br><span class="line">z = np.maximum(x, y)    <span class="comment"># The output z has shape (64, 3, 32, 10) like x.</span></span><br></pre></td></tr></table></figure>
<p>广播的操作如下：</p>
<ol>
<li>小张量增加轴（广播轴），加到和大的一样（ndim）</li>
<li>小张量的元素在新轴上重复，加到和大的一样（shape）</li>
</ol>
<p>E.g.</p>
<pre><code>x: (32, 10), y: (10,)
Step 1: add an empty first axis to y: Y -&gt; (1, 10)
Step 2: repeat y 32 times alongside this new axis: Y -&gt; (32, 10)
</code></pre>
<p>在完成后，有 <code>Y[i, :] == y for i in range(0, 32)</code></p>
<p>当然，在实际的实现里，我们不这样去复制，这样太浪费空间了，<br>
我们是直接在算法里实现这个“复制的”。<br>
比如，我们实现一个简单的向量和矩阵相加：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_add_matrix_and_vector</span><span class="params">(m, v)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(m.shape) == <span class="number">2</span>    <span class="comment"># m is a 2D Numpy tensor.</span></span><br><span class="line">    <span class="keyword">assert</span> len(v.shape) == <span class="number">1</span>    <span class="comment"># v is a Numpy vector.</span></span><br><span class="line">    <span class="keyword">assert</span> m.shape[<span class="number">1</span>] == v.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    m = m.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m.shape[<span class="number">1</span>]):</span><br><span class="line">            m[i, j] += v[j]</span><br><span class="line">    <span class="keyword">return</span> m</span><br><span class="line"></span><br><span class="line">naive_add_matrix_and_vector(np.array([[<span class="number">1</span> ,<span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]), </span><br><span class="line">                            np.array([<span class="number">1</span>, <span class="number">-1</span>, <span class="number">100</span>]))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>array([[  2,   1, 103],
       [  5,   4, 106],
       [  8,   7, 109]])
</code></pre>
<h3 id="张量点积dot"><a class="markdownIt-Anchor" href="#张量点积dot"></a> 张量点积(dot)</h3>
<p>张量点积，或者叫张量乘积，在 numpy 里用 <code>dot(x, y)</code> 完成。</p>
<p>点积的操作可以从如下的简单程序中看出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向量点积</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_vector_dot</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> len(y.shape) == <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> x.shape[<span class="number">0</span>] == y.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    z = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        z += x[i] * y[i]</span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵与向量点积</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_matrix_vector_dot</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    z = np.zeros(x.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        z[i] = naive_vector_dot(x[i, :], y)</span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵点积</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_matrix_dot</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">2</span></span><br><span class="line">    <span class="keyword">assert</span> len(y.shape) == <span class="number">2</span></span><br><span class="line">    <span class="keyword">assert</span> x.shape[<span class="number">1</span>] == y.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    z = np.zeros((x.shape[<span class="number">0</span>], y.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(y.shape[<span class="number">1</span>]):</span><br><span class="line">            row_x = x[i, :]</span><br><span class="line">            column_y = y[:, j]</span><br><span class="line">            z[i, j] = naive_vector_dot(row_x, column_y)</span><br><span class="line">    <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">              [<span class="number">-1</span>, <span class="number">2</span>, <span class="number">-3</span>],</span><br><span class="line">              [<span class="number">3</span>, <span class="number">-1</span>, <span class="number">4</span>]])</span><br><span class="line">b = np.array([[<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], </span><br><span class="line">              [<span class="number">-2</span>, <span class="number">-3</span>, <span class="number">1</span>], </span><br><span class="line">              [<span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>]])</span><br><span class="line">naive_matrix_dot(a, b)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>array([[  5.,   1.,  22.],
       [-13., -13., -18.],
       [ 24.,  24.,  39.]])
</code></pre>
<p>对于高维的张量点积，其实也是一样的。<br>
例如，(这说的是shape哈)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(a, b, c, d) . (d,) -&gt; (a, b, c)</span><br><span class="line">(a, b, c, d) . (d, e) -&gt; (a, b, c, e)</span><br></pre></td></tr></table></figure>
<h3 id="张量变形reshaping"><a class="markdownIt-Anchor" href="#张量变形reshaping"></a> 张量变形(reshaping)</h3>
<p>这个操作，简言之就是，，，还是那些元素，只是排列的方式变了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">              [<span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">              [<span class="number">4.</span>, <span class="number">5.</span>]])</span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>(3, 2)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.reshape((<span class="number">6</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>array([[0.],
       [1.],
       [2.],
       [3.],
       [4.],
       [5.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.reshape((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>array([[0., 1., 2.],
       [3., 4., 5.]])
</code></pre>
<p>「转置」(transposition) 是一种特殊的矩阵变形，<br>
转置就是行列互换。</p>
<p>原来的 <code>x[i, :]</code>，转置后就成了 <code>x[:, i]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = np.zeros((<span class="number">300</span>, <span class="number">20</span>))</span><br><span class="line">y = np.transpose(x)</span><br><span class="line">print(y.shape)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>(20, 300)
</code></pre>
<h2 id="神经网络的引擎-基于梯度的优化"><a class="markdownIt-Anchor" href="#神经网络的引擎-基于梯度的优化"></a> 神经网络的“引擎”: 基于梯度的优化</h2>
<p>再看一次我们的第一个神经网络例子中(MNIST)，每一层对输入数据做的运算：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = relu(dot(W, input) + b)</span><br></pre></td></tr></table></figure>
<p>这个式子里：W 和 b 是层的属性（权重，或着说可训练参数）。<br>
具体来说，</p>
<ul>
<li><code>W</code> 是 kernel 属性；</li>
<li><code>b</code> 是 bias 属性。</li>
</ul>
<p>这些「权重」就是神经网络从数据中学习到的东西。</p>
<p>一开始，这些权重被随机初始化成一些较小的值。然后从这次随机的输出开始，反馈调节，逐步改善。</p>
<p>这个改善的过程是在「训练循环」中完成的，只有必要，这个循环可以一直进行下去：</p>
<ol>
<li>抽取一批训练数据 x 以及对应的 y</li>
<li>向前传播，得到 x 经过网络算出来的预测 y_pred</li>
<li>通过 y_pred 与 y，计算出损失</li>
<li>通过某种方式调整参数，减小损失</li>
</ol>
<p>前三步都比较简单，第4步更新参数比较复杂，一种比较有效、可行的办法就是利用可微性，通过计算梯度，向梯度的反方向移动参数。</p>
<h3 id="导数derivative"><a class="markdownIt-Anchor" href="#导数derivative"></a> 导数(derivative)</h3>
<p>这一节解释了导数的定义。</p>
<p>(直接去看书吧。)</p>
<p>知道了导数，那要更新 x 来最小化一个函数 <code>f(x)</code>，其实只需将 x 向导数的反方向移动。</p>
<h3 id="梯度gradient"><a class="markdownIt-Anchor" href="#梯度gradient"></a> 梯度(gradient)</h3>
<p>「梯度」是张量运算的导数。或者说「梯度」是「导数」在多元函数上的推广。<br>
某点的梯度代表的是该点的曲率。</p>
<p>考虑:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred = dot(W, x)</span><br><span class="line">loss_value = loss(y_pred, y)</span><br></pre></td></tr></table></figure>
<p>若固定 x 和 y，则 loss_value 将是一个 W 的函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_value = f(W)</span><br></pre></td></tr></table></figure>
<p>设当前点为 <code>W0</code>，<br>
则 f 在 W0 的导数(梯度)记为 <code>gradient(f)(W0)</code>，<br>
这个梯度值与 W 同型。<br>
其中每个元素 <code>gradient(f) (W0)[i, j]</code> 代表改变 <code>W0[i, j]</code> 时，f 的变化方向及大小。</p>
<p>所以，要改变 W 的值来实现 <code>min f</code>，就可以向梯度的反方向（即<strong>梯度下降</strong>的方向）移动：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W1 = W0 - step * gradient(f)(W0)</span><br></pre></td></tr></table></figure>
<h3 id="随机梯度下降stochastic-gradient-descent"><a class="markdownIt-Anchor" href="#随机梯度下降stochastic-gradient-descent"></a> 随机梯度下降(Stochastic gradient descent)</h3>
<p>理论上，给定一个可微函数，其最小值一定在导数为0的点中取到。所以我们只有求出所有导数为0的点，比较其函数值，就可以得到最小值。</p>
<p>这个方法放到神经网络中，就需要解一个关于 <code>W</code> 的方程 <code>gradient(f)(W) = 0</code>，这是个 N 元方程（N=神经网络中参数个数），而实际上N一般不会少于1k，这使得解这个方程变得几乎不可行。</p>
<p>所以面对这个问题，我们利用上面介绍的4步法，其中第四步使用梯度下降，逐步往梯度的反方向更新参数，一小步一小步地朝减小损失的方向前进：</p>
<ol>
<li>抽取一批训练数据 x 以及对应的 y</li>
<li>向前传播，得到 x 经过网络算出来的预测 y_pred</li>
<li>通过 y_pred 与 y，计算出损失</li>
<li>通过某种方式调整参数，减小损失
<ol>
<li>向后传播，计算损失函数关于网络参数的梯度</li>
<li>朝梯度的反方向稍微移动参数即可减小损失（W -= step * gradient）</li>
</ol>
</li>
</ol>
<p>这个方法叫做「小批量随机梯度下降」（mini-batch stochastic gradient descent，mini-batch SGD）。<br>
随机一词是指我们在第1步抽取数据是随机抽取的。</p>
<p>有一些变种的 SGD 不只看当前梯度就更新值了，它们还要看上一次的权重更新。这些变体被称作「优化方法(optimization method)」或者「优化器(optimizer)」。在很多这些变体中，都会使用一个叫「动量(momentum)」的概念。</p>
<p>「动量」主要处理 SGD 中的两个问题：收敛速度和局部极小点。<br>
用动量可以避免在 learning rate 比较小时收敛到局部最优解，而不是向全局最优解继续前进。</p>
<p>这里的动量就是来自物理的那个动量概念。我们可以想象，一个小球在损失曲面上往下(梯度下降的方向)滚，如果有足够的动量，它就可以“冲过”局部最小值，不被困在那里。<br>
在这个例子中，小球的运动不但被当前位置的坡度（当前的加速度）决定，还受当前的速度（这取决于之前的加速度）的影响。</p>
<p>这个思想放到神经网络中，也就是，一次权重值的更新，不但看当前的梯度，还要看上一次权重更新：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># naive implementation of Optimization with momentum</span></span><br><span class="line">past_velocity = <span class="number">0.</span></span><br><span class="line">momentum = <span class="number">0.1</span>    <span class="comment"># Constant momentum factor</span></span><br><span class="line"><span class="keyword">while</span> loss &gt; <span class="number">0.01</span>:    <span class="comment"># Optimization loop</span></span><br><span class="line">    w, loss, gradient = get_current_parameters()</span><br><span class="line">    velocity = past_velocity * momentum + learning_rate * gradient</span><br><span class="line">    w = w + momentum * velocity - learning_rate * gradient</span><br><span class="line">    past_velocity = velocity</span><br><span class="line">    update_parameter(w)</span><br></pre></td></tr></table></figure>
<h3 id="反向传播算法链式求导"><a class="markdownIt-Anchor" href="#反向传播算法链式求导"></a> 反向传播算法：链式求导</h3>
<p>神经网络是一大堆张量操作链式和在一起的，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(W1, W2, W3) = a(W1, b(W2, c(W3)))    # 其中 W1, W2, W3 是权重</span><br></pre></td></tr></table></figure>
<p>微积分里有个「链式法则(chain rule)」可以给这种复合函数求导：<code>f(g(x)) = f'(g(x)) * g'(x)</code></p>
<p>把这个链式法则用到神经网络就搞出了一个叫「反向传播(Backpropagation)」的算法，<br>
这个算法也叫「反式微分(reverse-mode differentiation)」。</p>
<p>反向传播从最终算出的损失出发，从神经网络的最顶层反向作用至最底层，用这个链式法则算出每层里每个参数对于损失的贡献大小。</p>
<p>现在的 TensorFlow 之类的框架，都有种叫「符号微分(symbolic differentiation)」的能力。<br>
这使得这些框架可以自动求出给定神经网络里操作的梯度函数，然后我们就不用手动实现反向传播了（虽然有意思，但写起来真的烦），直接从梯度函数取值就行了。</p>

  </div>
</article>
<!--Disqus-->


<!--Livere-->

    <div class="blog-post-comments">
        <div id="lv-container" data-id="city" data-uid="MTAyMC80NjEzMi8yMjY0Mw==">
            <noscript>不启用 JavaScript 支持的人是看不到可爱的评论区的。😥</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#deep-learning-with-python"><span class="toc-number">1.</span> <span class="toc-text"> Deep Learning with Python</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#初窥神经网络"><span class="toc-number">1.1.</span> <span class="toc-text"> 初窥神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#导入mnist数据集"><span class="toc-number">1.1.1.</span> <span class="toc-text"> 导入MNIST数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#网络构建"><span class="toc-number">1.1.2.</span> <span class="toc-text"> 网络构建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编译"><span class="toc-number">1.1.3.</span> <span class="toc-text"> 编译</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#预处理"><span class="toc-number">1.1.4.</span> <span class="toc-text"> 预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#图形处理"><span class="toc-number">1.1.4.1.</span> <span class="toc-text"> 图形处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#标签处理"><span class="toc-number">1.1.4.2.</span> <span class="toc-text"> 标签处理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练网络"><span class="toc-number">1.1.5.</span> <span class="toc-text"> 训练网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络的数据表示"><span class="toc-number">1.2.</span> <span class="toc-text"> 神经网络的数据表示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#认识张量"><span class="toc-number">1.2.1.</span> <span class="toc-text"> 认识张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#标量-0d-tensors"><span class="toc-number">1.2.1.1.</span> <span class="toc-text"> 标量 (0D Tensors)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#向量-1d-tensors"><span class="toc-number">1.2.1.2.</span> <span class="toc-text"> 向量 (1D Tensors)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#矩阵-2d-tensors"><span class="toc-number">1.2.1.3.</span> <span class="toc-text"> 矩阵 (2D Tensors)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#高阶张量"><span class="toc-number">1.2.1.4.</span> <span class="toc-text"> 高阶张量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#张量的三要素"><span class="toc-number">1.2.2.</span> <span class="toc-text"> 张量的三要素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#numpy张量操作"><span class="toc-number">1.2.3.</span> <span class="toc-text"> Numpy张量操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#张量切片"><span class="toc-number">1.2.3.1.</span> <span class="toc-text"> 张量切片：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#数据批量"><span class="toc-number">1.2.3.2.</span> <span class="toc-text"> 数据批量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#常见数据张量表示"><span class="toc-number">1.2.4.</span> <span class="toc-text"> 常见数据张量表示</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络的齿轮-张量运算"><span class="toc-number">1.3.</span> <span class="toc-text"> 神经网络的“齿轮”: 张量运算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#逐元素操作element-wise"><span class="toc-number">1.3.1.</span> <span class="toc-text"> 逐元素操作(Element-wise)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#广播broadcasting"><span class="toc-number">1.3.2.</span> <span class="toc-text"> 广播(Broadcasting)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#张量点积dot"><span class="toc-number">1.3.3.</span> <span class="toc-text"> 张量点积(dot)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#张量变形reshaping"><span class="toc-number">1.3.4.</span> <span class="toc-text"> 张量变形(reshaping)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络的引擎-基于梯度的优化"><span class="toc-number">1.4.</span> <span class="toc-text"> 神经网络的“引擎”: 基于梯度的优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#导数derivative"><span class="toc-number">1.4.1.</span> <span class="toc-text"> 导数(derivative)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度gradient"><span class="toc-number">1.4.2.</span> <span class="toc-text"> 梯度(gradient)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#随机梯度下降stochastic-gradient-descent"><span class="toc-number">1.4.3.</span> <span class="toc-text"> 随机梯度下降(Stochastic gradient descent)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#反向传播算法链式求导"><span class="toc-number">1.4.4.</span> <span class="toc-text"> 反向传播算法：链式求导</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&text=Python深度学习之初窥神经网络"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&title=Python深度学习之初窥神经网络"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&is_video=false&description=Python深度学习之初窥神经网络"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Python深度学习之初窥神经网络&body=Check out this article: https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&title=Python深度学习之初窥神经网络"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&title=Python深度学习之初窥神经网络"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&title=Python深度学习之初窥神经网络"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&title=Python深度学习之初窥神经网络"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2020/05/12/DeepLearningWithPython/Deep-Learning with-Python-ch2/&name=Python深度学习之初窥神经网络&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2020 CDFMLR
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<!-- clipboard -->

  <script src="/lib/clipboard/clipboard.min.js"></script>
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功!");
      e.clearSelection();
    })
  })
  </script>

<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-146911386-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?9a0d2e6fde93dad496ac79f04f3aba97";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->


<!--Livere Comments-->

    <script type="text/javascript">
      (function (d, s) {
        var j, e = d.getElementsByTagName(s)[0];

        if (typeof LivereTower === 'function') { return; }

        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;

        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>

</body>
</html>
