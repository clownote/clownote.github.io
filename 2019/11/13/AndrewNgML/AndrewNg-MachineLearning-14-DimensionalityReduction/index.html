<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Notes of Andrew Ng’s Machine Learning —— (14) Dimensionality ReductionMotivationData CompressionDimensionality reduction is another type of unsupervised learning problem. There are a couple of differe">
<meta property="og:type" content="article">
<meta property="og:title" content="维数约减">
<meta property="og:url" content="https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/index.html">
<meta property="og:site_name" content="clownote">
<meta property="og:description" content="Notes of Andrew Ng’s Machine Learning —— (14) Dimensionality ReductionMotivationData CompressionDimensionality reduction is another type of unsupervised learning problem. There are a couple of differe">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8pbajmgl3j30mt0byq68.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8pbaeml8oj30p30brgt7.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8pd8wo33nj30po0dbn3e.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8pdanhskoj30nq0c0did.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8pde507hrj30ox0bhdiw.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8pksu0z5nj30nu07mn1t.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8plgct2i6j31iy0oywjp.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8ta88gafkj30oz0djaeo.jpg">
<meta property="article:published_time" content="2019-11-13T16:50:27.000Z">
<meta property="article:modified_time" content="2021-10-09T08:14:25.382Z">
<meta property="article:author" content="CDFMLR">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8pbajmgl3j30mt0byq68.jpg">
    
    
        
          
              <link rel="shortcut icon" href="/images/rabbit.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/rabbit_192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/rabbit_180.png">
          
        
    
    <!-- title -->
    <title>维数约减</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
    <!--Google search varification (PRIVATE)-->
    <meta name="google-site-verification" content="MrqlpFAD8nDanw3Ypv7ZsIWHLnTdhRuLa4QhSVwxIvc" />
    <!--Google AdSense 关联 (PRIVATE)-->
    <script data-ad-client="ca-pub-1510963483941114" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<meta name="generator" content="Hexo 5.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2019/11/18/blog/mac-localsnapshot-occupy/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/11/07/AndrewNgML/AndrewNg-MachineLearning-13-UnsupervisedLearning/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&text=维数约减"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&title=维数约减"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&is_video=false&description=维数约减"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=维数约减&body=Check out this article: https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&title=维数约减"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&title=维数约减"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&title=维数约减"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&title=维数约减"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&name=维数约减&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Notes-of-Andrew-Ng%E2%80%99s-Machine-Learning-%E2%80%94%E2%80%94-14-Dimensionality-Reduction"><span class="toc-number">1.</span> <span class="toc-text">Notes of Andrew Ng’s Machine Learning —— (14) Dimensionality Reduction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Motivation"><span class="toc-number">1.1.</span> <span class="toc-text">Motivation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-Compression"><span class="toc-number">1.1.1.</span> <span class="toc-text">Data Compression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-Visualization"><span class="toc-number">1.1.2.</span> <span class="toc-text">Data Visualization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Principal-Component-Analysis"><span class="toc-number">1.2.</span> <span class="toc-text">Principal Component Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#PCA-problem-formulation"><span class="toc-number">1.2.0.1.</span> <span class="toc-text">PCA problem formulation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PCA-Algorithm"><span class="toc-number">1.2.0.2.</span> <span class="toc-text">PCA Algorithm</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Data-preprocessing"><span class="toc-number">1.2.0.2.1.</span> <span class="toc-text">Data preprocessing</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Reduce-data-from-n-dimensions-to-k-dimensions"><span class="toc-number">1.2.0.2.2.</span> <span class="toc-text">Reduce data from $n$-dimensions to $k$-dimensions</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Applying-PCA"><span class="toc-number">1.3.</span> <span class="toc-text">Applying PCA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Reconstruction-from-Compressed-Representation"><span class="toc-number">1.3.1.</span> <span class="toc-text">Reconstruction from Compressed Representation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Choosing-the-k-number-of-principal-components"><span class="toc-number">1.3.2.</span> <span class="toc-text">Choosing the $k$ (number of principal components)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Advice-for-Applying-PCA"><span class="toc-number">1.3.3.</span> <span class="toc-text">Advice for Applying PCA</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Supervised-Learning-speedup"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">Supervised Learning speedup</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Application-of-PCA"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">Application of PCA</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bad-use-of-PCA-To-prevent-overfitting"><span class="toc-number">1.3.3.3.</span> <span class="toc-text">Bad use of PCA: To prevent overfitting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PCA-or-not"><span class="toc-number">1.3.3.4.</span> <span class="toc-text">PCA or not</span></a></li></ol></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        维数约减
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">clownote</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-11-13T16:50:27.000Z" itemprop="datePublished">2019-11-13</time>
        
        (Updated: <time datetime="2021-10-09T08:14:25.382Z" itemprop="dateModified">2021-10-09</time>)
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a> › <a class="category-link" href="/categories/Machine-Learning/AndrewNg/">AndrewNg</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="Notes-of-Andrew-Ng’s-Machine-Learning-——-14-Dimensionality-Reduction"><a href="#Notes-of-Andrew-Ng’s-Machine-Learning-——-14-Dimensionality-Reduction" class="headerlink" title="Notes of Andrew Ng’s Machine Learning —— (14) Dimensionality Reduction"></a>Notes of Andrew Ng’s Machine Learning —— (14) Dimensionality Reduction</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><h3 id="Data-Compression"><a href="#Data-Compression" class="headerlink" title="Data Compression"></a>Data Compression</h3><p><em>Dimensionality reduction</em> is another type of unsupervised learning problem.</p>
<p>There are a couple of different reasons why one might want to do dimensionality reduction. One is <strong>data compression</strong> which  not only allows us to compress the data and have it therefore use up less computer memory or disk space, but it will also allow us to speed up our learning algorithms.</p>
<p>What we will do in data compression is reduce data from a high dimensionality to a lower one.</p>
<p>For example:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8pbajmgl3j30mt0byq68.jpg" alt="image-20191107112251112"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8pbaeml8oj30p30brgt7.jpg" alt="屏幕快照 2019-11-07 11.24.44"></p>
<p>As we see, to make dimensionality reduce, intuitionally , we projects the high-D on a lower-D.</p>
<p>In the more typical example of dimensionality reduction we might have a thousand dimensional data that we might want to reduce to let’s say a hundred dimensional.</p>
<p>Generally, suppose we apply <code>dimensionality reduction</code> to a dataset of $m$ examples ${x^{(1)},x^{(2)},\cdots,x^{(m)}}$, where $x^{(i)}\in\R^n$. As a result of this, we will get out a lower dimensional dataset ${z^{(1)},z^{(2)},\cdots,z^{(m)}}$ of $m$ examples where $z^{(i)}\in\R^k$ for some value of $k$ and $k\le n$.</p>
<h3 id="Data-Visualization"><a href="#Data-Visualization" class="headerlink" title="Data Visualization"></a>Data Visualization</h3><p>We can also utilize dimensionality reduction for data visualization.</p>
<p>For a lot of machine learning applications, it really helps us to develop effective learning algorithms, if we can understand our data better. And visualizing the data is a useful in this.</p>
<p>Let’s say we have collected a large data set, which means we have a set of $x^{(i)}\in\R^n$ where $n$ is very large, and we can hardly visualize it because, you know, actually we can only draw a plot in or less than 3 dimension. In this case, what we need to do is dimensionality reduction.</p>
<p>For example, we’ve collected a large data set of many statistics about different countries. we may have a huge data set like this:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8pd8wo33nj30po0dbn3e.jpg" alt="屏幕快照 2019-11-07 12.32.30"></p>
<p>To visualize this data, what we can do is reduce the dimensionality from 50-D to 2-D, and then we can plot a 2-D plot to understand our data better.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8pdanhskoj30nq0c0did.jpg" alt="屏幕快照 2019-11-07 12.31.49"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8pde507hrj30ox0bhdiw.jpg" alt="image-20191107123745407"></p>
<p>To wrap up, suppose you have a dataset ${x^{(1)},x^{(2)},\cdots,x^{(m)}}$ where $x^{(i)}\in\R^n$. In order to visualize it, we apply dimensionality reduction and get ${z^{(1)},z^{(2)},\cdots,z^{(m)}}$ where $z^{(i)}\in\R^k$. And we usual make $k = 2$ or $k = 3$ (since we can plot 2D or 3D data but don’t have ways to visualize higher dimensional data).</p>
<h2 id="Principal-Component-Analysis"><a href="#Principal-Component-Analysis" class="headerlink" title="Principal Component Analysis"></a>Principal Component Analysis</h2><p>For the problem of dimensionality reduction, the most commonly used algorithm is something called <em>principal component analysis</em> (PCA).</p>
<h4 id="PCA-problem-formulation"><a href="#PCA-problem-formulation" class="headerlink" title="PCA problem formulation"></a>PCA problem formulation</h4><p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8pksu0z5nj30nu07mn1t.jpg" alt="image-20191107165349198"></p>
<ul>
<li><p>Reduce form 2-dimension to 1-dimension: </p>
<p>Find a direction (a vector $u^{(1)}\in\R^n$) onto which to project the data so as to minimize the projection error.</p>
</li>
<li><p>Reduce form n-dimension to k-dimension:</p>
<p>Find $k$ vectors $u^{(1)},u^{(2)},\cdots,u^{(k)}$ onto which to project the data, so as to minimize the projection error.</p>
</li>
</ul>
<p>Notice that PCA is not linear regression! In the linear regression, we minimize the vertical length  between the data point and our hypothesis line, while in the PCA, we minimize the distance between the data point and its projected point:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8plgct2i6j31iy0oywjp.jpg" alt="image-20191107171638816"></p>
<h4 id="PCA-Algorithm"><a href="#PCA-Algorithm" class="headerlink" title="PCA Algorithm"></a>PCA Algorithm</h4><h5 id="Data-preprocessing"><a href="#Data-preprocessing" class="headerlink" title="Data preprocessing"></a>Data preprocessing</h5><p>Training set: $x^{(1)},x^{(2)},\cdots,x^{(m)}$</p>
<p>Preprocessing (feature scaling &amp; mean normalization):</p>
<ul>
<li><p>$$<br>\mu_j=\frac{1}{m}\sum_{i=1}^m x_j^{(i)},\qquad s_j=\textrm{standard deviation of feature $j$}<br>$$</p>
</li>
<li><p>Replace each $x_j^{(i)}$ with $\frac{x_j-\mu_j}{s_j}$</p>
</li>
</ul>
<h5 id="Reduce-data-from-n-dimensions-to-k-dimensions"><a href="#Reduce-data-from-n-dimensions-to-k-dimensions" class="headerlink" title="Reduce data from $n$-dimensions to $k$-dimensions"></a>Reduce data from $n$-dimensions to $k$-dimensions</h5><ol>
<li><p>Compute <code>convariance matrix</code> (notated a big Sigma, notice that it’s different from a sum notation):<br>$$<br>\Sigma = \frac{1}{m}\sum_{i=1}^n(x^{(i)})(x^{(i)})^T<br>$$</p>
</li>
<li><p>Compute <code>eigenvectors</code> of matrix $\Sigma$:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[U, S, V] &#x3D; svd(Sigma);</span><br></pre></td></tr></table></figure></li>
<li><p>Get from <code>svd</code>:<br>$$<br>U = \left[\begin{array}{cccc}<br>| &amp; | &amp;  &amp; |\<br>u^{(1)} &amp; u^{(2)} &amp; \cdots &amp; u^{(n)}\<br>| &amp; | &amp;  &amp; |<br>\end{array}\right]<br>\in \R^{n\times n}<br>\Rightarrow<br>U_{reduce}=\left[\begin{array}{cccc}<br>| &amp; | &amp;  &amp; |\<br>u^{(1)} &amp; u^{(2)} &amp; \cdots &amp; u^{(k)}\<br>| &amp; | &amp;  &amp; |<br>\end{array}\right]<br>$$</p>
</li>
<li><p>Make $x\in\R^n\to z\in\R^k$:<br>$$<br>z = U_{reduce}^Tx<br>=\left[\begin{array}{ccc}<br>– &amp; (u^{(1)})^T &amp; –\<br> &amp; \vdots &amp; \<br>– &amp; (u^{(k)})^T &amp; –\<br>\end{array}\right]x<br>$$</p>
</li>
</ol>
<p>This does do the right thing of minimizing this square projection error although the  mathematical proof of that is beyond the scope of this course.</p>
<p>Summary:</p>
<p>After mean normalization (ensure every feature has zero mean) and optionally feature scaling:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Sigma &#x3D; 1&#x2F;m * X&#39; * X;</span><br><span class="line">[U, S, V] &#x3D; svd(Sigma);</span><br><span class="line"></span><br><span class="line">Ureduce &#x3D; U(:, 1:K);</span><br><span class="line">Z &#x3D; X * Ureduce;</span><br></pre></td></tr></table></figure>
<h2 id="Applying-PCA"><a href="#Applying-PCA" class="headerlink" title="Applying PCA"></a>Applying PCA</h2><h3 id="Reconstruction-from-Compressed-Representation"><a href="#Reconstruction-from-Compressed-Representation" class="headerlink" title="Reconstruction from Compressed Representation"></a>Reconstruction from Compressed Representation</h3><p>With PCA, we compressed data from n-dimension to a lower k-dimension. There should be a way to go back from the compressed representation to an approximation of our original high-dimensional data. We call this way <em>reconstruction</em>.</p>
<p>Recall that our PCA is working as $z= U_{reduce}^Tx$.</p>
<p>To reconstruction, what we will do is:<br>$$<br>x_{approx}=U_{reduce}z<br>$$<br>It makes $z\in\R^k \to x_{approx}\in\R^n$. The $x_{approx}$ will be close to $x$, but we can hardly expect $x=x_{approx}$.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8ta88gafkj30oz0djaeo.jpg" alt="image-20191110215011122"></p>
<p>Previously, we said that PCA chooses a direction $u^{(1)},\cdots,u^{(k)}$ onto which to project the data so as to minimize the (squared) projection error. With our new defined $x_{approx}$, another way to say the same is that PCA tries to minimize:<br>$$<br>\frac{1}{m}\sum_{i=1}^{m}||x^{(i)}-x_{approx}^{(i)}||^2<br>$$</p>
<h3 id="Choosing-the-k-number-of-principal-components"><a href="#Choosing-the-k-number-of-principal-components" class="headerlink" title="Choosing the $k$ (number of principal components)"></a>Choosing the $k$ (number of principal components)</h3><p>About PCA, we can see:</p>
<ul>
<li>Average squared projection error: $\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2$</li>
<li>Total variation in the data: $\frac{1}{m}\sum_{i=1}^{m}||x^{(i)}||^2$</li>
</ul>
<p>Typically, choose $k$ to be smallest value so that:<br>$$<br>\frac{\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\frac{1}{m}\sum_{i=1}^{m}||x^{(i)}||^2}\le0.01<br>$$<br>We call it “99% of variance is retained”.</p>
<p>We can also choose other numbers instead of the usual $0.01$, in practice, $0.01\sim0.05$ is all capable.</p>
<p>The algorithm utilized for $k$ choosing is that:</p>
<blockquote>
<p>Try PCA with $k=1,\cdots,n$:</p>
<p>Compute $U_{reduce},z^{(1)},\cdots,z^{(m)},x_{approx}^{(1)},\cdots,x_{approx}^{m}$;</p>
<p>Check if $\frac{\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\frac{1}{m}\sum_{i=1}^{m}||x^{(i)}||^2}\le0.01$?</p>
</blockquote>
<p>In octave, we can implement it as:</p>
<ol>
<li><p><code>[U, S, V] = svd(Sigma)</code>, after running we can get a <code>S</code> that is useful here:</p>
</li>
<li><p>Pick smallest value of $k$ for which (99% of variance retained for example)<br>$$<br>\frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^m S_{ii}}\ge0.99<br>$$</p>
</li>
</ol>
<h3 id="Advice-for-Applying-PCA"><a href="#Advice-for-Applying-PCA" class="headerlink" title="Advice for Applying PCA"></a>Advice for Applying PCA</h3><h4 id="Supervised-Learning-speedup"><a href="#Supervised-Learning-speedup" class="headerlink" title="Supervised Learning speedup"></a>Supervised Learning speedup</h4><p>In supervised learning, we have data like this:<br>$$<br>(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\cdots,(x^{(m)},y^{(m)})<br>$$<br>We can <strong>extract inputs</strong> by:</p>
<ul>
<li>Unlabled dataset: $x^{(1)},x^{(2)},\cdots,x^{(m)}\in \R^n$</li>
<li>Applying PCA: $z^{(1)},z^{(2)},\cdots,z^{(m)}\in \R^k$, where $k$ is lower than $n$</li>
</ul>
<p>Then we get <strong>new training set</strong>:<br>$$<br>(z^{(1)},y^{(1)}),(z^{(2)},y^{(2)}),\cdots,(z^{(m)},y^{(m)})<br>$$<br>Note: Mapping $x^{(i)}\to z^{(i)}$ should be defined by running PCA only on the <strong>training set</strong> . This mapping can be applied as well to the examples $x_{cv}^{(i)}$ and $x_{test}^{(i)}$ in the cross validation and test sets.</p>
<h4 id="Application-of-PCA"><a href="#Application-of-PCA" class="headerlink" title="Application of PCA"></a>Application of PCA</h4><ul>
<li><p>Compression</p>
<ul>
<li>Reduce memory/disk needed to store data</li>
<li>Speed up learning algorithm</li>
</ul>
<p>In this case, we should choose $k$ by <code>% of  variance retained</code>.</p>
</li>
<li><p>Visualization</p>
<p>In this case, $k=2$ or $k=3$</p>
</li>
</ul>
<h4 id="Bad-use-of-PCA-To-prevent-overfitting"><a href="#Bad-use-of-PCA-To-prevent-overfitting" class="headerlink" title="Bad use of PCA: To prevent overfitting"></a>Bad use of PCA: To prevent overfitting</h4><blockquote>
<p>Use $z^{(i)}$ instead of $x^{(i)}$ to reduce the number of features to $k&lt;n$.</p>
<p>Thus, fewer features, less likely to overfit.</p>
</blockquote>
<p>👆 This is a BAD use of PCA.</p>
<p>This might work OK, but isn’t a good way to address overfitting. Use regularization instead:</p>
<p>$$<br>\min_\theta\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2<br>$$</p>
<h4 id="PCA-or-not"><a href="#PCA-or-not" class="headerlink" title="PCA or not"></a>PCA or not</h4><p>PCA is sometimes used where it shouldn’t be, here is an example:</p>
<p>When designing a ML system, we wrote down this steps:</p>
<ol>
<li>Get training set ${(x^{(1)},y^{(1)}),\cdots,(x^{(m)},y^{(m)})}$</li>
<li>Run PCA to reduce $x^{(i)}$ in dimension to get $z^{i}$</li>
<li>Train logistic regression on ${(z^{(1)},y^{(1)}),\cdots,(z^{(m)},y^{(m)})}$</li>
<li>Test on test set: Map $x_{test}^{(i)}$ to $z_{test}^{(i)}$. Run $h_\theta(z)$ on ${(z_{test}^{(1)},y^{(1)}),\cdots,(z_{test}^{(m)},y^{(m)})}$</li>
</ol>
<p>But before doing this, we should think: How about the whole thing without using PCA?</p>
<p>Before implementing PCA, first try running whatever you want to do with the original/raw data $x^{(i)}$. Only if that doesn’t do what you want, then implement PCA and consider using $z^{(i)}$.</p>

  </div>
</article>
<!--Disqus-->


<!--Livere-->

    <div class="blog-post-comments">
        <div id="lv-container" data-id="city" data-uid="MTAyMC80NjEzMi8yMjY0Mw==">
            <noscript>不启用 JavaScript 支持的人是看不到可爱的评论区的。😥</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Notes-of-Andrew-Ng%E2%80%99s-Machine-Learning-%E2%80%94%E2%80%94-14-Dimensionality-Reduction"><span class="toc-number">1.</span> <span class="toc-text">Notes of Andrew Ng’s Machine Learning —— (14) Dimensionality Reduction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Motivation"><span class="toc-number">1.1.</span> <span class="toc-text">Motivation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-Compression"><span class="toc-number">1.1.1.</span> <span class="toc-text">Data Compression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-Visualization"><span class="toc-number">1.1.2.</span> <span class="toc-text">Data Visualization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Principal-Component-Analysis"><span class="toc-number">1.2.</span> <span class="toc-text">Principal Component Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#PCA-problem-formulation"><span class="toc-number">1.2.0.1.</span> <span class="toc-text">PCA problem formulation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PCA-Algorithm"><span class="toc-number">1.2.0.2.</span> <span class="toc-text">PCA Algorithm</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Data-preprocessing"><span class="toc-number">1.2.0.2.1.</span> <span class="toc-text">Data preprocessing</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Reduce-data-from-n-dimensions-to-k-dimensions"><span class="toc-number">1.2.0.2.2.</span> <span class="toc-text">Reduce data from $n$-dimensions to $k$-dimensions</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Applying-PCA"><span class="toc-number">1.3.</span> <span class="toc-text">Applying PCA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Reconstruction-from-Compressed-Representation"><span class="toc-number">1.3.1.</span> <span class="toc-text">Reconstruction from Compressed Representation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Choosing-the-k-number-of-principal-components"><span class="toc-number">1.3.2.</span> <span class="toc-text">Choosing the $k$ (number of principal components)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Advice-for-Applying-PCA"><span class="toc-number">1.3.3.</span> <span class="toc-text">Advice for Applying PCA</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Supervised-Learning-speedup"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">Supervised Learning speedup</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Application-of-PCA"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">Application of PCA</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bad-use-of-PCA-To-prevent-overfitting"><span class="toc-number">1.3.3.3.</span> <span class="toc-text">Bad use of PCA: To prevent overfitting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PCA-or-not"><span class="toc-number">1.3.3.4.</span> <span class="toc-text">PCA or not</span></a></li></ol></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&text=维数约减"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&title=维数约减"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&is_video=false&description=维数约减"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=维数约减&body=Check out this article: https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&title=维数约减"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&title=维数约减"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&title=维数约减"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&title=维数约减"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/11/13/AndrewNgML/AndrewNg-MachineLearning-14-DimensionalityReduction/&name=维数约减&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2021 CDFMLR
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-146911386-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?9a0d2e6fde93dad496ac79f04f3aba97";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->


<!--Livere Comments-->

    <script type="text/javascript">
      (function (d, s) {
        var j, e = d.getElementsByTagName(s)[0];

        if (typeof LivereTower === 'function') { return; }

        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;

        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>

</body>
</html>
