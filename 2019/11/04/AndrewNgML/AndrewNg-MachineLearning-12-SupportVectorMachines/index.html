<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Notes of Andrew Ng‚Äôs Machine Learning ‚Äî‚Äî (12) Support Vector MachinesLarge Margin ClassificationOptimization ObjectiveThe Support Vector Machine (or SVM) is a powerful algorithm that is widely used in">
<meta property="og:type" content="article">
<meta property="og:title" content="ÊîØÊåÅÂêëÈáèÊú∫">
<meta property="og:url" content="https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/index.html">
<meta property="og:site_name" content="clownote">
<meta property="og:description" content="Notes of Andrew Ng‚Äôs Machine Learning ‚Äî‚Äî (12) Support Vector MachinesLarge Margin ClassificationOptimization ObjectiveThe Support Vector Machine (or SVM) is a powerful algorithm that is widely used in">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8f4rfgycnj31lc0mqjsr.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8f58f0dolj31ao0h6wpq.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8f5vx0x43j31dm0ni156.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8f6tz8op1j30q60kon78.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8f7bsn392j30zm0iqwmu.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8h7ruhj8kj31em0s0nfd.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8h9w173flj309l06t3zr.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8harwzv9cj324i0nu4qp.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jjopayn9j30c5083jui.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jljliww8j30o90dmwjt.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jmrwo86pj30ph0dwwm1.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jmzc2qq4j30ot0ditej.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jukxlnxkj306u04laad.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jul1utnaj306i04ezk4.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jul8myufj30mv0cz0xu.jpg">
<meta property="article:published_time" content="2019-11-04T16:38:39.000Z">
<meta property="article:modified_time" content="2021-01-25T04:49:37.411Z">
<meta property="article:author" content="CDFMLR">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g8f4rfgycnj31lc0mqjsr.jpg">
    
    
        
          
              <link rel="shortcut icon" href="/images/rabbit.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/rabbit_192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/rabbit_180.png">
          
        
    
    <!-- title -->
    <title>ÊîØÊåÅÂêëÈáèÊú∫</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
    <!--Google search varification (PRIVATE)-->
    <meta name="google-site-verification" content="MrqlpFAD8nDanw3Ypv7ZsIWHLnTdhRuLa4QhSVwxIvc" />
    <!--Google AdSense ÂÖ≥ËÅî (PRIVATE)-->
    <script data-ad-client="ca-pub-1510963483941114" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<meta name="generator" content="Hexo 5.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">È¶ñÈ°µ</a></li>
         
          <li><a href="/about/">ÂÖ≥‰∫é</a></li>
         
          <li><a href="/archives/">ÂΩíÊ°£</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">È°πÁõÆ</a></li>
         
          <li><a href="/search/">ÊêúÁ¥¢</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2019/11/07/AndrewNgML/AndrewNg-MachineLearning-13-UnsupervisedLearning/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/10/26/AndrewNgML/AndrewNg-MachineLearning-11-MachineLearningSystemDesign/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">‰∏ä‰∏ÄÁØá</span>
      <span id="i-next" class="info" style="display:none;">‰∏ã‰∏ÄÁØá</span>
      <span id="i-top" class="info" style="display:none;">ËøîÂõûÈ°∂ÈÉ®</span>
      <span id="i-share" class="info" style="display:none;">ÂàÜ‰∫´ÊñáÁ´†</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&text=ÊîØÊåÅÂêëÈáèÊú∫"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&title=ÊîØÊåÅÂêëÈáèÊú∫"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&is_video=false&description=ÊîØÊåÅÂêëÈáèÊú∫"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=ÊîØÊåÅÂêëÈáèÊú∫&body=Check out this article: https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&title=ÊîØÊåÅÂêëÈáèÊú∫"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&title=ÊîØÊåÅÂêëÈáèÊú∫"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&title=ÊîØÊåÅÂêëÈáèÊú∫"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&title=ÊîØÊåÅÂêëÈáèÊú∫"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&name=ÊîØÊåÅÂêëÈáèÊú∫&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Notes-of-Andrew-Ng%E2%80%99s-Machine-Learning-%E2%80%94%E2%80%94-12-Support-Vector-Machines"><span class="toc-number">1.</span> <span class="toc-text">Notes of Andrew Ng‚Äôs Machine Learning ‚Äî‚Äî (12) Support Vector Machines</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Large-Margin-Classification"><span class="toc-number">1.1.</span> <span class="toc-text">Large Margin Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Optimization-Objective"><span class="toc-number">1.1.1.</span> <span class="toc-text">Optimization Objective</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Large-Margin-Intuition"><span class="toc-number">1.1.2.</span> <span class="toc-text">Large Margin Intuition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mathematics-Behind-Large-Margin-Classification"><span class="toc-number">1.1.3.</span> <span class="toc-text">Mathematics Behind Large Margin Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Vector-Inner-Product"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">Vector Inner Product</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SVM-Decision-Boundary"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">SVM Decision Boundary</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kernels"><span class="toc-number">1.2.</span> <span class="toc-text">kernels</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kernels-and-Similarity"><span class="toc-number">1.2.1.</span> <span class="toc-text">Kernels and Similarity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SVM-with-Kernels"><span class="toc-number">1.2.2.</span> <span class="toc-text">SVM with Kernels</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Support-Vector-Machine-With-Kernels-Algorithm"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">The Support Vector Machine With Kernels Algorithm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SVM-parameters"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">SVM parameters</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM-in-Practice"><span class="toc-number">1.3.</span> <span class="toc-text">SVM in Practice</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Using-An-SVM"><span class="toc-number">1.3.1.</span> <span class="toc-text">Using An SVM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Other-choices-of-kernel"><span class="toc-number">1.3.2.</span> <span class="toc-text">Other choices of kernel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-class-classification"><span class="toc-number">1.3.3.</span> <span class="toc-text">Multi-class classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Logistic-regression-v-s-SVMs"><span class="toc-number">1.3.4.</span> <span class="toc-text">Logistic regression v.s. SVMs</span></a></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        ÊîØÊåÅÂêëÈáèÊú∫
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">clownote</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-11-04T16:38:39.000Z" itemprop="datePublished">2019-11-04</time>
        
        (Updated: <time datetime="2021-01-25T04:49:37.411Z" itemprop="dateModified">2021-01-25</time>)
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a> ‚Ä∫ <a class="category-link" href="/categories/Machine-Learning/AndrewNg/">AndrewNg</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="Notes-of-Andrew-Ng‚Äôs-Machine-Learning-‚Äî‚Äî-12-Support-Vector-Machines"><a href="#Notes-of-Andrew-Ng‚Äôs-Machine-Learning-‚Äî‚Äî-12-Support-Vector-Machines" class="headerlink" title="Notes of Andrew Ng‚Äôs Machine Learning ‚Äî‚Äî (12) Support Vector Machines"></a>Notes of Andrew Ng‚Äôs Machine Learning ‚Äî‚Äî (12) Support Vector Machines</h1><h2 id="Large-Margin-Classification"><a href="#Large-Margin-Classification" class="headerlink" title="Large Margin Classification"></a>Large Margin Classification</h2><h3 id="Optimization-Objective"><a href="#Optimization-Objective" class="headerlink" title="Optimization Objective"></a>Optimization Objective</h3><p>The <em>Support Vector Machine</em> (or <em>SVM</em>) is a powerful algorithm that is widely used in both industry and academia. Compared to both logistic regression and neural networks, the SVM somethimes gives a cleaner and more powerful way of learning complex non-linear functions.</p>
<p>What the <strong>cost</strong> of example in our logistic regression is as:<br>$$<br>-y\log\Big(\frac{1}{1+e^{-\theta^Tx}}\Big)-(1-y)\log\Big(1-\frac{1}{1+e^{-\theta^Tx}}\Big)<br>$$<br>It looks like this:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8f4rfgycnj31lc0mqjsr.jpg" alt="Â±èÂπïÂø´ÁÖß 2019-09-19 23.23.39"></p>
<p>With SVM, we‚Äôd like to make a little change of them so that:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8f58f0dolj31ao0h6wpq.jpg" alt="image-20191029161946046"></p>
<p>We set a $\textrm{cost}_1$ instead of $-\log(h)$ and set a $\textrm{cost}_0$ instead of $-\log(1-h)$.</p>
<p>So, for logistic regression, we are going to:<br>$$<br>\min_\theta\frac{1}{m} \sum_{i=1}^m \large[ -y^{(i)}\ \log (h_\theta (x^{(i)})) - (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2<br>$$<br>And for Support Vector Machine, we change it to:<br>$$<br>\min_\theta<br>C\sum_{i=1}^m \large[ y^{(i)} \mathop{\textrm{cost}_1}(\theta^Tx^{(i)})</p>
<ul>
<li>(1 - y^{(i)})\ \mathop{\textrm{cost}_0}(\theta^Tx^{(i)})\large]</li>
<li>\frac{1}{2}\sum_{j=1}^n \theta_j^2<br>$$<br>We dropped the $\frac{1}{m}$ in both terms, then multiplied a $C=\lambda$ to them. In this case we can promise that we can get the same min $\theta$ with those two operators.</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8f5vx0x43j31dm0ni156.jpg" alt="image-20191029164223055"></p>
<h3 id="Large-Margin-Intuition"><a href="#Large-Margin-Intuition" class="headerlink" title="Large Margin Intuition"></a>Large Margin Intuition</h3><p>Let‚Äôs say the SVM decision boundary:<br>$$<br>\min_\theta<br>C\sum_{i=1}^m \large[ y^{(i)} \mathop{\textrm{cost}_1}(\theta^Tx^{(i)})</p>
<ul>
<li>(1 - y^{(i)})\ \mathop{\textrm{cost}_0}(\theta^Tx^{(i)})\large]</li>
<li>\frac{1}{2}\sum_{j=1}^n \theta_j^2<br>$$<br>If we set $C$ a very  large value, $100000$ for example, when this optimization objective , we‚Äôre going to be highly motivated to choose a value, so that <strong>the first term is equal to zero</strong>. And our goal become:<br>$$<br>\begin{array}{l}<br>  \min_\theta \frac{1}{2}\sum_{j=1}^{n}\theta_j^2 \\<br>  s.t. \quad \begin{array}{l}<pre><code>  \theta^Tx^&#123;(i)&#125; \ge 1 &amp; \textrm&#123;if &#125; y^&#123;(i)&#125;=1\\
  \theta^Tx^&#123;(i)&#125; \le -1 &amp; \textrm&#123;if &#125; y^&#123;(i)&#125;=0</code></pre>
  \end{array}<br>\end{array}<br>$$<br>We will get a very intersting decision boundary. The Support Vector Machines will choose a decision boundary that does a great job of separating the positive and negative examples (the black line below install of others).</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8f6tz8op1j30q60kon78.jpg" alt="image-20191029171503318"></p>
<p>we see that the black decision boundary has some larger minimum distance from any of my training examples, whereas the magenta and the green lines come awfully close to the training examples.</p>
<p>And mathematically, what that does is, this black decision boundary has a larger distance (the blue lines).That distance is called the <em>margin</em> of the support vector machine and this gives the SVM a certain robustness, because it tries to separate the data with as a large a margin as possible. So the support vector machine is sometimes also called a large margin classifier.</p>
<p>If $C$ is very large than the SVM will be sensitive to <strong>outliers</strong>. For example, we a very large $C$, $100000$ maybe, with a outliers at the left bottom, the learning algorithm will get the magenta decision boundary. However, if $C$ is not too large, we are going to get the black one as wanted:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8f7bsn392j30zm0iqwmu.jpg" alt="image-20191029173209265"></p>
<h3 id="Mathematics-Behind-Large-Margin-Classification"><a href="#Mathematics-Behind-Large-Margin-Classification" class="headerlink" title="Mathematics Behind Large Margin Classification"></a>Mathematics Behind Large Margin Classification</h3><h4 id="Vector-Inner-Product"><a href="#Vector-Inner-Product" class="headerlink" title="Vector Inner Product"></a>Vector Inner Product</h4><p>Let‚Äôs say we have two vectors:<br>$$<br>u=\left[\begin{array}{l}u1 \ u2\end{array}\right],\qquad v=\left[\begin{array}{l}v1 \ v2\end{array}\right]<br>$$<br>We define their <em>Vector Inner Product</em> as:<br>$$<br>\textrm{Inner Product} =<br>u^Tv=v^Tu<br>$$<br>And we can also get the inner product by getting:<br>$$<br>\begin{array}{ccl}<br>||u|| &amp;=&amp; \textrm{length of vector } u= \sqrt{u_1^2+u_2^2}\<br>p &amp;=&amp; \textrm{length of projection of } v \textrm{ onto } u \textrm{ (signed)} \ \<br>u^Tv &amp;=&amp; p \cdot ||u||<br>\end{array}<br>$$<br><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8h7ruhj8kj31em0s0nfd.jpg" alt="image-20191031111759471"></p>
<h4 id="SVM-Decision-Boundary"><a href="#SVM-Decision-Boundary" class="headerlink" title="SVM Decision Boundary"></a>SVM Decision Boundary</h4><p>When the $C$ is very large, we can get this:<br>$$<br>\begin{array}{l}<br>    \min_\theta \frac{1}{2}\sum_{j=1}^{n}\theta_j^2<br>    =\frac{1}{2}\Big(\sqrt{\sum_{j=1}^n\theta_j^2}\Big)^2<br>    =\frac{1}{2}||\theta||^2 \\<br>    s.t. \quad \begin{array}{l}<br>        \theta^Tx^{(i)} \ge 1 &amp; \textrm{if } y^{(i)}=1\<br>        \theta^Tx^{(i)} \le -1 &amp; \textrm{if } y^{(i)}=0<br>    \end{array}<br>\end{array}<br>$$<br>If we make a simplification as $\theta_0=0, n=2$, so that $\theta=[\theta_1,\theta_2]^T$, then our $\theta^Tx^{(i)}=p^{(i)}\cdot ||\theta||=\theta_1x_1^{(i)}+\theta_2x_2^{(i)}$.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8h9w173flj309l06t3zr.jpg" alt="image-20191031123156678"></p>
<p>So, our description can be:<br>$$<br>\begin{array}{l}<br>    \min_\theta \frac{1}{2}\sum_{j=1}^{n}\theta_j^2<br>    =\frac{1}{2}\Big(\sqrt{\sum_{j=1}^n\theta_j^2}\Big)^2<br>    =\frac{1}{2}||\theta||^2 \\<br>    s.t. \quad \begin{array}{l}<br>        p^{(i)}\cdot ||\theta|| \ge 1 &amp; \textrm{if } y^{(i)}=1\<br>        p^{(i)}\cdot ||\theta|| \le -1 &amp; \textrm{if } y^{(i)}=0<br>    \end{array}\\<br>    \textrm{where $p^{(i)}$ is the projection of $x^{(i)}$ onto the vector $\theta$.}<br>\end{array}<br>$$<br>In the condition of $y=1$, to min the $\theta$, we should promise $||\theta||$ is small as well as make $p^{(i)}\cdot ||\theta|| \ge 1$ as possible. So, we get a large $p^{(i)}$.</p>
<p>For the negative example, similarly, we will get a $p^{(i)}$ whos absolute value is large.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8harwzv9cj324i0nu4qp.jpg" alt="Â±èÂπïÂø´ÁÖß 2019-10-31 12.58.43"></p>
<p>As we seen, a large $p^{(i)}$ gives us a large margin.</p>
<h2 id="kernels"><a href="#kernels" class="headerlink" title="kernels"></a>kernels</h2><h3 id="Kernels-and-Similarity"><a href="#Kernels-and-Similarity" class="headerlink" title="Kernels and Similarity"></a>Kernels and Similarity</h3><p>We can develop complex nonlinear classifiers by adapting support vector machines. The main technique for this is something called <em>kernels</em>.</p>
<p>Let‚Äôs say if we a training set that looks this:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jjopayn9j30c5083jui.jpg" alt="image-20191102114127170"></p>
<p>Recall the past, we will work with lots of polynomial features to hypothesis $y=1$ if $\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1x_2+\theta_4x_1^2+\theta_5x_2^2+\cdots\ge 0$ and predict $0$ otherwise.</p>
<p>Another way of that is:<br>$$<br>\begin{array}{lcl}<br>\textrm{Predict } y=1 &amp;\textrm{if}&amp;\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3+\cdots\ge 0<br>\end{array}<br>$$<br>Where $f_1=x_1,f_2=x_2,f_3=x_1x_2,f_4=x_1^2,‚Ä¶$</p>
<p>If we have a lot of features, then there will be too many polynomial terms, which can become very computationally expensive. So is there a different or a better choice of the features that we can use to plug into this sort of hypothesis form, or in another words, how to define a set of new $f_1,f_2,f_3$ to avoid  high polynomial calculations?</p>
<p>Here is one idea:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jljliww8j30o90dmwjt.jpg" alt="image-20191102124615797"></p>
<p>To simplify the description and make it more intuitional, let‚Äôs say we have only $x_0,x_1,x_2$ and we are going to define only three new features ($f_1,f_2,f_3$). </p>
<p>Leave $x_0$ alone, we manually pick a few points $(x_1^{(i)},x_2^{(i)})$ from a plot of $x_1$-$x_2$, in this example they are notated as $l^{(1)},l^{(2)},l^{(3)}$. We call this points <code>landmarks</code>.</p>
<p>And then, given a $x$, we can compute new feature depending on proximity to landmarks $l^{(1)},l^{(2)},l^{(3)}$ as follow:<br>$$<br>f_i = \mathop{\textrm{similarity}}(x,l^{(i)}) = \exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2}) = \exp(-\frac{\sum_{j=1}^n(x_j-l_j^{(i)})^2}{2\sigma^2})<br>$$<br>The mathematical term for the simiarity function is <code>kernel function</code>. There are many formula of kernel functions can be chosen. And the specific kernel we‚Äôre using here is actually called a <em>Gaussian Kernel</em>.</p>
<p>Note. $||w||$ means the length of the vector $w$.</p>
<p>Given  $x$, for each $l^{(i)}$ we can get a $f_i$:</p>
<ul>
<li>If x is near $l^{(i)}$:</li>
</ul>
<p>$$<br>f_i \approx \lim_{x\to l^{(i)}}\exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2}) = \exp(-\frac{0^2}{2\sigma^2}) = 1<br>$$</p>
<ul>
<li>If $x$ far from $l^{(i)}$:</li>
</ul>
<p>$$<br>f_i \approx \lim_{||x-l^{(i)}||\to +\infin}\exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2}) = \exp(-\frac{\infin^2}{2\sigma^2}) = 0<br>$$</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jmrwo86pj30ph0dwwm1.jpg" alt="image-20191102132852867"></p>
<p>We can see that: If sigma squared is large, as we move away from $l^{(1)}$, the value of $f_1$ falls away much more slowly.</p>
<p>So, given this definition of the features, let‚Äôs see what source of hypothesis we can learn:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jmzc2qq4j30ot0ditej.jpg" alt="image-20191102133600603"></p>
<p>We can see that, in this example, only given the $x$ near $l^{(1)}$ or $l^{(2)}$ will it predict ‚Äú1‚Äù, if the $x$ if far from this two landmarks, it will predict ‚Äú0‚Äù.</p>
<h3 id="SVM-with-Kernels"><a href="#SVM-with-Kernels" class="headerlink" title="SVM with Kernels"></a>SVM with Kernels</h3><p>We have talked about the process of picking a few landmarks. But how to choose our $l^{(i)}$s? Actually we will simply choose the location of our landmarks to be exactly near the locations of our m training examples. Here is what we are going to do:</p>
<hr>
<h4 id="The-Support-Vector-Machine-With-Kernels-Algorithm"><a href="#The-Support-Vector-Machine-With-Kernels-Algorithm" class="headerlink" title="The Support Vector Machine With Kernels Algorithm"></a>The Support Vector Machine With Kernels Algorithm</h4><p>Given a set of training data: $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),‚Ä¶,(x^{(m)},y^{(m)})$ </p>
<p>Choose landmarks: $l^{(i)}=x^{(i)} \quad \textrm{for }i=1,2,\cdots,m$</p>
<p>For example $x$: $f_i=\mathop{\textrm{similarity}}(x,l^{(i)}) \quad \textrm{for }i=1,2,\cdots,m$</p>
<p>Group $f_i$s into a vector: $f=[f_0,f_1,f_2,\cdots,f_m]^T$, in which the $f_0 \equiv 1$ is a extra feature for convention.</p>
<p>Hypothesis: </p>
<ul>
<li>Given $x$, compute features $f\in\R^{m+1}$</li>
<li>Predict $y=1$ if $\theta^Tf=\theta_0f_0+\theta_1f_1+\cdots\ge 0$ (where $\theta\in\R^{m+1}$)</li>
</ul>
<p>Training:<br>$$<br>\min_\theta<br>C\sum_{i=1}^m \large[ y^{(i)} \mathop{\textrm{cost}_1}(\theta^Tf^{(i)})</p>
<ul>
<li>(1 - y^{(i)})\ \mathop{\textrm{cost}_0}(\theta^Tf^{(i)})\large]</li>
<li>\frac{1}{2}\sum_{j=1}^n \theta_j^2<br>$$<br>Notice: instead of $\theta^Tx^{(i)}$ that we use before, now we use $\theta^Tf^{(i)}$. And here our $n$ is equal to $m$. </li>
</ul>
<p>Mathematical tricks in practice: Ignoring the $\theta_0$ (s.t. $\theta=[\theta_1,\cdots,\theta_m]^T$), we can get $\sum_j\theta_j^2=\theta^T\theta$. Moreover, for mathematical convenience when deal with a large training set, we gonna use <strong>$\theta^T M \theta$</strong> instead of $\theta^T\theta$.</p>
<hr>
<h4 id="SVM-parameters"><a href="#SVM-parameters" class="headerlink" title="SVM parameters"></a>SVM parameters</h4><p>Here are parameters that we are going to choose when use the algorithm above:</p>
<ul>
<li>$C$ (=$\frac{1}{m}$)<ul>
<li>Large $C$: Lower bias, high variance (small $\lambda$)</li>
<li>Small $C$: Higher bias, low variance (large $\lambda$)</li>
</ul>
</li>
<li>$\sigma^2$<ul>
<li>Large $\sigma^2$: Features $f_i$ vary more smoothly. Higher bias, lower variance.<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jukxlnxkj306u04laad.jpg" alt="Â±èÂπïÂø´ÁÖß 2019-11-02 15.51.46"></li>
<li>Small $\sigma^2$: Features $f_i$ vary less smoothly. Lower bias, higher variance.<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jul1utnaj306i04ezk4.jpg" alt="Â±èÂπïÂø´ÁÖß 2019-11-02 15.51.50"></li>
</ul>
</li>
</ul>
<p>E.g. Suppose we train an SVM and find it overfits our training data. A reasonable next step is going to decrease $C$ or increase $\sigma^2$.</p>
<h2 id="SVM-in-Practice"><a href="#SVM-in-Practice" class="headerlink" title="SVM in Practice"></a>SVM in Practice</h2><h3 id="Using-An-SVM"><a href="#Using-An-SVM" class="headerlink" title="Using An SVM"></a>Using An SVM</h3><p>We are not recommended writing our own software to solve for the parameters via a SVM. We should call some library functions to do that.</p>
<p>When using SVM software package (e.g. <code>liblinear</code>, <code>libsvm</code>) to solve for parameters $\theta$, we need to specify:</p>
<ul>
<li>Choice of parameter $C$;</li>
<li>Choice of kernel (similarity function):<ul>
<li><strong>No kernel</strong> (‚Äúlinear kernel‚Äù, do our common logistic regression): Predict $y=1$ if $\theta^Tx\ge0$. For large $n$ and small $m$ case (no kernel can avoid overfitting).</li>
<li><strong>Gaussian kernel</strong>: $f_i=\exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2})$, where $l^{(i)}=x^{(i)}$.(Need to choose $\sigma^2$). For small $n$ and large $m$ case (Gaussian kernel can fit a more complex nonlinear decision boundary).</li>
</ul>
</li>
</ul>
<p>If we decide to use a Gaussian kernel, here is what we are going to do:</p>
<ul>
<li>Provide a kernel (similarity) function:<br>$$<br>\begin{array}{l}<br>\textrm{function f = kernel(x1, x2)}\<br>\qquad \textrm{f} = \exp(-\frac{||\textrm{x1}-\textrm{x2}||^2}{2\sigma^2})\<br>\textrm{return}<br>\end{array}<br>$$<br>Note: Do perform feature scaling before using the Gaussian kernel.</li>
</ul>
<h3 id="Other-choices-of-kernel"><a href="#Other-choices-of-kernel" class="headerlink" title="Other choices of kernel"></a>Other choices of kernel</h3><p>Not all similarity functions $\mathop{\textrm{similarity}}(x,l)$ make valid kernels. (Need to satisfy technical condiction called ‚ÄúMercer‚Äôs Theorem‚Äù to make sure SVM packages‚Äô optimizations run correctly, and do not diverge).</p>
<p>There are many off-the-shelf kernels available, for example:</p>
<ul>
<li>Polynomial kernel: $k(x,l)=(X^Tl+constant)^{degree}$, need to choose the $constant$ and $degree$. Usually performs worse than Gaussian kernel. Only for $X$ and $l$ are all strictly non negative.</li>
<li>More esoteric: String kernel (for input is text string),  chi-square kernel, histogram intersection kernel, ‚Ä¶</li>
</ul>
<h3 id="Multi-class-classification"><a href="#Multi-class-classification" class="headerlink" title="Multi-class classification"></a>Multi-class classification</h3><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jul8myufj30mv0cz0xu.jpg" alt="image-20191102171432072"></p>
<h3 id="Logistic-regression-v-s-SVMs"><a href="#Logistic-regression-v-s-SVMs" class="headerlink" title="Logistic regression v.s. SVMs"></a>Logistic regression v.s. SVMs</h3><p>$n$ = number of features ($x\in\R^{n+1}$);</p>
<p>$m$ = number of training examples;</p>
<ul>
<li>If $n$ is large (relative to $m$): ($n\ge m$, e.g. $n=10,000, m=10 \sim 1000$)<ul>
<li>Use logistic regression, or SVM without a kernel (‚Äúlinear kernel‚Äù) </li>
</ul>
</li>
<li>If $n$ is small, $m$ is intermediate: (e.g. $n=1\sim1000,m=50,000+$)<ul>
<li>Use SVM with Gaussian kernel</li>
</ul>
</li>
<li>If $n$ is small, $m$ is large:<ul>
<li>Create/add more features, then use logistic regression or SVM without a kernel</li>
</ul>
</li>
</ul>
<p>Note: Neural network likely to work well for most of these settings, but may be slower to train.</p>

  </div>
</article>
<!--Disqus-->


<!--Livere-->

    <div class="blog-post-comments">
        <div id="lv-container" data-id="city" data-uid="MTAyMC80NjEzMi8yMjY0Mw==">
            <noscript>‰∏çÂêØÁî® JavaScript ÊîØÊåÅÁöÑ‰∫∫ÊòØÁúã‰∏çÂà∞ÂèØÁà±ÁöÑËØÑËÆ∫Âå∫ÁöÑ„ÄÇüò•</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">È¶ñÈ°µ</a></li>
         
          <li><a href="/about/">ÂÖ≥‰∫é</a></li>
         
          <li><a href="/archives/">ÂΩíÊ°£</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">È°πÁõÆ</a></li>
         
          <li><a href="/search/">ÊêúÁ¥¢</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Notes-of-Andrew-Ng%E2%80%99s-Machine-Learning-%E2%80%94%E2%80%94-12-Support-Vector-Machines"><span class="toc-number">1.</span> <span class="toc-text">Notes of Andrew Ng‚Äôs Machine Learning ‚Äî‚Äî (12) Support Vector Machines</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Large-Margin-Classification"><span class="toc-number">1.1.</span> <span class="toc-text">Large Margin Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Optimization-Objective"><span class="toc-number">1.1.1.</span> <span class="toc-text">Optimization Objective</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Large-Margin-Intuition"><span class="toc-number">1.1.2.</span> <span class="toc-text">Large Margin Intuition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mathematics-Behind-Large-Margin-Classification"><span class="toc-number">1.1.3.</span> <span class="toc-text">Mathematics Behind Large Margin Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Vector-Inner-Product"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">Vector Inner Product</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SVM-Decision-Boundary"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">SVM Decision Boundary</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kernels"><span class="toc-number">1.2.</span> <span class="toc-text">kernels</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kernels-and-Similarity"><span class="toc-number">1.2.1.</span> <span class="toc-text">Kernels and Similarity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SVM-with-Kernels"><span class="toc-number">1.2.2.</span> <span class="toc-text">SVM with Kernels</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Support-Vector-Machine-With-Kernels-Algorithm"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">The Support Vector Machine With Kernels Algorithm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SVM-parameters"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">SVM parameters</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM-in-Practice"><span class="toc-number">1.3.</span> <span class="toc-text">SVM in Practice</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Using-An-SVM"><span class="toc-number">1.3.1.</span> <span class="toc-text">Using An SVM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Other-choices-of-kernel"><span class="toc-number">1.3.2.</span> <span class="toc-text">Other choices of kernel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-class-classification"><span class="toc-number">1.3.3.</span> <span class="toc-text">Multi-class classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Logistic-regression-v-s-SVMs"><span class="toc-number">1.3.4.</span> <span class="toc-text">Logistic regression v.s. SVMs</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&text=ÊîØÊåÅÂêëÈáèÊú∫"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&title=ÊîØÊåÅÂêëÈáèÊú∫"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&is_video=false&description=ÊîØÊåÅÂêëÈáèÊú∫"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=ÊîØÊåÅÂêëÈáèÊú∫&body=Check out this article: https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&title=ÊîØÊåÅÂêëÈáèÊú∫"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&title=ÊîØÊåÅÂêëÈáèÊú∫"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&title=ÊîØÊåÅÂêëÈáèÊú∫"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&title=ÊîØÊåÅÂêëÈáèÊú∫"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/11/04/AndrewNgML/AndrewNg-MachineLearning-12-SupportVectorMachines/&name=ÊîØÊåÅÂêëÈáèÊú∫&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> ËèúÂçï</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> ÁõÆÂΩï</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> ÂàÜ‰∫´</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> ËøîÂõûÈ°∂ÈÉ®</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2021 CDFMLR
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">È¶ñÈ°µ</a></li>
         
          <li><a href="/about/">ÂÖ≥‰∫é</a></li>
         
          <li><a href="/archives/">ÂΩíÊ°£</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">È°πÁõÆ</a></li>
         
          <li><a href="/search/">ÊêúÁ¥¢</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Â§çÂà∂Âà∞Á≤òË¥¥Êùø!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Â§çÂà∂ÊàêÂäü!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-146911386-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?9a0d2e6fde93dad496ac79f04f3aba97";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->


<!--Livere Comments-->

    <script type="text/javascript">
      (function (d, s) {
        var j, e = d.getElementsByTagName(s)[0];

        if (typeof LivereTower === 'function') { return; }

        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;

        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>

</body>
</html>
