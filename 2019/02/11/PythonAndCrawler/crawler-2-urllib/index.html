<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="爬虫请求库的使用之Urllib 本文主要介绍 Urllib 库。  urlliburllib 是 Python 内置的 HTTP 请求库。 urllib 有 request，error，parse，robotparser 四个模块。 urllib.request 发送请求urlopen(): 发送请求12345678910# 获取 python 官网 HTML 源码import urllib.re">
<meta property="og:type" content="article">
<meta property="og:title" content="python请求库urllib">
<meta property="og:url" content="https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/index.html">
<meta property="og:site_name" content="clownote">
<meta property="og:description" content="爬虫请求库的使用之Urllib 本文主要介绍 Urllib 库。  urlliburllib 是 Python 内置的 HTTP 请求库。 urllib 有 request，error，parse，robotparser 四个模块。 urllib.request 发送请求urlopen(): 发送请求12345678910# 获取 python 官网 HTML 源码import urllib.re">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2019-02-11T22:49:55.000Z">
<meta property="article:modified_time" content="2021-01-14T04:02:37.309Z">
<meta property="article:author" content="CDFMLR">
<meta property="article:tag" content="Crawler">
<meta name="twitter:card" content="summary">
    
    
        
          
              <link rel="shortcut icon" href="/images/rabbit.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/rabbit_192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/rabbit_180.png">
          
        
    
    <!-- title -->
    <title>python请求库urllib</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
    <!--Google search varification (PRIVATE)-->
    <meta name="google-site-verification" content="MrqlpFAD8nDanw3Ypv7ZsIWHLnTdhRuLa4QhSVwxIvc" />
    <!--Google AdSense 关联 (PRIVATE)-->
    <script data-ad-client="ca-pub-1510963483941114" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<meta name="generator" content="Hexo 5.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2019/02/12/PythonAndCrawler/crawler-3-requests/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/02/10/PythonAndCrawler/crawler-1-basic/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&text=python请求库urllib"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&title=python请求库urllib"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&is_video=false&description=python请求库urllib"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=python请求库urllib&body=Check out this article: https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&title=python请求库urllib"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&title=python请求库urllib"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&title=python请求库urllib"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&title=python请求库urllib"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&name=python请求库urllib&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E8%AF%B7%E6%B1%82%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8%E4%B9%8BUrllib"><span class="toc-number">1.</span> <span class="toc-text">爬虫请求库的使用之Urllib</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#urllib"><span class="toc-number">1.1.</span> <span class="toc-text">urllib</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#urllib-request-%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82"><span class="toc-number">1.1.1.</span> <span class="toc-text">urllib.request 发送请求</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#urlopen-%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">urlopen(): 发送请求</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#urlopen-%E5%B8%A6-data-%E5%8F%82%E6%95%B0-%E7%94%A8-POST-%E5%8F%91%E9%80%81%E4%B8%80%E4%BA%9B%E6%95%B0%E6%8D%AE"><span class="toc-number">1.1.1.1.1.</span> <span class="toc-text">urlopen() 带 data 参数: 用 POST 发送一些数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#urlopen-%E5%B8%A6-timeout-%E5%8F%82%E6%95%B0-%E5%A6%82%E6%9E%9C%E8%AF%B7%E6%B1%82%E8%B6%85%E5%87%BA%E4%BA%86%E8%AE%BE%E7%BD%AE%E7%9A%84%E8%BF%99%E4%B8%AA%E6%97%B6%E9%97%B4%EF%BC%8C%E8%BF%98%E6%B2%A1%E6%9C%89%E5%BE%97%E5%88%B0%E5%93%8D%E5%BA%94%EF%BC%8C%E5%B0%B1%E4%BC%9A%E6%8A%9B%E5%87%BA%E5%BC%82%E5%B8%B8%E3%80%82"><span class="toc-number">1.1.1.1.2.</span> <span class="toc-text">urlopen() 带 timeout 参数: 如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常。</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Request-%E7%B1%BB%E6%9E%84%E5%BB%BA-Headers"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">Request 类构建 Headers</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Request%E7%B1%BB-%E7%9A%84%E6%9E%84%E5%BB%BA%E5%8F%82%E6%95%B0"><span class="toc-number">1.1.1.2.1.</span> <span class="toc-text">Request类 的构建参数</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">高级用法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Handler"><span class="toc-number">1.1.1.3.1.</span> <span class="toc-text">Handler</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Opener"><span class="toc-number">1.1.1.3.2.</span> <span class="toc-text">Opener</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%84%E7%90%86-HTTP-%E5%9F%BA%E6%9C%AC%E8%AE%A4%E8%AF%81"><span class="toc-number">1.1.1.3.3.</span> <span class="toc-text">处理 HTTP 基本认证</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-%E4%BB%A3%E7%90%86"><span class="toc-number">1.1.1.3.4.</span> <span class="toc-text">使用 代理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%84%E7%90%86-Cookies"><span class="toc-number">1.1.1.3.5.</span> <span class="toc-text">处理 Cookies</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96-Cookies"><span class="toc-number">1.1.1.3.5.1.</span> <span class="toc-text">获取 Cookies</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%8F%96%E7%94%A8-Cookies"><span class="toc-number">1.1.1.3.5.2.</span> <span class="toc-text">取用 Cookies</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#urllib-error-%E5%A4%84%E7%90%86%E5%BC%82%E5%B8%B8"><span class="toc-number">1.1.2.</span> <span class="toc-text">urllib.error 处理异常</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#URLError"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">URLError:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HTTPError"><span class="toc-number">1.1.2.2.</span> <span class="toc-text">HTTPError</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%BC%E5%90%88%E4%BD%BF%E7%94%A8%EF%BC%9A"><span class="toc-number">1.1.2.3.</span> <span class="toc-text">综合使用：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#urllib-parse-%E8%A7%A3%E6%9E%90%E9%93%BE%E6%8E%A5"><span class="toc-number">1.1.3.</span> <span class="toc-text">urllib.parse 解析链接</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#quote-%E5%B0%86%E5%86%85%E5%AE%B9%E8%BD%AC%E5%8C%96%E4%B8%BA-URL-%E7%BC%96%E7%A0%81%E7%9A%84%E6%A0%BC%E5%BC%8F"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">quote() 将内容转化为 URL 编码的格式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#urlparse-URL%E7%9A%84%E8%AF%86%E5%88%AB%E5%92%8C%E5%88%86%E6%AE%B5"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">urlparse() URL的识别和分段</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#urlunparse-%E5%90%88%E6%88%90URL"><span class="toc-number">1.1.3.3.</span> <span class="toc-text">urlunparse() 合成URL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#urlencode-%E5%AD%97%E5%85%B8%E7%B1%BB%E5%9E%8B-%E8%BD%AC%E5%8C%96%E4%B8%BA-GET%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0"><span class="toc-number">1.1.3.4.</span> <span class="toc-text">urlencode()  字典类型 转化为 GET请求参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#parse-qs-GET%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0-%E8%BD%AC%E5%8C%96%E4%B8%BA-%E5%AD%97%E5%85%B8%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.1.3.5.</span> <span class="toc-text">parse_qs()  GET请求参数 转化为 字典类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">1.1.3.6.</span> <span class="toc-text">其他</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#urllib-robotparser-%E5%88%86%E6%9E%90Robots%E5%8D%8F%E8%AE%AE"><span class="toc-number">1.1.4.</span> <span class="toc-text">urllib.robotparser 分析Robots协议</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Robots%E5%8D%8F%E8%AE%AE"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">Robots协议</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E6%9E%90-Robots-txt-%E2%80%94-urllib-robotparser-RobotFileParser-%E7%B1%BB"><span class="toc-number">1.1.4.2.</span> <span class="toc-text">解析 Robots.txt —- urllib.robotparser.RobotFileParser 类</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#RobotFileParser-%E5%AE%9E%E4%BE%8B%E7%9A%84%E6%9E%84%E9%80%A0"><span class="toc-number">1.1.4.2.1.</span> <span class="toc-text">RobotFileParser 实例的构造</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#RobotFileParser-%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">1.1.4.2.2.</span> <span class="toc-text">RobotFileParser 常用方法</span></a></li></ol></li></ol></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        python请求库urllib
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">clownote</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-02-11T22:49:55.000Z" itemprop="datePublished">2019-02-11</time>
        
        (Updated: <time datetime="2021-01-14T04:02:37.309Z" itemprop="dateModified">2021-01-14</time>)
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Crawler/">Crawler</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Crawler/" rel="tag">Crawler</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="爬虫请求库的使用之Urllib"><a href="#爬虫请求库的使用之Urllib" class="headerlink" title="爬虫请求库的使用之Urllib"></a>爬虫请求库的使用之Urllib</h1><blockquote>
<p>本文主要介绍 <strong>Urllib 库</strong>。</p>
</blockquote>
<h2 id="urllib"><a href="#urllib" class="headerlink" title="urllib"></a>urllib</h2><p><code>urllib</code> 是 Python 内置的 HTTP 请求库。</p>
<p>urllib 有 request，error，parse，robotparser 四个模块。</p>
<h3 id="urllib-request-发送请求"><a href="#urllib-request-发送请求" class="headerlink" title="urllib.request 发送请求"></a>urllib.request 发送请求</h3><h4 id="urlopen-发送请求"><a href="#urlopen-发送请求" class="headerlink" title="urlopen(): 发送请求"></a><code>urlopen()</code>: 发送请求</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取 python 官网 HTML 源码</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.python.org&#x27;</span>)</span><br><span class="line"></span><br><span class="line">print(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))  <span class="comment"># 查看response内容</span></span><br><span class="line">print(<span class="built_in">type</span>(response))                   <span class="comment"># 查看response类型</span></span><br><span class="line">print(response.status)                  <span class="comment"># 查看response状态码</span></span><br><span class="line">print(response.getheaders())            <span class="comment"># 查看响应头</span></span><br><span class="line">print(response.getheader(<span class="string">&#x27;Server&#x27;</span>))     <span class="comment"># 查看特定的响应头项</span></span><br></pre></td></tr></table></figure>
<p><code>urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)</code></p>
<h5 id="urlopen-带-data-参数-用-POST-发送一些数据"><a href="#urlopen-带-data-参数-用-POST-发送一些数据" class="headerlink" title="urlopen() 带 data 参数: 用 POST 发送一些数据"></a><code>urlopen()</code> 带 <code>data</code> 参数: 用 POST 发送一些数据</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">data = <span class="built_in">bytes</span>(</span><br><span class="line">    urllib.parse.urlencode(&#123;<span class="string">&#x27;Hello&#x27;</span>: <span class="string">&#x27;World&#x27;</span>&#125;),</span><br><span class="line">    encoding=<span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    )</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://httpbin.org/post&#x27;</span>, data=data)</span><br><span class="line">print(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h5 id="urlopen-带-timeout-参数-如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常。"><a href="#urlopen-带-timeout-参数-如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常。" class="headerlink" title="urlopen() 带 timeout 参数: 如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常。"></a><code>urlopen()</code> 带 <code>timeout</code> 参数: 如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常。</h5><p>设置<code>timeout=Sec</code>，Sce 为超时的秒数（可以为小数）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">&#x27;http://httpbin.org/get&#x27;</span>, timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br><span class="line">        </span><br><span class="line">print(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h4 id="Request-类构建-Headers"><a href="#Request-类构建-Headers" class="headerlink" title="Request 类构建 Headers"></a>Request 类构建 Headers</h4><p>利用更强大的 Request类来构建一个完整的请求。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(<span class="string">&#x27;http://python.org&#x27;</span>)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">print(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h5 id="Request类-的构建参数"><a href="#Request类-的构建参数" class="headerlink" title="Request类 的构建参数"></a>Request类 的构建参数</h5><p><code>class urllib.request.Request(ur1, data=None, headers=&#123;&#125;,origin_req_host=None, unverifiable=False, method=None)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://httpbin.org/post&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.3 Safari/605.1.15&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Host&#x27;</span>: <span class="string">&#x27;httpbin.org&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">dic = &#123;</span><br><span class="line">        <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;CDFMLR&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">data = <span class="built_in">bytes</span>(parse.urlencode(dic), encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">req = request.Request(url=url, data=data, headers=headers, method=<span class="string">&#x27;POST&#x27;</span>)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line">print(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h4 id="高级用法"><a href="#高级用法" class="headerlink" title="高级用法"></a>高级用法</h4><h5 id="Handler"><a href="#Handler" class="headerlink" title="Handler"></a>Handler</h5><p>Handler 包含 各种处理器，有专门处理登录验证的，有处理 Cookies 的，有处理代理设置的…</p>
<p>urllib.request 模块里的 <code>BaseHandler</code> 类是所有其他 Handler 的父类，它提供最基本的方法。</p>
<p>其他的 Handler 详见 <a target="_blank" rel="noopener" href="https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler">Handler</a></p>
<h5 id="Opener"><a href="#Opener" class="headerlink" title="Opener"></a>Opener</h5><p>之前用过 urlopen()方法，实际上就是 urllib 提供的一个 Opener。 </p>
<p>urlopen() 相当于类库封装好了极其常用的请求方法，利用它可以完成基本的请求，但如果需要实现更高级的功能，就需要深入一层进行配置，使用更底层的实例来完成操作，就用到了 Opener。 </p>
<p>Opener 可以使用 open()方法，返回的类型和 urlopen()如出一辙。 </p>
<p>我们需要利用 Handler来构建 Opener。 </p>
<hr>
<p>（以下是几个 Hander &amp; Opener 的应用）</p>
<h5 id="处理-HTTP-基本认证"><a href="#处理-HTTP-基本认证" class="headerlink" title="处理 HTTP 基本认证"></a>处理 <code>HTTP 基本认证</code></h5><p><code>HTTP基本认证</code>：</p>
<blockquote>
<p>有一种 web 登录方式不是通过 <code>cookie</code>，而是把 <code>用户名:密码</code> 用 <code>base64</code> 编码之后的字符串放在 request 中的 <code>header Authorization</code> 中发送给服务端。<br>当打开网页提示需要输入账号和密码时，假设密码/账号错误，服务器会返回401错误。</p>
</blockquote>
<p>这种网站在打开时就会弹出提示框，直接提示输入用户名和密码，验证成功后才能查看页面，我们现在来处理这种页面。</p>
<p>要请求这种页面，可以借用 <code>HTTPBasicAuthHandler</code> 完成：</p>
<p><em>在我们打算自己实现一个使用 http基本认证 的示例网站时，我们发现了 <a target="_blank" rel="noopener" href="http://pythonscraping.com/pages/auth/login.php">一个很好的例子</a>，所以，我们现在来爬取它。</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"></span><br><span class="line">username = <span class="string">&#x27;username&#x27;</span></span><br><span class="line">password = <span class="string">&#x27;password&#x27;</span></span><br><span class="line">url = <span class="string">&#x27;http://pythonscraping.com/pages/auth/login.php&#x27;</span></span><br><span class="line"></span><br><span class="line">p = HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line">p.add_password(<span class="literal">None</span>, url, username, password)</span><br><span class="line">auth_handler = HTTPBasicAuthHandler(p)</span><br><span class="line">opener = build_opener(auth_handler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = opener.<span class="built_in">open</span>(url)</span><br><span class="line">    html = result.read().decode(<span class="string">&#x27;utf-8&#x27;</span>, errors=<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    print(html)</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<p>在此，我们也想附上这种 <code>http基本认证</code> 的 php 实现，以供实践：</p>
<figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line">  <span class="keyword">if</span> (!<span class="keyword">isset</span>(<span class="variable">$_SERVER</span>[<span class="string">&#x27;PHP_AUTH_USER&#x27;</span>])) &#123;</span><br><span class="line">    header(<span class="string">&#x27;WWW-Authenticate: Basic realm=&quot;My Realm&quot;&#x27;</span>);</span><br><span class="line">    header(<span class="string">&#x27;HTTP/1.0 401 Unauthorized&#x27;</span>);</span><br><span class="line">    <span class="keyword">echo</span> <span class="string">&#x27;Text to send if user hits Cancel button&#x27;</span>;</span><br><span class="line">    <span class="keyword">exit</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">echo</span> <span class="string">&quot;&lt;p&gt;Hello <span class="subst">&#123;$_SERVER[&#x27;PHP_AUTH_USER&#x27;]&#125;</span>.&lt;/p&gt;&quot;</span>;</span><br><span class="line">    <span class="keyword">echo</span> <span class="string">&quot;&lt;p&gt;You entered <span class="subst">&#123;$_SERVER[&#x27;PHP_AUTH_PW&#x27;]&#125;</span> as your password.&lt;/p&gt;&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="meta">?&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="使用-代理"><a href="#使用-代理" class="headerlink" title="使用 代理"></a>使用 <code>代理</code></h5><p> 尝试找一个<a target="_blank" rel="noopener" href="https://www.xicidaili.com/nn/">免费的代理服务器</a>，然后用这个服务器来代理：</p>
<p> <em>寻找可用代理的时候可以尝试在浏览器中访问代理服务器的 IP:Port，可用的是会出现结果的:)</em></p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> ProxyHandler, build_opener</span><br><span class="line"></span><br><span class="line">proxy_handler = ProxyHandler(&#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://171.41.80.197:9999&#x27;</span>,        <span class="comment"># 用 ‘&lt;http||https&gt;://代理服务器_IP:代理服务器_端口’</span></span><br><span class="line">    <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;https://171.41.80.197:9999&#x27;</span></span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)  <span class="comment"># 要通过代理爬取的网页</span></span><br><span class="line">    print(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>, errors=<span class="string">&#x27;ignore&#x27;</span>))</span><br><span class="line">    print(response.getheaders())</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<h5 id="处理-Cookies"><a href="#处理-Cookies" class="headerlink" title="处理 Cookies"></a>处理 Cookies</h5><h6 id="获取-Cookies"><a href="#获取-Cookies" class="headerlink" title="获取 Cookies"></a>获取 Cookies</h6><p>获取 <code>http://www.baidu.com</code> 的 Cookies，依次打印出 name = value：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.CookieJar()</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"></span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name, <span class="string">&#x27;=&#x27;</span>, item.value)</span><br></pre></td></tr></table></figure>
<p>还是获取 <code>http://www.baidu.com</code> 的 Cookies，把它们保存到文件中（正如实际上 Cookies 的保存方式那样）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">filename = <span class="string">&#x27;cookies_from_baidu.txt&#x27;</span></span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar(filename)      <span class="comment"># 用 MozillaCookieJar 类把 Cookies 储存为 Mozilla 型浏览器的格式。</span></span><br><span class="line"><span class="comment"># cookie = http.cookiejar.LWPCookieJar(filename)         # 用这行代替上一行，可以把 Cookies 储存为 libwww-perl(LWP) 的格式。</span></span><br><span class="line"></span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"></span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"></span><br><span class="line">cookie.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><code>MozillaCookieJar</code> 和 <code>LWPCookieJar</code> 都可以<strong>读取</strong>和<strong>保存</strong> Cookies。只是它们的保存格式有别。</p>
<h6 id="取用-Cookies"><a href="#取用-Cookies" class="headerlink" title="取用 Cookies"></a>取用 Cookies</h6><p>取用我们刚才保存下来的 Cookies：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar()</span><br><span class="line">cookie.load(<span class="string">&#x27;cookies_from_baidu.txt&#x27;</span>, ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"></span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line">print(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>, errors=<span class="string">&#x27;ignore&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h3 id="urllib-error-处理异常"><a href="#urllib-error-处理异常" class="headerlink" title="urllib.error 处理异常"></a>urllib.error 处理异常</h3><h4 id="URLError"><a href="#URLError" class="headerlink" title="URLError:"></a>URLError:</h4><p><code>URLError</code> 类来自 <code>Urllib</code> 库的 <code>error</code> 模块，它继承自 <code>OSError</code> 类，是 error 异常模块的基类，由 request 模块生的异常都可以通过捕获这个类来处理。</p>
<p>它具有一个属性 <code>reason</code>，即返回错误的原因:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;http://www.google.com&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<p>通过如上操作，我们就可以避免程序异常终止，同时异常得到了有效处理。</p>
<h4 id="HTTPError"><a href="#HTTPError" class="headerlink" title="HTTPError"></a>HTTPError</h4><p><code>HTTPError</code> 是 <code>URLError</code> 的子类。用来处理 <strong>HTTP 请求错误</strong>，比如认证请求失败等等。</p>
<p><code>HTTPError</code> 的属性：</p>
<ul>
<li><code>code</code>，返回 HTTP Status Code，即状态码，比如 404 网页不存在，500 服务器内部错误等等。</li>
<li><code>reason</code>，同父类一样，返回错误的原因。</li>
<li><code>headers</code>，返回 Request Headers。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;http://cr0123.gz01.bdysite.com/no_such_a_page.html&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">&#x27;\n[Reason]&#x27;</span>, e.reason,</span><br><span class="line">        <span class="string">&#x27;\n[code]&#x27;</span>, e.code,</span><br><span class="line">        <span class="string">&#x27;\n[headers]&#x27;</span>, e.headers</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h4 id="综合使用："><a href="#综合使用：" class="headerlink" title="综合使用："></a>综合使用：</h4><p>在实际的使用过程中，我们可以先选择捕获子类的错误，再去捕获父类的错误，从而使处理完整，层次清晰。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;http://www.google.com&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">&#x27;HTTPError:&#x27;</span>)</span><br><span class="line">    print(<span class="string">&#x27;[Reason]&#x27;</span>, e.reason, <span class="string">&#x27;[code]&#x27;</span>, e.code, <span class="string">&#x27;[headers]&#x27;</span>, e.headers, sep=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">&#x27;URLError:&#x27;</span>)</span><br><span class="line">    print(<span class="string">&#x27;[Error Reason]\n&#x27;</span>, e.reason)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">&#x27;Exception:&#x27;</span>)</span><br><span class="line">    print(e)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">&#x27;Request Successfully&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="urllib-parse-解析链接"><a href="#urllib-parse-解析链接" class="headerlink" title="urllib.parse 解析链接"></a>urllib.parse 解析链接</h3><p><code>urllib.parse</code> 模块定义了处理 URL 的标准接口，例如实现 URL 各部分的抽取，合并以及链接转换。</p>
<h4 id="quote-将内容转化为-URL-编码的格式"><a href="#quote-将内容转化为-URL-编码的格式" class="headerlink" title="quote() 将内容转化为 URL 编码的格式"></a>quote() 将内容转化为 URL 编码的格式</h4><p>可以用来把中文字符转化为 URL 编码：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>quote(<span class="string">&#x27;中文&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;%E4%B8%AD%E6%96%87&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>url = <span class="string">&#x27;https://www.baidu.com/s?wd=&#x27;</span> + quote(<span class="string">&#x27;URL 编码&#x27;</span>)    <span class="comment"># 合成的网址即可百度搜索‘URL 编码’</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(url)</span><br><span class="line">https://www.baidu.com/s?wd=URL%20%E7%BC%96%E7%A0%81</span><br></pre></td></tr></table></figure>
<p>要解码，可是使用对应的 <code>unquote()</code>。</p>
<h4 id="urlparse-URL的识别和分段"><a href="#urlparse-URL的识别和分段" class="headerlink" title="urlparse() URL的识别和分段"></a>urlparse() URL的识别和分段</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">&#x27;http://www.baidu.com/index.html;user?id=5#comment&#x27;</span>)</span><br><span class="line">print(<span class="built_in">type</span>(result))</span><br><span class="line">print(result)</span><br><span class="line">print(result.netloc)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;(results)</span></span><br><span class="line"><span class="string">&lt;class &#x27;urllib.parse.ParseResult&#x27;&gt;</span></span><br><span class="line"><span class="string">ParseResult(scheme=&#x27;http&#x27;, netloc=&#x27;www.baidu.com&#x27;, path=&#x27;/index.html&#x27;, params=&#x27;user&#x27;, query=&#x27;id=5&#x27;, fragment=&#x27;comment&#x27;)</span></span><br><span class="line"><span class="string">www.baidu.com</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>ParseResult 包含：</p>
<ul>
<li>scheme ———— 协议</li>
<li>netloc ———— 主机名</li>
<li>path ———— 路径</li>
<li>params ———— <code>;XXX</code></li>
<li>query ———— <code>?XXX</code></li>
<li>fragment ———— <code>#XXX</code></li>
</ul>
<h4 id="urlunparse-合成URL"><a href="#urlunparse-合成URL" class="headerlink" title="urlunparse() 合成URL"></a>urlunparse() 合成URL</h4><p><strong>依次</strong>传入六个部分：(scheme, netloc, path, params, query, fragment)，<br>将合成一个: <code>scheme://netloc/path;params?query#fragment</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunparse</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = [<span class="string">&#x27;https&#x27;</span>, <span class="string">&#x27;www.baidu.com&#x27;</span>, <span class="string">&#x27;/index.php&#x27;</span>, <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;id=5&#x27;</span>, <span class="string">&#x27;123&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>urlunparse(data)</span><br><span class="line"><span class="string">&#x27;https://www.baidu.com/index.php;user?id=5#123&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="urlencode-字典类型-转化为-GET请求参数"><a href="#urlencode-字典类型-转化为-GET请求参数" class="headerlink" title="urlencode()  字典类型 转化为 GET请求参数"></a>urlencode()  字典类型 转化为 GET请求参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;germey&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>: <span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">&#x27;http://www.baidu.com?&#x27;</span></span><br><span class="line">url = base_url + urlencode(params)</span><br><span class="line">print(url)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;(result)</span></span><br><span class="line"><span class="string">http://www.baidu.com?name=germey&amp;age=22</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="parse-qs-GET请求参数-转化为-字典类型"><a href="#parse-qs-GET请求参数-转化为-字典类型" class="headerlink" title="parse_qs()  GET请求参数 转化为 字典类型"></a>parse_qs()  GET请求参数 转化为 字典类型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qs</span><br><span class="line"></span><br><span class="line">query = <span class="string">&#x27;name=germey&amp;age=22&#x27;</span></span><br><span class="line">print(parse_qs(query))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;(result)</span></span><br><span class="line"><span class="string">&#123;&#x27;name&#x27;: [&#x27;germey&#x27;], &#x27;age&#x27;: [&#x27;22&#x27;]&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>另有一个 <code>parse_qsl()</code> 与 <code>parse_qs()</code> 类似，只是得到的结果是 元组</p>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>这个库中还有一些 <code>urljoin</code> 等方式，可以在 <a target="_blank" rel="noopener" href="https://germey.gitbooks.io/python3webspider/content/3.1.3-%E8%A7%A3%E6%9E%90%E9%93%BE%E6%8E%A5.html">这里</a> 查看。</p>
<h3 id="urllib-robotparser-分析Robots协议"><a href="#urllib-robotparser-分析Robots协议" class="headerlink" title="urllib.robotparser 分析Robots协议"></a>urllib.robotparser 分析Robots协议</h3><h4 id="Robots协议"><a href="#Robots协议" class="headerlink" title="Robots协议"></a>Robots协议</h4><p>网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。<br>它通常是一个叫做 robots.txt 的文本文件，放在网站的根目录下。</p>
<p>Robots协议的写法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">User-agent: *               这里的*代表的所有的搜索引擎种类，*是一个通配符</span><br><span class="line">Disallow: &#x2F;admin&#x2F;           这里定义是禁止爬寻admin目录下面的目录</span><br><span class="line">Disallow: &#x2F;require&#x2F;         这里定义是禁止爬寻require目录下面的目录</span><br><span class="line">Disallow: &#x2F;ABC&#x2F;             这里定义是禁止爬寻ABC目录下面的目录</span><br><span class="line">Disallow: &#x2F;cgi-bin&#x2F;*.htm    禁止访问&#x2F;cgi-bin&#x2F;目录下的所有以&quot;.htm&quot;为后缀的URL(包含子目录)。</span><br><span class="line">Disallow: &#x2F;*?*              禁止访问网站中所有包含问号 (?) 的网址</span><br><span class="line">Disallow: &#x2F;.jpg$            禁止抓取网页所有的.jpg格式的图片</span><br><span class="line">Disallow:&#x2F;ab&#x2F;adc.html       禁止爬取ab文件夹下面的adc.html文件。</span><br><span class="line">Allow: &#x2F;cgi-bin&#x2F;　          这里定义是允许爬寻cgi-bin目录下面的目录</span><br><span class="line">Allow: &#x2F;tmp                 这里定义是允许爬寻tmp的整个目录</span><br><span class="line">Allow: .htm$                仅允许访问以&quot;.htm&quot;为后缀的URL。</span><br><span class="line">Allow: .gif$                允许抓取网页和gif格式图片</span><br><span class="line">Sitemap:                    网站地图 告诉爬虫这个页面是网站地图</span><br></pre></td></tr></table></figure>
<p>使用例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">禁止所有搜索引擎访问网站的任何部分</span><br><span class="line">User-agent: *</span><br><span class="line">Disallow: &#x2F;</span><br><span class="line"></span><br><span class="line">允许所有的robot访问</span><br><span class="line">User-agent: *</span><br><span class="line">Allow:　&#x2F;</span><br><span class="line"></span><br><span class="line">允许某个搜索引擎的访问</span><br><span class="line">User-agent: Baiduspider</span><br><span class="line">allow:&#x2F;</span><br></pre></td></tr></table></figure>
<h4 id="解析-Robots-txt-—-urllib-robotparser-RobotFileParser-类"><a href="#解析-Robots-txt-—-urllib-robotparser-RobotFileParser-类" class="headerlink" title="解析 Robots.txt —- urllib.robotparser.RobotFileParser 类"></a>解析 Robots.txt —- <code>urllib.robotparser.RobotFileParser</code> 类</h4><p>robotparser 模块提供了一个 <code>RobotFileParser</code> 类。它可以根据某网站的 robots.txt 文件来判断一个爬取爬虫是否有权限来爬取这个网页。</p>
<h5 id="RobotFileParser-实例的构造"><a href="#RobotFileParser-实例的构造" class="headerlink" title="RobotFileParser 实例的构造"></a><code>RobotFileParser</code> 实例的构造</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.robotparser.RobotFileParser(url&#x3D;&#39;&#39;)      # 传入 robots.txt 的链接</span><br></pre></td></tr></table></figure>

<h5 id="RobotFileParser-常用方法"><a href="#RobotFileParser-常用方法" class="headerlink" title="RobotFileParser 常用方法"></a><code>RobotFileParser</code> 常用方法</h5><ul>
<li><code>set_url(url)</code> 用来设置 robots.txt 的链接</li>
<li><code>read()</code> 读取 robots.txt 文件并进行分析。<br>⚠️【注意】如果不 read()，所有的判断函数都会返回 False！</li>
<li><code>parse()</code> 用来解析 robots.txt 文件，传入的参数是 robots.txt 某些行的内容，它会按照 robots.txt 的语法规则来分析这些内容。</li>
<li><code>can_fetch()</code> 传入两个参数，第一个是 User-agent，第二个是要抓取的 URL，返回的内容是该搜索引擎是否可以抓取这个 URL，返回结果是 True 或 False。</li>
<li><code>mtime()</code>，返回的是上次抓取和分析 robots.txt 的时间，这个对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的 robots.txt。</li>
<li><code>modified()</code>，同样的对于长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析 robots.txt 的时间。</li>
</ul>

  </div>
</article>
<!--Disqus-->


<!--Livere-->

    <div class="blog-post-comments">
        <div id="lv-container" data-id="city" data-uid="MTAyMC80NjEzMi8yMjY0Mw==">
            <noscript>不启用 JavaScript 支持的人是看不到可爱的评论区的。😥</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E8%AF%B7%E6%B1%82%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8%E4%B9%8BUrllib"><span class="toc-number">1.</span> <span class="toc-text">爬虫请求库的使用之Urllib</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#urllib"><span class="toc-number">1.1.</span> <span class="toc-text">urllib</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#urllib-request-%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82"><span class="toc-number">1.1.1.</span> <span class="toc-text">urllib.request 发送请求</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#urlopen-%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">urlopen(): 发送请求</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#urlopen-%E5%B8%A6-data-%E5%8F%82%E6%95%B0-%E7%94%A8-POST-%E5%8F%91%E9%80%81%E4%B8%80%E4%BA%9B%E6%95%B0%E6%8D%AE"><span class="toc-number">1.1.1.1.1.</span> <span class="toc-text">urlopen() 带 data 参数: 用 POST 发送一些数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#urlopen-%E5%B8%A6-timeout-%E5%8F%82%E6%95%B0-%E5%A6%82%E6%9E%9C%E8%AF%B7%E6%B1%82%E8%B6%85%E5%87%BA%E4%BA%86%E8%AE%BE%E7%BD%AE%E7%9A%84%E8%BF%99%E4%B8%AA%E6%97%B6%E9%97%B4%EF%BC%8C%E8%BF%98%E6%B2%A1%E6%9C%89%E5%BE%97%E5%88%B0%E5%93%8D%E5%BA%94%EF%BC%8C%E5%B0%B1%E4%BC%9A%E6%8A%9B%E5%87%BA%E5%BC%82%E5%B8%B8%E3%80%82"><span class="toc-number">1.1.1.1.2.</span> <span class="toc-text">urlopen() 带 timeout 参数: 如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常。</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Request-%E7%B1%BB%E6%9E%84%E5%BB%BA-Headers"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">Request 类构建 Headers</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Request%E7%B1%BB-%E7%9A%84%E6%9E%84%E5%BB%BA%E5%8F%82%E6%95%B0"><span class="toc-number">1.1.1.2.1.</span> <span class="toc-text">Request类 的构建参数</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">高级用法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Handler"><span class="toc-number">1.1.1.3.1.</span> <span class="toc-text">Handler</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Opener"><span class="toc-number">1.1.1.3.2.</span> <span class="toc-text">Opener</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%84%E7%90%86-HTTP-%E5%9F%BA%E6%9C%AC%E8%AE%A4%E8%AF%81"><span class="toc-number">1.1.1.3.3.</span> <span class="toc-text">处理 HTTP 基本认证</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-%E4%BB%A3%E7%90%86"><span class="toc-number">1.1.1.3.4.</span> <span class="toc-text">使用 代理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%84%E7%90%86-Cookies"><span class="toc-number">1.1.1.3.5.</span> <span class="toc-text">处理 Cookies</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96-Cookies"><span class="toc-number">1.1.1.3.5.1.</span> <span class="toc-text">获取 Cookies</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%8F%96%E7%94%A8-Cookies"><span class="toc-number">1.1.1.3.5.2.</span> <span class="toc-text">取用 Cookies</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#urllib-error-%E5%A4%84%E7%90%86%E5%BC%82%E5%B8%B8"><span class="toc-number">1.1.2.</span> <span class="toc-text">urllib.error 处理异常</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#URLError"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">URLError:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HTTPError"><span class="toc-number">1.1.2.2.</span> <span class="toc-text">HTTPError</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%BC%E5%90%88%E4%BD%BF%E7%94%A8%EF%BC%9A"><span class="toc-number">1.1.2.3.</span> <span class="toc-text">综合使用：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#urllib-parse-%E8%A7%A3%E6%9E%90%E9%93%BE%E6%8E%A5"><span class="toc-number">1.1.3.</span> <span class="toc-text">urllib.parse 解析链接</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#quote-%E5%B0%86%E5%86%85%E5%AE%B9%E8%BD%AC%E5%8C%96%E4%B8%BA-URL-%E7%BC%96%E7%A0%81%E7%9A%84%E6%A0%BC%E5%BC%8F"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">quote() 将内容转化为 URL 编码的格式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#urlparse-URL%E7%9A%84%E8%AF%86%E5%88%AB%E5%92%8C%E5%88%86%E6%AE%B5"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">urlparse() URL的识别和分段</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#urlunparse-%E5%90%88%E6%88%90URL"><span class="toc-number">1.1.3.3.</span> <span class="toc-text">urlunparse() 合成URL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#urlencode-%E5%AD%97%E5%85%B8%E7%B1%BB%E5%9E%8B-%E8%BD%AC%E5%8C%96%E4%B8%BA-GET%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0"><span class="toc-number">1.1.3.4.</span> <span class="toc-text">urlencode()  字典类型 转化为 GET请求参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#parse-qs-GET%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0-%E8%BD%AC%E5%8C%96%E4%B8%BA-%E5%AD%97%E5%85%B8%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.1.3.5.</span> <span class="toc-text">parse_qs()  GET请求参数 转化为 字典类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">1.1.3.6.</span> <span class="toc-text">其他</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#urllib-robotparser-%E5%88%86%E6%9E%90Robots%E5%8D%8F%E8%AE%AE"><span class="toc-number">1.1.4.</span> <span class="toc-text">urllib.robotparser 分析Robots协议</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Robots%E5%8D%8F%E8%AE%AE"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">Robots协议</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E6%9E%90-Robots-txt-%E2%80%94-urllib-robotparser-RobotFileParser-%E7%B1%BB"><span class="toc-number">1.1.4.2.</span> <span class="toc-text">解析 Robots.txt —- urllib.robotparser.RobotFileParser 类</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#RobotFileParser-%E5%AE%9E%E4%BE%8B%E7%9A%84%E6%9E%84%E9%80%A0"><span class="toc-number">1.1.4.2.1.</span> <span class="toc-text">RobotFileParser 实例的构造</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#RobotFileParser-%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">1.1.4.2.2.</span> <span class="toc-text">RobotFileParser 常用方法</span></a></li></ol></li></ol></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&text=python请求库urllib"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&title=python请求库urllib"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&is_video=false&description=python请求库urllib"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=python请求库urllib&body=Check out this article: https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&title=python请求库urllib"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&title=python请求库urllib"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&title=python请求库urllib"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&title=python请求库urllib"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/02/11/PythonAndCrawler/crawler-2-urllib/&name=python请求库urllib&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2021 CDFMLR
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">项目</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-146911386-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?9a0d2e6fde93dad496ac79f04f3aba97";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->


<!--Livere Comments-->

    <script type="text/javascript">
      (function (d, s) {
        var j, e = d.getElementsByTagName(s)[0];

        if (typeof LivereTower === 'function') { return; }

        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;

        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>

</body>
</html>
