<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Notes of Andrew Ngâ€™s Machine Learning â€”â€” (3) Multivariate Linear RegressionMultiple FeaturesLinear regression with multiple variables is alse known as â€œmultivariate linear regressionâ€œ. NotationsWe now">
<meta property="og:type" content="article">
<meta property="og:title" content="å¤šå˜é‡çº¿æ€§å›å½’">
<meta property="og:url" content="https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/index.html">
<meta property="og:site_name" content="clownote">
<meta property="og:description" content="Notes of Andrew Ngâ€™s Machine Learning â€”â€” (3) Multivariate Linear RegressionMultiple FeaturesLinear regression with multiple variables is alse known as â€œmultivariate linear regressionâ€œ. NotationsWe now">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g6ljh8xnnuj30g508ttam.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g6mi7c257sj30eg07wt9v.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g6mi84pb5bj30e807pjsd.jpg">
<meta property="article:published_time" content="2019-09-05T17:11:21.000Z">
<meta property="article:modified_time" content="2022-06-15T09:00:04.587Z">
<meta property="article:author" content="CDFMLR">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/006y8mN6ly1g6ljh8xnnuj30g508ttam.jpg">
    
    
        
          
              <link rel="shortcut icon" href="/images/rabbit.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/rabbit_192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/rabbit_180.png">
          
        
    
    <!-- title -->
    <title>å¤šå˜é‡çº¿æ€§å›å½’</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
    <!--Google search varification (PRIVATE)-->
    <meta name="google-site-verification" content="MrqlpFAD8nDanw3Ypv7ZsIWHLnTdhRuLa4QhSVwxIvc" />
    <!--Google AdSense å…³è” (PRIVATE)-->
    <script data-ad-client="ca-pub-1510963483941114" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<meta name="generator" content="Hexo 5.4.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">é¦–é¡µ</a></li>
         
          <li><a href="/about/">å…³äº</a></li>
         
          <li><a href="/archives/">å½’æ¡£</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">é¡¹ç›®</a></li>
         
          <li><a href="/search/">æœç´¢</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2019/09/07/AndrewNgML/AndrewNg-MachineLearning-4-NormalEquation/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/09/02/AndrewNgML/AndrewNg-MachineLearning-2-LinearAlgebraReview/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">ä¸Šä¸€ç¯‡</span>
      <span id="i-next" class="info" style="display:none;">ä¸‹ä¸€ç¯‡</span>
      <span id="i-top" class="info" style="display:none;">è¿”å›é¡¶éƒ¨</span>
      <span id="i-share" class="info" style="display:none;">åˆ†äº«æ–‡ç« </span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&text=å¤šå˜é‡çº¿æ€§å›å½’"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&title=å¤šå˜é‡çº¿æ€§å›å½’"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&is_video=false&description=å¤šå˜é‡çº¿æ€§å›å½’"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=å¤šå˜é‡çº¿æ€§å›å½’&body=Check out this article: https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&title=å¤šå˜é‡çº¿æ€§å›å½’"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&title=å¤šå˜é‡çº¿æ€§å›å½’"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&title=å¤šå˜é‡çº¿æ€§å›å½’"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&title=å¤šå˜é‡çº¿æ€§å›å½’"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&name=å¤šå˜é‡çº¿æ€§å›å½’&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Notes-of-Andrew-Ng%E2%80%99s-Machine-Learning-%E2%80%94%E2%80%94-3-Multivariate-Linear-Regression"><span class="toc-number">1.</span> <span class="toc-text">Notes of Andrew Ngâ€™s Machine Learning â€”â€” (3) Multivariate Linear Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Multiple-Features"><span class="toc-number">1.1.</span> <span class="toc-text">Multiple Features</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Notations"><span class="toc-number">1.1.1.</span> <span class="toc-text">Notations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hypothesis"><span class="toc-number">1.1.2.</span> <span class="toc-text">Hypothesis</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent-For-Multiple-Variables"><span class="toc-number">1.2.</span> <span class="toc-text">Gradient Descent For Multiple Variables</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Feature-Scaling"><span class="toc-number">1.3.</span> <span class="toc-text">Feature Scaling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Feature-scaling"><span class="toc-number">1.3.1.</span> <span class="toc-text">Feature scaling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mean-normalization"><span class="toc-number">1.3.2.</span> <span class="toc-text">Mean normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implement"><span class="toc-number">1.3.3.</span> <span class="toc-text">Implement</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#In-octave"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">In octave</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Learning-Rate"><span class="toc-number">1.4.</span> <span class="toc-text">Learning Rate</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Debugging-gradient-descent"><span class="toc-number">1.4.1.</span> <span class="toc-text">Debugging gradient descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Automatic-convergence-test"><span class="toc-number">1.4.2.</span> <span class="toc-text">Automatic convergence test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Making-sure-gradient-descent-is-working-correctly"><span class="toc-number">1.4.3.</span> <span class="toc-text">Making sure gradient descent is working correctly</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implement-1"><span class="toc-number">1.4.4.</span> <span class="toc-text">Implement</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Features-and-Polynomial-Regression"><span class="toc-number">1.5.</span> <span class="toc-text">Features and Polynomial Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Combine-Features"><span class="toc-number">1.5.1.</span> <span class="toc-text">Combine Features</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Polynomial-Regression"><span class="toc-number">1.5.2.</span> <span class="toc-text">Polynomial Regression</span></a></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        å¤šå˜é‡çº¿æ€§å›å½’
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">clownote</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-09-05T17:11:21.000Z" itemprop="datePublished">2019-09-05</time>
        
        (Updated: <time datetime="2022-06-15T09:00:04.587Z" itemprop="dateModified">2022-06-15</time>)
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a> â€º <a class="category-link" href="/categories/Machine-Learning/AndrewNg/">AndrewNg</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="Notes-of-Andrew-Ngâ€™s-Machine-Learning-â€”â€”-3-Multivariate-Linear-Regression"><a href="#Notes-of-Andrew-Ngâ€™s-Machine-Learning-â€”â€”-3-Multivariate-Linear-Regression" class="headerlink" title="Notes of Andrew Ngâ€™s Machine Learning â€”â€” (3) Multivariate Linear Regression"></a>Notes of Andrew Ngâ€™s Machine Learning â€”â€” (3) Multivariate Linear Regression</h1><h2 id="Multiple-Features"><a href="#Multiple-Features" class="headerlink" title="Multiple Features"></a>Multiple Features</h2><p>Linear regression with multiple variables is alse known as â€œ<strong>multivariate linear regression</strong>â€œ.</p>
<h3 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h3><p>We now introduce notation for equations where we can have any number of input variables (Multiple Features, i.e. Multivariate):</p>
<ul>
<li>$m$: the number of training examples.</li>
<li>$n$: the number of features.</li>
<li>$x^{(i)}$: the input (features) of the $i^{th}$ training example, a n-dimensional vector.</li>
<li>$x^{(i)}_j$: the value of feature $j$ in the $i^{th}$ training example.  </li>
</ul>
<h3 id="Hypothesis"><a href="#Hypothesis" class="headerlink" title="Hypothesis"></a>Hypothesis</h3><p>The multivariable form of the hypothesis function accommodating these multiple features is as follows:<br>$$<br>h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+ â€¦ + \theta_nx_n<br>$$</p>
<blockquote>
<p>In order to develop intuition about this function, we can think about $\theta_0$ as the basic price of a house, $\theta_1$ as the price per square meter, $\theta_2$ as the price per floor, etc. $x_1$ will be the number of square meters in the house, $x_2$ the number of floors, etc.</p>
</blockquote>
<p>Using the definition of matrix muyltiplication, our multivariable hypothesis function can be concisely represented as:<br>$$<br>h_\theta(x)=<br>\left[\begin{array}{c}\theta_0 &amp; \theta_1 &amp; \ldots &amp; \theta_n\end{array}\right]<br>\left[\begin{array}{c}x_0 \ x_1 \ \vdots \ x_n \end{array}\right]<br>=<br>\theta^Tx<br>$$<br>This is a vectorization of our hypothesis function for one training example.</p>
<p>Remark: Note that for convenience reasons, we assume <strong>$x^{(i)}_0=1 \quad \textrm{for(}i \in 1, â€¦,m \textrm{)}$</strong>. This allows us to do matrix operations with $\theta$ and $x$. Hence making two vector $\theta$ and $x^{(i)}$ match each other element-wise (that is, have the same number of elements: n+1).</p>
<h2 id="Gradient-Descent-For-Multiple-Variables"><a href="#Gradient-Descent-For-Multiple-Variables" class="headerlink" title="Gradient Descent For Multiple Variables"></a>Gradient Descent For Multiple Variables</h2><p>Letâ€™s say the condition about the multiple variables:</p>
<blockquote>
<p>Hypothesis: $h_\theta(x)=\sum_{i=0}^m \theta_ix_i$.</p>
<p>Parameters: $\theta_0, \theta_1, â€¦,\theta_n$.</p>
<p>Cost Function: $J(\theta_0, \theta_1, â€¦, \theta_n)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$.</p>
</blockquote>
<p>Or vectorizedly:</p>
<blockquote>
<p>Hypothesis: $h_\theta(x)=\theta^Tx$.</p>
<p>Parameters: $\theta$.</p>
<p>Cost Function: $J(\theta)=\frac{1}{2m}\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})^2$.</p>
</blockquote>
<p>The Gradient Descent will be like this:</p>
<blockquote>
<p>repeat until convergence {</p>
<p>$\qquad \theta_j := \theta_j - \alpha \frac{1}{m} \sum^m_{i=1}[h_\theta(x^{(i)})-y^{(i)}] \cdot x_j^{(i)}\qquad \textrm{for }j:=0, â€¦, n$</p>
<p>}</p>
</blockquote>
<p>The following image compares gradient descent with one variable to gradient descent with multiple variables:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6ljh8xnnuj30g508ttam.jpg" alt="img"></p>
<h2 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h2><p>We can speed up gradient descent by <strong>having each of our input values in roughly the same range</strong>. This is because $\theta$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p>
<p>The way to prevent this is to <strong>modify the ranges of our input variables</strong> so that they are all roughly the same. Ideally:<br>$$<br>\begin{array}{c}<br>-1 \le x_{i} \le 1\<br>\textrm{or}\<br>-0.5 \le x_{i} \le 0.5<br>\end{array}<br>$$<br>These arenâ€™t exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.</p>
<p>In practice, we offen think itâ€™s ok for variables in range $[-3,-\frac{1}{3}) \cup (+\frac{1}{3}, +3]$.</p>
<p>Two techniques to help with this are <code>feature scaling</code> and <code>mean normalization</code>.</p>
<h3 id="Feature-scaling"><a href="#Feature-scaling" class="headerlink" title="Feature scaling"></a>Feature scaling</h3><p>Feature scaling involves <strong>dividing the input values by <em>the range</em></strong> (i.e. the maximum value minus the minimum value) of input variable, resulting in a new range of just 1.<br>$$<br>\begin{array}{rl}<br>\textrm{Range:} &amp; s_i = max(x_i)-min(x_i)\<br>\textrm{Scaling:} &amp; x_i:=\frac{x_i}{s_i}<br>\end{array}<br>$$</p>
<h3 id="Mean-normalization"><a href="#Mean-normalization" class="headerlink" title="Mean normalization"></a>Mean normalization</h3><p>Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.<br>$$<br>\begin{array}{rl}<br>\textrm{Average:} &amp; \mu_i = \frac{sum(x_i)}{m}\<br>\textrm{Mean normalizing:} &amp; x_i:=x_i-\mu_i<br>\end{array}<br>$$</p>
<h3 id="Implement"><a href="#Implement" class="headerlink" title="Implement"></a>Implement</h3><p>We always implement both of these techniques via adjusting our input values as shown in this formula:<br>$$<br>x_i:=\frac{x_i-\mu_i}{s_i}<br>$$</p>
<ul>
<li><strong>$\mu_i$</strong> is the <strong>average of all the values for feature (i)</strong></li>
<li><strong>$s_i$</strong> is the <strong>range of values (<code>max - min</code>)</strong>, or $s_i$ could also be the <strong>standard deviation</strong>.</li>
</ul>
<p>For example, if $x_i$ represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, $x_i := \dfrac{price-1000}{1900}$.</p>
<h4 id="In-octave"><a href="#In-octave" class="headerlink" title="In octave"></a>In octave</h4><p>In octave, the function <code>mean</code> can offer us the avaerage of values for feature (i), when the function <code>std</code> gives us the standard deviation of values for feature (i). So we can program like this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">function [X_norm, mu, sigma] &#x3D; featureNormalize(X)</span><br><span class="line">%FEATURENORMALIZE Normalizes the features in X </span><br><span class="line">%   FEATURENORMALIZE(X) returns a normalized version of X where</span><br><span class="line">%   the mean value of each feature is 0 and the standard deviation</span><br><span class="line">%   is 1. This is often a good preprocessing step to do when</span><br><span class="line">%   working with learning algorithms.</span><br><span class="line"></span><br><span class="line">X_norm &#x3D; X;</span><br><span class="line">mu &#x3D; zeros(1, size(X, 2));</span><br><span class="line">sigma &#x3D; zeros(1, size(X, 2));</span><br><span class="line"></span><br><span class="line">% Instructions: First, for each feature dimension, compute the mean</span><br><span class="line">%               of the feature and subtract it from the dataset,</span><br><span class="line">%               storing the mean value in mu. Next, compute the </span><br><span class="line">%               standard deviation of each feature and divide</span><br><span class="line">%               each feature by it&#39;s standard deviation, storing</span><br><span class="line">%               the standard deviation in sigma. </span><br><span class="line">%</span><br><span class="line">%               Note that X is a matrix where each column is a </span><br><span class="line">%               feature and each row is an example. You need </span><br><span class="line">%               to perform the normalization separately for </span><br><span class="line">%               each feature. </span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">mu &#x3D; mean(X);</span><br><span class="line">sigma &#x3D; std(X);</span><br><span class="line">X_norm &#x3D; (X - mu) .&#x2F; sigma;</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h2><h3 id="Debugging-gradient-descent"><a href="#Debugging-gradient-descent" class="headerlink" title="Debugging gradient descent"></a>Debugging gradient descent</h3><p>Make a plot with <em>number of iterations</em> on the x-axis. Now plot the cost functiong $J(\theta)$ over the number of iterations of  gradient descent. If $J(\theta)$ ever increases, then you probably need to decrease learning rate $\alpha$.</p>
<h3 id="Automatic-convergence-test"><a href="#Automatic-convergence-test" class="headerlink" title="Automatic convergence test"></a>Automatic convergence test</h3><p>Declare convergence if $J(\theta)$ decreases by less than $\epsilon$, where $\epsilon$ is some small value such as $10^{-3}$. However in practice itâ€™s difficult to choose this threshold value.</p>
<h3 id="Making-sure-gradient-descent-is-working-correctly"><a href="#Making-sure-gradient-descent-is-working-correctly" class="headerlink" title="Making sure gradient descent is working correctly"></a>Making sure gradient descent is working correctly</h3><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6mi7c257sj30eg07wt9v.jpg" alt="img"></p>
<p>It has been proven that if learning rate Î± is sufficiently small, then J(Î¸) will decrease on every iteration.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6mi84pb5bj30e807pjsd.jpg" alt="img"></p>
<p>To summarize:</p>
<blockquote>
<p>If $\alpha$ is too small: slow convergence.</p>
<p>If $\alpha$ is too large: ï¿¼may not decrease on every iteration and thus may not converge.</p>
</blockquote>
<h3 id="Implement-1"><a href="#Implement-1" class="headerlink" title="Implement"></a>Implement</h3><p>We should try different $\alpha$ to find a fit one by drawing #iterations-J(Î¸) plots.</p>
<p>E.g. To choose $\alpha$, try:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">... , 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ...</span><br></pre></td></tr></table></figure>

<h2 id="Features-and-Polynomial-Regression"><a href="#Features-and-Polynomial-Regression" class="headerlink" title="Features and Polynomial Regression"></a>Features and Polynomial Regression</h2><h3 id="Combine-Features"><a href="#Combine-Features" class="headerlink" title="Combine Features"></a>Combine Features</h3><p>We can improve our features and the form of our hypothesis function in a couple different ways.</p>
<p>For example, we can <strong>combine</strong> multiple features into one, Such as combining $x_1$ and $x_2$ into a new feature $x_3$ by taking $x_1 \cdot x_2$.</p>
<h3 id="Polynomial-Regression"><a href="#Polynomial-Regression" class="headerlink" title="Polynomial Regression"></a>Polynomial Regression</h3><p>To fit the data well, our hypothesis function may need to be non-linear. So we can <strong>change the behavior or curve</strong> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).</p>
<p>For example, if our hypothesis function is $h_\theta(x)=\theta_0+\theta_1x_1$ then we can create additional features based on $x_1$, to get the quadratic function $h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_1^2$ or the cubic function $h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_1^2+\theta_3x_1^3$, to make it a square root function, we could do: $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 \sqrt{x_1}$.</p>
<p>In the cubic version, we can create new features $x_2$, $x_3$ where $x_2=x_1^2$ and $x_3=x_1^3$, then we can get a set of thetas via gradient descent for multiple variables.</p>
<p>âš ï¸Note. if you choose your features this way then <strong>feature scaling</strong> becomes very important.</p>
<p>e.g. if x has range 1 ~ 1000:</p>
<p>Then range of x^2 becomes 1 ~ 1000000;</p>
<p>And range of x^3 becomes 1 ~ 1000000000;</p>

  </div>
</article>
<!--Disqus-->


<!--Livere-->

    <div class="blog-post-comments">
        <div id="lv-container" data-id="city" data-uid="MTAyMC80NjEzMi8yMjY0Mw==">
            <noscript>ä¸å¯ç”¨ JavaScript æ”¯æŒçš„äººæ˜¯çœ‹ä¸åˆ°å¯çˆ±çš„è¯„è®ºåŒºçš„ã€‚ğŸ˜¥</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">é¦–é¡µ</a></li>
         
          <li><a href="/about/">å…³äº</a></li>
         
          <li><a href="/archives/">å½’æ¡£</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">é¡¹ç›®</a></li>
         
          <li><a href="/search/">æœç´¢</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Notes-of-Andrew-Ng%E2%80%99s-Machine-Learning-%E2%80%94%E2%80%94-3-Multivariate-Linear-Regression"><span class="toc-number">1.</span> <span class="toc-text">Notes of Andrew Ngâ€™s Machine Learning â€”â€” (3) Multivariate Linear Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Multiple-Features"><span class="toc-number">1.1.</span> <span class="toc-text">Multiple Features</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Notations"><span class="toc-number">1.1.1.</span> <span class="toc-text">Notations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hypothesis"><span class="toc-number">1.1.2.</span> <span class="toc-text">Hypothesis</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent-For-Multiple-Variables"><span class="toc-number">1.2.</span> <span class="toc-text">Gradient Descent For Multiple Variables</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Feature-Scaling"><span class="toc-number">1.3.</span> <span class="toc-text">Feature Scaling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Feature-scaling"><span class="toc-number">1.3.1.</span> <span class="toc-text">Feature scaling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mean-normalization"><span class="toc-number">1.3.2.</span> <span class="toc-text">Mean normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implement"><span class="toc-number">1.3.3.</span> <span class="toc-text">Implement</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#In-octave"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">In octave</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Learning-Rate"><span class="toc-number">1.4.</span> <span class="toc-text">Learning Rate</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Debugging-gradient-descent"><span class="toc-number">1.4.1.</span> <span class="toc-text">Debugging gradient descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Automatic-convergence-test"><span class="toc-number">1.4.2.</span> <span class="toc-text">Automatic convergence test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Making-sure-gradient-descent-is-working-correctly"><span class="toc-number">1.4.3.</span> <span class="toc-text">Making sure gradient descent is working correctly</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implement-1"><span class="toc-number">1.4.4.</span> <span class="toc-text">Implement</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Features-and-Polynomial-Regression"><span class="toc-number">1.5.</span> <span class="toc-text">Features and Polynomial Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Combine-Features"><span class="toc-number">1.5.1.</span> <span class="toc-text">Combine Features</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Polynomial-Regression"><span class="toc-number">1.5.2.</span> <span class="toc-text">Polynomial Regression</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&text=å¤šå˜é‡çº¿æ€§å›å½’"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&title=å¤šå˜é‡çº¿æ€§å›å½’"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&is_video=false&description=å¤šå˜é‡çº¿æ€§å›å½’"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=å¤šå˜é‡çº¿æ€§å›å½’&body=Check out this article: https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&title=å¤šå˜é‡çº¿æ€§å›å½’"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&title=å¤šå˜é‡çº¿æ€§å›å½’"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&title=å¤šå˜é‡çº¿æ€§å›å½’"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&title=å¤šå˜é‡çº¿æ€§å›å½’"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://clownote.github.io/2019/09/05/AndrewNgML/AndrewNg-MachineLearning-3-MultivariateLinearRegression/&name=å¤šå˜é‡çº¿æ€§å›å½’&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> èœå•</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> ç›®å½•</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> åˆ†äº«</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> è¿”å›é¡¶éƒ¨</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2022 CDFMLR
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">é¦–é¡µ</a></li>
         
          <li><a href="/about/">å…³äº</a></li>
         
          <li><a href="/archives/">å½’æ¡£</a></li>
         
          <li><a target="_blank" rel="noopener" href="https://github.com/cdfmlr">é¡¹ç›®</a></li>
         
          <li><a href="/search/">æœç´¢</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"å¤åˆ¶åˆ°ç²˜è´´æ¿!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "å¤åˆ¶æˆåŠŸ!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-146911386-1', 'auto');
        ga('send', 'pageview');
    </script>
    
    <!-- New: global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-146911386-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-146911386-1');
    </script>
    

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?9a0d2e6fde93dad496ac79f04f3aba97";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->


<!--Livere Comments-->

    <script type="text/javascript">
      (function (d, s) {
        var j, e = d.getElementsByTagName(s)[0];

        if (typeof LivereTower === 'function') { return; }

        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;

        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>


</body>
</html>
